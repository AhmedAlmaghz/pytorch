.. Cudagraph الأشجار

CUDAGraph هي ميزة في CUDA تسمح للمطورين ببناء أشجار تعتمد على الرسوم البيانية لمعالجة البيانات بشكل أكثر كفاءة. توفر هذه الميزة طريقة لتنظيم العمليات الحسابية وتنفيذها بشكل متوازٍ على وحدات معالجة الرسوميات (GPUs).

تتيح أشجار CUDAGraph للمطورين وصف الخوارزميات المعقدة وتنفيذها باستخدام بنية شجرية. كل عقدة في الشجرة يمكن أن تمثل عملية حسابية مختلفة، مع إمكانية نقل البيانات بين العقد بفعالية.

تعد أشجار CUDAGraph مفيدة بشكل خاص في تطبيقات التعلم الآلي ومعالجة الرسوميات، حيث يمكن أن تساعد في تحسين استخدام موارد وحدة معالجة الرسوميات، مما يؤدي إلى أوقات تنفيذ أقصر وحوسبة أكثر كفاءة.

يوفر CUDA أدوات ومكتبات لتسهيل إنشاء وإدارة أشجار CUDAGraph. يمكن للمطورين تحديد الخوارزميات المعقدة باستخدام بناء جملة واضح وبديهي، والاستفادة من قدرات الحوسبة المتوازية لوحدات معالجة الرسوميات الحديثة.

توفر أشجار CUDAGraph المرونة والكفاءة في معالجة البيانات على وحدات معالجة الرسوميات، مما يجعلها أداة قوية للمطورين الذين يسعون إلى استغلال قوة الحوسبة لوحدات معالجة الرسوميات في تطبيقاتهم.
================

**الخلفية**
~~~~~~~~~~~~~~

CUDAGraph
-------------

للحصول على خلفية أكثر تفصيلاً حول CUDAGraphs، اقرأ `تسريع PyTorch باستخدام CUDAGraphs <https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/>`_.

`CUDA Graphs <https://developer.nvidia.com/blog/cuda-10-features-revealed/>`_، التي ظهرت لأول مرة في CUDA 10، تسمح بتعريف سلسلة من نوى CUDA وتغليفها كوحدة واحدة، أي رسم بياني للعمليات، بدلاً من تسلسل من العمليات التي يتم إطلاقها بشكل فردي. يوفر آلية لإطلاق العديد من عمليات GPU من خلال عملية CPU واحدة، وبالتالي يقلل من النفقات العامة للإطلاق.

يمكن أن توفر CUDA Graphs تسريعًا كبيرًا، خاصة بالنسبة للنماذج ذات النفقات العامة العالية لوحدة المعالجة المركزية أو الحسابات الصغيرة. هناك عدد من القيود بسبب الحاجة إلى تشغيل نفس النواة بنفس الحجج والاعتمادية وعناوين الذاكرة.

- لا يمكن التحكم في التدفق
- تسبب النواة التي تطلق عمليات المزامنة من المضيف إلى الجهاز (مثل .item()) أخطاء
- يتم تثبيت جميع الحجج المدخلة إلى النواة كما كانت مسجلة
- يتم تثبيت عناوين ذاكرة CUDA، ولكن يمكن تغيير قيم الذاكرة في تلك العناوين
- لا توجد عمليات CPU أساسية أو آثار جانبية لوحدة المعالجة المركزية

دمج PyTorch CUDAGraph
-----------------------------

يوفر PyTorch `غلاف ملاءمة <https://pytorch.org/docs/stable/generated/torch.cuda.CUDAGraph.html>`_ حول CUDAGraphs التي تتعامل مع بعض التفاعلات المعقدة مع مخصص ذاكرة التخزين المؤقت لـ PyTorch.

يستخدم CachingAllocator مجموعة ذاكرة منفصلة لجميع المخصصات الجديدة. أثناء تسجيل CUDAGraph، تتم محاسبة الذاكرة وتخصيصها وإلغاء تخصيصها تمامًا كما هو الحال أثناء التشغيل المتلهف. في إعادة التشغيل، يتم استدعاء النواة فقط، ولا توجد تغييرات على المخصص. بعد التسجيل الأولي، لا يعرف المخصص أي ذاكرة يتم استخدامها بنشاط في برامج المستخدم.

قد يؤدي استخدام مجموعة ذاكرة منفصلة بين المخصصات المتلهفة وتخصيصات cudagraph إلى زيادة ذاكرة برنامجك إذا كان هناك قدر كبير من الذاكرة المخصصة لكليهما.

جعل الدالات قابلة للرسم
----------------------

`جعل الدالات قابلة للرسم <https://pytorch.org/docs/stable/generated/torch.cuda.make_graphed_callables.html>`_ هو تجريد لـ PyTorch لمشاركة مجموعة ذاكرة واحدة عبر سلسلة من الدالات القابلة للاستدعاء. تستفيد الدالات القابلة للرسم من حقيقة أنه عند تسجيل CUDA Graph، تتم محاسبة الذاكرة بشكل دقيق بواسطة مخصص ذاكرة التخزين المؤقت لمشاركة الذاكرة بأمان بين تسجيلات CUDA Graph المنفصلة. في كل استدعاء، يتم الاحتفاظ بالإخراج كذاكرة حية، مما يمنع إحدى الدالات القابلة للاستدعاء من الكتابة فوق الذاكرة الحية لدالة أخرى. يمكن استدعاء الدالات القابلة للرسم بتسلسل واحد فقط؛ يتم حرق عناوين الذاكرة من التشغيل الأول في الثاني، وهكذا.

دمج TorchDynamo السابق لـ CUDA Graphs
-----------------------------------

لا يؤدي التشغيل باستخدام ``cudagraph_trees=False`` إلى إعادة استخدام الذاكرة عبر عمليات التقاط الرسوم البيانية المنفصلة، والتي يمكن أن تؤدي إلى تراجعات كبيرة في الذاكرة. حتى بالنسبة للنموذج الذي لا يحتوي على فواصل رسومية، توجد مشكلات. يعد التوجيه والعودة عبارة عن عمليات التقاط رسومية منفصلة، لذا لا يتم مشاركة مجموعات الذاكرة للأمام والخلف. على وجه الخصوص، لا يمكن استرداد الذاكرة للتنشيطات التي يتم حفظها في الأمام في الخلف.

**دمج CUDAGraph Trees**
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

مثل الدالات القابلة للرسم، تستخدم CUDA Graph Trees مجموعة ذاكرة واحدة عبر جميع عمليات التقاط الرسوم البيانية. ومع ذلك، بدلاً من طلب تسلسل استدعاءات واحد، تقوم CUDA Graph Trees بإنشاء أشجار منفصلة من عمليات التقاط CUDA Graph. دعنا نلقي نظرة على مثال توضيحي:

.. code-block:: python

    @torch.compile(mode="reduce-overhead")
    def foo(x):
        # GRAPH 1
        y = x * x * x
        # يتم تشغيل كسر الرسم البياني هنا
        if y.sum() > 0:
            # GRAPH 2
            z = y ** y
        else:
            # GRAPH 3
            z = (y.abs() ** y.abs())
        torch._dynamo.graph_break()
        # GRAPH 4
        return z * torch.rand_like(z)

    # تعمل الجري الأول على تسخين كل رسم بياني، مما يؤدي إلى أشياء مثل CuBlas أو Triton benchmarking
    foo(torch.arange(0, 10، device="cuda"))
    # يقوم التشغيل الثاني بتسجيل CUDA Graph، ثم إعادة تشغيله
    foo(torch.arange(0, 10، device="cuda"))
    # أخيرًا، نضرب مسار إعادة تشغيل CUDA Graph المحسن
    foo(torch.arange(0, 10، device="cuda"))


في هذا المثال، هناك مساران منفصلان نجريهما عبر الوظيفة: 1 -> 2 -> 4، أو 1 -> 3 -> 4.

نحن نشارك كل الذاكرة في مجموعة ذاكرة واحدة بين التسجيلات المنفصلة من خلال بناء شريط من تسجيلات CUDA Graph، في هذه الحالة، 1 -> 2 -> 4. نضيف قيودًا لضمان أن تكون الذاكرة دائمًا في نفس الموقع كما كانت مسجلة، وأنه لا توجد توترات حية في برامج المستخدم قد تتم الكتابة فوقها.

- تنطبق نفس القيود من CUDA Graphs: يجب استدعاء نفس النواة بنفس الحجج (الأحجام الثابتة والعناوين، إلخ)
- يجب ملاحظة نفس نمط الذاكرة بين التسجيل وإعادة التشغيل: إذا ماتت مخرجات tensor لواحد graph بعد graph آخر أثناء التسجيل، فيجب أن تفعل ذلك أيضًا أثناء إعادة التشغيل.
- تفرض الذاكرة الحية في مجموعة CUDA اعتمادًا بين تسجيلين
- يمكن استدعاء هذه التسجيلات بتسلسل واحد فقط 1 -> 2 -> 4

يتم مشاركة كل الذاكرة في مجموعة ذاكرة واحدة، لذا لا توجد نفقات عامة إضافية للذاكرة مقارنة بالتشغيل المتلهف. الآن، ماذا يحدث إذا كنا سنضرب مسارًا جديدًا ونشغل Graph 3؟

يتم إعادة تشغيل Graph 1، ثم نضرب Graph 3، والذي لم نقم بتسجيله بعد. في عمليات إعادة تشغيل الرسوم البيانية، لا يتم تحديث مجموعة الذاكرة الخاصة، لذا لا يتم عكس y في المخصص. بدون عناية، سنقوم بالكتابة فوقه. لدعم إعادة استخدام مجموعة الذاكرة نفسها بعد إعادة تشغيل الرسوم البيانية الأخرى، نقوم بتشغيل نقطة تفتيش لمجموعة الذاكرة إلى حالتها في نهاية الرسم البياني 1. الآن بعد أن تم عكس التوترات الحية لدينا في مخصص ذاكرة التخزين المؤقت، يمكننا تشغيل رسم بياني جديد بأمان.

أولاً، سنضرب مسار CUDA Graph.replay() المحسن الذي قمنا بتسجيله بالفعل في الرسم البياني 1. ثم سنضرب Graph 3. تمامًا كما هو الحال قبل ذلك، سنحتاج إلى تسخين الرسم البياني مرة واحدة قبل التسجيل. في جريان التسخين، لا يتم تثبيت عناوين الذاكرة، لذا فإن الرسم البياني 4 سوف يتراجع أيضًا إلى مسار الاستقراء، غير استدعاء cudagraph.

في المرة الثانية التي نضرب فيها الرسم البياني 3، نكون دافئين وجاهزين للتسجيل. نقوم بتسجيل الرسم البياني 3 ثم نقوم بتسجيل الرسم البياني 4 مرة أخرى نظرًا لتغيير عناوين الذاكرة المدخلة. هذا يخلق شجرة من تسجيلات CUDA Graph. شجرة CUDA Graph!

::

    1
   / \\
  2   3
   \\   \\
    4   4


دعم طفرة الإدخال
----------------------

تشير دالة طفرة الإدخال إلى دالة تقوم بكتابات في المكان إلى tensor المدخلة،
كما هو موضح أدناه:

.. code-block:: python

    def foo(x، y):
        # يغير المدخلات x
        x.add_(1)
        return x + y

تؤدي دالات طفرة الإدخال بشكل عام إلى تحديات لـ CUDA Graph Trees. نظرًا لمتطلبات عنوان ذاكرة CUDA الثابتة من CUDAGraph، بالنسبة لكل tensor مدخلة x، قد تقوم CUDA Graph Trees
بتخصيص عنوان ذاكرة ثابت x'. أثناء التنفيذ، تقوم CUDA Graph Trees أولاً بنسخ tensor المدخلة x إلى عنوان الذاكرة الثابت x'، ثم إعادة تشغيل CUDAGraph المسجل. بالنسبة لدالة طفرة الإدخال، يتم تحديث x' في المكان، وهو ما لا ينعكس على tensor المدخلة x نظرًا لأن x و x' يقيمون في عناوين ذاكرة CUDA منفصلة.

إن إلقاء نظرة فاحصة على دالات طفرة الإدخال يكشف أنه توجد ثلاثة أنواع من المدخلات:

* **الإدخالات من المتلهف**: نفترض أن هذه التوترات ستختلف عناوين tensor من
  تنفيذ إلى تنفيذ. لأن cudagraphs تجمد عناوين الذاكرة، نحتاج إلى نسخ هذه
  الإدخالات إلى tensor عنوان ثابت قبل تسجيل الرسم البياني والتنفيذ.
* **المعلمات والبوفرات**: نفترض (ونفحص وقت التشغيل) أن هذه التوترات لها نفس عناوين tensor
  في كل تنفيذ. لا نحتاج إلى نسخ محتوياتها لأن عنوان الذاكرة المسجل سيكون هو نفسه
  عنوان الذاكرة المنفذة.
* **التوترات التي هي مخرجات سابقة من CUDAGraph Trees**: نظرًا لأن عناوين tensor الإخراج من cudagraph ثابتة، إذا قمنا بتشغيل CUDAGraph1، ثم قمنا بتشغيل CUDAGraph2، فستكون المدخلات التي جاءت من CUDAGraph1 إلى CUDAGraph2 لها عنوان ذاكرة ثابت. لا تتطلب هذه الإدخالات، مثل المعلمات والبوفرات، نسخها إلى tensor عنوان ثابت. نحن نتأكد من أن هذه الإدخالات مستقرة في وقت التشغيل، وإذا لم تكن كذلك، فسنعيد التسجيل.

تدعم CUDAGraph Trees طفرة الإدخال على المعلمات والبوفرات، والتوترات التي هي مخرجات سابقة من CUDAGraph Trees. بالنسبة لطفرة الإدخال على الإدخالات من المتلهف، ستقوم CUDAGraph Trees بتشغيل الدالة بدون CUDAGraph وإصدار *تخطي بسبب سجلات الإدخال المطفرة*. يوضح المثال التالي دعم CUDAGraph Trees للتوترات التي هي مخرجات سابقة من CUDAGraph Trees.


.. code-block:: python

    import torch

    @torch.compile(mode="reduce-overhead")
    def foo(x):
        return x + 1

    @torch.compile(mode="reduce-overhead")
    def mut(x):
        return x.add_(2)

    # تمكين دعم طفرة الإدخال
    torch._inductor.config.triton.cudagraph_support_input_mutation = True

    for i in range(3):
        torch.compiler.cudagraph_mark_step_begin()
        inp = torch.rand([4]، device="cuda")

        # يتم تطبيق CUDAGraph نظرًا لأن "foo" لا يغير "inp"
        tmp = foo(inp)
        # على الرغم من أن "mut" يغير "tmp"، وهو مخرج دالة يديرها CUDAGraph
        # لذلك يتم تطبيق CUDAGraph أيضًا.
        mut(tmp)


    torch.compiler.cudagraph_mark_step_begin()
    inp = torch.rand([4]، device="cuda")

    tmp = foo(inp)
    # في حين أن "tmp" هو مخرج دالة شجرة CUDAGraph، فإن "tmp.clone()"
    # ليس كذلك. لذلك لا يتم تطبيق CUDAGraph على "mut" وهناك سجل
    # "تخطي cudagraphs بسبب إدخالات مطفرة"
    mut(tmp.clone())


لتمكين CUDAGraph Trees لدالة تطفو المدخلات من المتلهف، يرجى إعادة كتابة
الدالة لتجنب طفرة الإدخال.

.. ملاحظة:: تمكين دعم طفرة الإدخال عن طريق تعيين
  `torch._inductor.config.cudagraph_support_input_mutation = True <https://github.com/pytorch/pytorch/blob/main/torch/_inductor/config.py#L662>`_
  لوضع "reduce-overhead".


دعم الشكل الديناميكي
-------------

`الشكل الديناميكي <https://pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html>`_
يعني أن tensor المدخلة لها أشكال مختلفة عبر استدعاءات الدالة. نظرًا لأن CUDAGraph
يتطلب عناوين tensor ثابتة، تقوم CUDAGraph Trees بإعادة تسجيل CUDAGraph لكل شكل فريد
من tensor المدخلة. يؤدي ذلك إلى وجود عدة CUDAGraphs لرسم بياني واحد للاستقراء.
عندما تكون هناك أشكال محدودة (على سبيل المثال، أحجام الدُفعات في الاستدلال)، يكون من المجدي
إعادة تسجيل CUDAGraphs. ومع ذلك، إذا تغيرت أشكال tensor المدخلة بشكل متكرر أو حتى في
كل استدعاء، فقد لا يكون إعادة تسجيل CUDAGraph مربحًا. تستخدم Nvidia 64 كيلوبايت من
ذاكرة الجهاز لكل عملية إطلاق kernel في CUDAGraph، حتى CUDA 12.4 وDriver Version 550+.
يمكن أن تكون هذه التكلفة كبيرة مع العديد من عمليات إعادة تسجيل CUDAGraph.

بالنسبة للوظائف ذات أشكال tensor المدخلة المتغيرة بشكل متكرر، نقترح استخدام التوترات المدخلة ذات الأشكال الثابتة القليلة للاستمتاع بفوائد CUDAGraph. بالإضافة إلى ذلك،
يتيح تعيين `torch._inductor.config.triton.cudagraph_skip_dynamic_graphs=True <https://github.com/pytorch/pytorch/blob/main/torch/_inductor/config.py#L653>`_
تخطي وظائف cudagraphing ذات المدخلات الديناميكية والقيام فقط بوظائف cudagraphing
مع أشكال tensor المدخلة الثابتة.


دعم NCCL
------------

تدعم CUDAGraph Trees الوظائف التي تحتوي على مشغلات nccl. بينما تقوم CUDAGraph Trees بأداء تسجيل لكل جهاز لـ CUDAGraph، يسمح دعم NCCL بالاتصال عبر الأجهزة.

.. code-block:: python

    @torch.compile(mode="reduce-overhead")
    def func(x):
        y = x * x
        y = torch.distributed.all_reduce(y، op=torch.distributed.ReduceOp.SUM)
        x = torch.nn.functional.silu(x)
        return x * y


أسباب تخطي CUDAGraph
------------------------

نظرًا لأن CUDAGraph لديه متطلبات مثل عناوين ثابتة للمدخلات وعدم دعم مشغلات وحدة المعالجة المركزية، فإن CUDAGraph Trees يتحقق مما إذا كانت الدالة تستوفي هذه المتطلبات وقد تتخطى CUDAGraph عند الضرورة. وفيما يلي، نقدم قائمة بالأسباب الشائعة لتخطي CUDAGraph.

* **تعديل الإدخال**: تتخطى CUDAGraph Trees الدوال التي تقوم بتعديل الإدخال في مكانه.
  لا يزال تعديل المعلمات والذاكرات المؤقتة في مكانها، أو إخراج المنسوجات من الدوال التي تديرها CUDAGraph Tree مدعومًا. يرجى الاطلاع على قسم "دعم تعديل الإدخال" لمزيد من التفاصيل.
* **مشغلات وحدة المعالجة المركزية**: يتم تخطي الدوال التي تحتوي على مشغل وحدة المعالجة المركزية. يرجى تقسيم الدالة إلى دوال متعددة وتطبيق CUDAGraph Trees على الدوال التي تحتوي على مشغلات GPU فقط.
* **مشغلات متعددة الأجهزة**: يتم تخطي الدالة إذا كانت تحتوي على مشغلات على أجهزة متعددة. حاليًا، يتم تطبيق CUDAGraph على أساس كل جهاز على حدة. يرجى استخدام المكتبات المدعومة مثل NCCL للتواصل بين الأجهزة. يرجى الاطلاع على قسم "دعم NCCL" لمزيد من التفاصيل.
* **الرموز الحرة غير المدعومة**: تحدث الرموز الحرة غير المدعومة عادة أثناء "الأشكال الديناميكية <https://pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html>".
  تقوم CUDAGraph Trees حاليًا بتسجيل CUDAGraph لكل شكل فريد من أشكال المنسوجات المدخلة.
  يرجى الاطلاع على قسم "دعم الشكل الديناميكي" لمزيد من التفاصيل.
* **مشغلات غير متوافقة**: تتخطى CUDAGraph Trees الدالة إذا كانت تحتوي على مشغلات غير متوافقة. يرجى استبدال هذه المشغلات في الدالة بالمشغلات المدعومة. نقدم قائمة شاملة بالمشغلات غير المتوافقة:


.. code-block:: python

    aten._fused_moving_avg_obs_fq_helper.default
    aten._fused_moving_avg_obs_fq_helper_functional.default
    aten.multinomial.default
    fbgemm.dense_to_jagged.default
    fbgemm.jagged_to_padded_dense.default
    run_and_save_rng_state
    run_with_rng_state
    aten._local_scalar_dense
    aten._assert_scalar


المشغلات التالية غير متوافقة عندما `torch.are_deterministic_algorithms_enabled() <https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html>`_.


.. code-block:: python

    aten._fused_moving_avg_obs_fq_helper.default
    aten._fused_moving_avg_obs_fq_helper_functional.default
    aten.multinomial.default
    fbgemm.dense_to_jaggged.default
    fbgemm.jagged_to_padded_dense.default
    run_and_save_rng_state
    run_with_rng_state
    aten._local_scalar_dense
    aten._assert_scalar


القيود
----

نظرًا لأن CUDA Graph يقوم بتثبيت عناوين الذاكرة، فإن CUDA Graphs لا تملك طريقة جيدة للتعامل مع المنسوجات الحية من استدعاء سابق.

لنفترض أننا نقوم باختبار الأداء لتشغيل الاستدلال باستخدام الكود التالي:

.. code-block:: python

    import torch

    @torch.compile(mode="reduce-overhead")
    def my_model(x):
        y = torch.matmul(x, x)
        return y

    x = torch.randn(10, 10)
    y1 = my_model(x)
    y2 = my_model(x)
    print(y1)
    # RuntimeError: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run.

في تنفيذ CUDA Graph المنفصل، سيتم الكتابة فوق الإخراج من الاستدعاء الأول بواسطة الاستدعاء الثاني. في CUDAGraph
Trees، لا نريد إضافة تبعيات غير مقصودة بين الحلقات التي قد تمنعنا من الوصول إلى المسار السريع، ولا نريد
تحرير الذاكرة بشكل مبكر من استدعاء سابق. تنص افتراضاتنا على أنه في الاستدلال، نبدأ حلقة جديدة في كل استدعاء لـ
torch.compile، وفي التدريب نفعل الشيء نفسه طالما لم يتم استدعاء التخلف إلى الوراء. إذا كانت هذه الافتراضات
خاطئة، يمكنك وضع علامة على بداية حلقة جديدة باستخدام
`torch.compiler.mark_step_begin() <https://pytorch.org/docs/stable/generated/torch.compiler.cudagraph_mark_step_begin.html>`_، أو استنساخ
المنسوجات من تكرار سابق (خارج torch.compile) قبل بدء الجري التالي.


مقارنات
-------

.. list-table::
   :widths: 20 40 40
   :header-rows: 1

   * - الأخطاء
     - CUDA Graph المنفصل
     - CUDAGraph Trees
   * - يمكن أن تزداد الذاكرة
     - مع كل تجميع للرسم البياني (أحجام جديدة، إلخ)
     - إذا كنت تقوم أيضًا بتشغيل ذاكرة غير CUDAGraph
   * - التسجيلات
     - مع أي استدعاء جديد للرسم البياني
     - سيتم إعادة التسجيل مع أي مسار جديد فريد تقوم به خلال برنامجك
   * - الأخطاء
     - سيؤدي استدعاء رسم بياني واحد إلى الكتابة فوق الاستدعاء السابق
     - لا يمكن الاحتفاظ بالذاكرة بين عمليات تشغيل منفصلة عبر نموذجك - حلقة تدريب واحدة، أو تشغيل استدلال واحد