:orphan:

.. _distributed-autograd-design:

تصميم Autograd الموزع
==============================

ستقدم هذه الملاحظة التصميم التفصيلي لـ autograd الموزع وستشرح داخلياته. تأكد من أنك على دراية بـ :ref:`autograd-mechanics` و :ref:`distributed-rpc-framework` قبل المتابعة.

الخلفية
^^^^^^^^

لنفترض أن لديك عقدتين ونموذجًا بسيطًا للغاية مقسمًا عبر عقدتين. يمكن تنفيذ هذا باستخدام :mod:`torch.distributed.rpc` كما يلي:

.. code::

   استيراد الشعلة
   استيراد الشعلة. distributed.rpc كما rpc

   def my_add (t1، t2):
     return torch.add (t1، t2)

   # على العامل 0:
   t1 = torch.rand ((3، 3)، يتطلب_grad = صحيح)
   t2 = torch.rand ((3، 3)، يتطلب_grad = صحيح)

   # أداء بعض العمليات الحسابية عن بعد.
   t3 = rpc.rpc_sync ("worker1"، my_add، args = (t1، t2))

   # أداء بعض العمليات الحسابية المحلية بناءً على النتيجة البعيدة.
   t4 = torch.rand ((3، 3)، يتطلب_grad = صحيح)
   t5 = torch.mul (t3، t4)

   # احسب بعض الخسائر.
   الخسارة = t5.sum ()

الدافع الرئيسي وراء autograd الموزع هو تمكين تشغيل تمريرة خلفية
على مثل هذه النماذج الموزعة مع "الخسارة" التي قمنا بحسابها وتسجيل التدرجات المناسبة
لكل المنسوجات التي تتطلب التدرجات.

.. attaching_send_recv_functions:

تسجيل Autograd أثناء التمرير للأمام
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

تُنشئ PyTorch رسم autograd أثناء التمرير للأمام ويستخدم هذا الرسم
لتنفيذ التمريرة الخلفية. لمزيد من التفاصيل، راجع :ref:`how-autograd-encodes-history`.

بالنسبة لـ autograd الموزع، نحتاج إلى تتبع جميع RPCs أثناء التمرير للأمام
لضمان تنفيذ التمريرة الخلفية بشكل مناسب. ولهذا الغرض، نقوم بتعليق
وظائف "الإرسال" و "الاستقبال" على رسم autograd عندما نؤدي
RPC.

- يتم تعليق وظيفة "الإرسال" على مصدر RPC وتتجه مخرجاتها
  الحواف إلى دالة autograd للمدخلات المنسوجة لـ RPC.
  يتم استقبال الإدخال لهذه الوظيفة أثناء التمريرة الخلفية من
  الوجهة باعتبارها إخراج وظيفة "الاستقبال" المناسبة.
- يتم تعليق وظيفة "الاستقبال" على وجهة RPC و
  يتم استرداد المدخلات من المشغلين المنفذين على الوجهة باستخدام
  المنسوجات المدخلة. يتم إرسال تدرجات الإخراج لهذه الوظيفة إلى
  العقدة المصدر إلى وظيفة "الإرسال" المناسبة أثناء التمريرة الخلفية.
- يتم تعيين كل زوج "إرسال-استقبال" معرفًا فريدًا عالميًا
  ``autograd_message_id`` لتحديد الزوج بشكل فريد. هذا مفيد
  للبحث عن الوظيفة المقابلة على عقدة بعيدة أثناء التمريرة الخلفية.
- بالنسبة لـ :ref:`rref`، عندما نستدعي :meth:`torch.distributed.rpc.RRef.to_here`
  نعلق زوج "إرسال-استقبال" المناسب للمنسوجات المعنية.

على سبيل المثال، سيكون رسم autograd لمثالنا أعلاه كما يلي
(t5.sum() مستبعد للبساطة):

.. image:: ../_static/img/distributed_autograd/send_recv_functions.png

.. autograd_context:

سياق Autograd الموزع
^^^^^^^^^^^^^^^^^^^^^^^

يتم تعيين كل تمريرة للأمام والخلف تستخدم autograd الموزع
:class:`torch.distributed.autograd.context` فريد عالميًا وهذا السياق
:attr:`~torch.distributed.autograd.context.autograd_context_id`. يتم إنشاء هذا السياق على كل عقدة
حسب الحاجة.

يخدم هذا السياق الغرض التالي:

1. قد تقوم عدة عقد بتشغيل تمريرات خلفية موزعة
   قد تتراكم التدرجات على نفس المنسوجة ونتيجة لذلك
   قد يحتوي حقل ".grad" للمنسوجة على تدرجات من مجموعة متنوعة
   من التمريرات الخلفية الموزعة قبل أن تتاح لنا الفرصة
   لتشغيل المحسن. هذا مشابه لاستدعاء :meth:`torch.autograd.backward`
   محليا. لتوفير طريقة لفصل التدرجات لكل تمريرة خلفية،
   يتم تراكم التدرجات في :class:`torch.distributed.autograd.context`
   لكل تمريرة خلفية.
2. أثناء التمرير للأمام، نقوم بتخزين وظائف "الإرسال" و "الاستقبال" لكل
   تمريرة autograd في هذا السياق. هذا يضمن أننا نحتفظ بالإشارات إلى
   العقد المناسبة في رسم autograd لإبقائه على قيد الحياة. بالإضافة إلى
   هذا، من السهل البحث عن وظائف "الإرسال" و "الاستقبال" المناسبة أثناء التمريرة الخلفية.
3. بشكل عام، نستخدم هذا السياق أيضًا لتخزين بعض البيانات الوصفية لكل
   تمريرة autograd الموزعة.

|

من منظور المستخدم، يتم إعداد سياق autograd كما يلي:

.. code::

   استيراد الشعلة. distributed.autograd كما dist_autograd
   مع dist_autograd.context () كما context_id:
     الخسارة = model.forward ()
     dist_autograd.backward (context_id، loss)

من المهم ملاحظة أن تمريرة للأمام للنموذج يجب أن تتم داخل
مدير سياق autograd الموزع، حيث يلزم وجود سياق صالح
لضمان تخزين جميع وظائف "الإرسال" و "الاستقبال" بشكل صحيح
لتشغيل التمريرة الخلفية عبر جميع العقد المشاركة.

تمريرة خلفية موزعة
^^^^^^^^^^^^^^^

في هذا القسم، نوضح تحدي حساب التبعيات بدقة أثناء
تمريرة خلفية موزعة ونصف خوارزميتين (مع المزايا والفوائد)
كيف يمكننا تنفيذ تمريرة خلفية موزعة.

حساب التبعيات
-----------

خذ في الاعتبار التالي قطعة من التعليمات البرمجية التي يتم تشغيلها على جهاز واحد

.. code::

   استيراد الشعلة
   أ = الشعلة. rand ((3، 3)، يتطلب_grad = صحيح)
   ب = الشعلة. rand ((3، 3)، يتطلب_grad = صحيح)
   ج = الشعلة. rand ((3، 3)، يتطلب_grad = صحيح)
   د = أ + ب
   ه = ب * ج
   د. sum (). الخلفي ()

هذا هو ما ستبدو عليه رسم autograd للرمز أعلاه:

.. image:: ../_static/img/distributed_autograd/local_dependencies.png
  :scale: 80%

الخطوة الأولى التي يقوم بها محرك autograd كجزء من التمريرة الخلفية هي
حساب عدد التبعيات لكل عقدة في رسم autograd. هذا
يساعد محرك autograd على معرفة متى تكون العقدة في الرسم جاهزة للتنفيذ.
تشير الأرقام بين قوسين لـ "add(1)" و "mul(0)" إلى عدد
التابعات. كما ترون، هذا يعني أثناء التمريرة الخلفية أن عقدة "add"
تحتاج إلى إدخال واحد ولا تحتاج عقدة "mul" إلى أي إدخالات (بمعنى آخر
لا تحتاج إلى التنفيذ). يحسب محرك autograd المحلي هذه التبعيات
باستكشاف الرسم من الجذور (في هذه الحالة "d").

إن حقيقة أن بعض العقد في رسم autograd قد لا يتم تنفيذها في
تشكل التمريرة الخلفية تحديًا لـ autograd الموزع. ضع في اعتبارك هذه القطعة
من التعليمات البرمجية التي تستخدم RPC.

.. code::

   استيراد الشعلة
   استيراد الشعلة. distributed.rpc كما rpc

   أ = الشعلة. rand ((3، 3)، يتطلب_grad = صحيح)
   ب = الشعلة. rand ((3، 3)، يتطلب_grad = صحيح)
   ج = الشعلة. rand ((3، 3)، يتطلب_grad = صحيح)

   د = rpc.rpc_sync ("worker1"، الشعلة. إضافة، args = (أ، ب))
   ه = rpc.rpc_sync ("worker1"، الشعلة. mul، args = (ب، ج))
   الخسارة = د. sum ()

سيكون رسم autograd المرتبط بالرمز أعلاه كما يلي:

.. image:: ../_static/img/distributed_autograd/distributed_dependencies.png

حساب التبعيات لهذا الرسم autograd الموزع أكثر
التحدي ويتطلب بعض النفقات العامة (إما من حيث الحساب أو
اتصال الشبكة).

بالنسبة للتطبيقات الحساسة للأداء، يمكننا تجنب الكثير من النفقات العامة
من خلال افتراض أن كل وظائف "الإرسال" و "الاستقبال" صالحة
كجزء من التمريرة الخلفية (لا تقوم معظم التطبيقات بإجراء RPCs التي لا تستخدم). هذا
تبسيط خوارزمية autograd الموزعة وأكثر كفاءة، ولكن بتكلفة
يجب أن يكون التطبيق على دراية بالقيود. تسمى هذه الخوارزمية خوارزمية "وضع FAST"
يتم وصفه بالتفصيل أدناه.

في الحالة العامة، قد لا يكون من الضروري أن تكون كل وظيفة "إرسال" و "استقبال"
صالحة كجزء من التمريرة الخلفية. لمعالجة ذلك، اقترحنا
خوارزمية "وضع SMART" التي يتم وصفها في قسم لاحق. يرجى ملاحظة أن
حاليا، يتم تنفيذ خوارزمية "وضع FAST" فقط.

.. _fast-mode-algorithm:

خوارزمية الوضع السريع
-------------------

الافتراض الرئيسي لهذه الخوارزمية هو أن لكل وظيفة "إرسال"
تبعية 1 عند تشغيل تمريرة خلفية. بعبارة أخرى، نفترض أننا
ستتلقى تدرجًا عبر RPC من عقدة أخرى.

الخوارزمية هي كما يلي:

1. نبدأ من العامل الذي لديه الجذور للتمريرة الخلفية
   (يجب أن تكون جميع الجذور محلية).
2. البحث عن جميع وظائف "الإرسال" لسياق autograd الموزع الحالي
   `سياق Autograd الموزع`_.
3. احسب التبعيات محليًا بدءًا من الجذور المقدمة وجميعها
   وظائف "الإرسال" التي استرجعناها.
4. بعد حساب التبعيات، ابدأ تشغيل محرك autograd المحلي مع
   الجذور المقدمة.
5. عندما ينفذ محرك autograd وظيفة "الاستقبال"، ترسل وظيفة "الاستقبال"
   تدرجات الإدخال عبر RPC إلى العامل المناسب.
   تعرف كل وظيفة "استقبال" معرف العامل الوجهة لأنه يتم تسجيله
   كجزء من التمرير للأمام. ترسل وظيفة "الاستقبال" أيضًا
   ``autograd_context_id`` و ``autograd_message_id`` إلى المضيف البعيد.
6. عندما يتم استقبال هذا الطلب على المضيف البعيد، نستخدم
   ``autograd_context_id`` و ``autograd_message_id`` للبحث عن
   وظيفة "الإرسال" المناسبة.
7. إذا كانت هذه هي المرة الأولى التي يسمع فيها عامل عن طلب
   ``autograd_context_id``، فسيقوم بحساب التبعيات المحلية كما هو موضح
   في النقاط من 1 إلى 3 أعلاه.
8. يتم وضع الوظيفة "الإرسال" المستردة في الخطوة 6. في قائمة الانتظار للتنفيذ على
   محرك autograd المحلي لهذا العامل.
9. أخيرًا، بدلاً من تراكم التدرجات في حقل ".grad" للمنسوجة، نقوم بتراكمها بشكل منفصل
   لكل `سياق Autograd الموزع`_. يتم تخزين التدرجات في
   ``Dict [Tensor، Tensor]``، وهو عبارة عن خريطة من Tensor إلى
   تدرجه المرتبط ويمكن استرداد هذه الخريطة باستخدام
   :meth:`~torch.distributed.autograd.get_gradients` API.

|

كمثال سيكون الرمز الكامل مع autograd الموزع كما يلي:

.. code::

   استيراد الشعلة
   استيراد الشعلة. distributed.autograd كما dist_autograd
   استيراد الشعلة. distributed.rpc كما rpc

   def my_add (t1، t2):
     return torch.add (t1، t2)

   # على العامل 0:

   # إعداد سياق autograd. يجب أن تكون الحسابات التي تشارك في
   # التمريرة الخلفية الموزعة ضمن مدير سياق autograd الموزع.
   مع dist_autograd.context () كما context_id:
     t1 = torch.rand ((3، 3)، يتطلب_grad = صحيح)
     t2 = torch.rand ((3، 3)، يتطلب_grad = صحيح)

     # أداء بعض العمليات الحسابية عن بعد.
     t3 = rpc.rpc_sync ("worker1"، my_add، args = (t1، t2))

     # أداء بعض العمليات الحسابية المحلية بناءً على النتيجة البعيدة.
     t4 = torch.rand ((3، 3)، يتطلب_grad = صحيح)
     t5 = torch.mul (t3، t4)

     # احسب بعض الخسائر.
     الخسارة = t5.sum ()

     # تشغيل التمريرة الخلفية.
     dist_autograd.backward (context_id، [loss])

     # استرداد التدرجات من السياق.
     dist_autograd.get_gradients (context_id)

سيكون رسم autograd الموزع مع التبعيات كما يلي (t5.sum() مستبعد للبساطة):

.. image:: ../_static/img/distributed_autograd/distributed_dependencies_computed.png

تطبق خوارزمية "وضع FAST" على المثال أعلاه كما يلي:

1. على "Worker 0" نبدأ من الجذور "loss" و "send1" لحساب
   التبعيات. ونتيجة لذلك، يتم وضع علامة "send1" بتبعية 1 و "mul"
   على "Worker 0" يتم وضع علامة بتبعية 1.
2. الآن، نبدأ تشغيل محرك autograd المحلي على "Worker 0". أولاً ننفذ
   وظيفة "المضاعفة"، ونتراكم ناتجها في سياق autograd كـ
   تدرج لـ "t4". ثم ننفذ "recv2" الذي يرسل التدرجات إلى
   "Worker 1".
3. نظرًا لأن هذه هي المرة الأولى التي يسمع فيها "Worker 1" عن هذه التمريرة الخلفية،
   يبدأ في حساب التبعيات المحلية كما هو موضح في النقاط 1-3 أعلاه.
4. بعد ذلك، نقوم بتعليق "الإرسال 2" في قائمة الانتظار للتنفيذ على
   محرك autograd المحلي لـ "Worker 1"، والذي بدوره ينفذ "add" و
   "recv1".
5. عندما يتم تنفيذ "recv1"، فإنه يرسل التدرجات إلى "Worker 0".
6. نظرًا لأن "Worker 0" قام بالفعل بحساب التبعيات لهذه التمريرة الخلفية،
   فهو يقوم فقط بتعليق "الإرسال 1" وتنفيذه محليًا.
7. أخيرًا، يتم تراكم التدرجات لـ "t1" و "t2" و "t4" في
   `سياق Autograd الموزع`_.

خوارزمية الوضع الذكي
----------------
لا تزال التفاصيل الكاملة لهذا الخوارزمية قيد الإعداد، ولكن للحصول على الفكرة العامة، يمكنك الرجوع إلى قسم "وضع الذكاء لخوارزمية Autograd الموزعة" في `RFC`_.

المحسن الموزع
^^^^^^^^^^^

يعمل :class:`~torch.distributed.optim.DistributedOptimizer` على النحو التالي:

1. يأخذ قائمة من المعلمات البعيدة (:class:`~torch.distributed.rpc.RRef`) لتحسينها. يمكن أن تكون هذه أيضًا معلمات محلية ملفوفة داخل "RRef" محلي.
2. يأخذ فئة :class:`~torch.optim.Optimizer` كمحسن محلي لتشغيله على جميع مالكي "RRef" المميزين.
3. يقوم المحسن الموزع بإنشاء مثيل للمحسن المحلي "Optimizer" على كل من عقد العمل ويحتفظ بـ "RRef" لهم.
4. عندما يتم استدعاء :meth:`torch.distributed.optim.DistributedOptimizer.step`، يستخدم المحسن الموزع RPC لتنفيذ جميع المحسنات المحلية عن بُعد على العمال البعيدين المناسبين. يجب توفير معرف "context_id" للمحسن الموزع كإدخال إلى :meth:`torch.distributed.optim.DistributedOptimizer.step`. يستخدمه المحسنون المحليون لتطبيق التدرجات المخزنة في السياق المقابل.
5. إذا كان هناك عدة محسنات موزعة متزامنة تقوم بتحديث نفس المعلمات على عامل، يتم تسلسل هذه التحديثات عبر قفل.

مثال بسيط من البداية إلى النهاية
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

عند الجمع بين كل ما سبق، فإن ما يلي هو مثال بسيط من البداية إلى النهاية باستخدام المحسن الموزع وخاصية autograd الموزعة. إذا تم وضع الكود في ملف يسمى "dist_autograd_simple.py"، فيمكن تشغيله باستخدام الأمر :code:`MASTER_ADDR="localhost" MASTER_PORT=29500 python dist_autograd_simple.py`:

.. code::

  import torch
  import torch.multiprocessing as mp
  import torch.distributed.autograd as dist_autograd
  from torch.distributed import rpc
  from torch import optim
  from torch.distributed.optim import DistributedOptimizer

  def random_tensor():
      return torch.rand((3, 3), requires_grad=True)

  def _run_process(rank, dst_rank, world_size):
      name = "worker{}".format(rank)
      dst_name = "worker{}".format(dst_rank)

      # Initialize RPC.
      rpc.init_rpc(
          name=name,
          rank=rank,
          world_size=world_size
      )

      # Use a distributed autograd context.
      with dist_autograd.context() as context_id:
          # Forward pass (create references on remote nodes).
          rref1 = rpc.remote(dst_name, random_tensor)
          rref2 = rpc.remote(dst_name, random_tensor)
          loss = rref1.to_here() + rref2.to_here()

          # Backward pass (run distributed autograd).
          dist_autograd.backward(context_id, [loss.sum()])

          # Build DistributedOptimizer.
          dist_optim = DistributedOptimizer(
          optim.SGD,
          [rref1, rref2],
          lr=0.05,
          )

          # Run the distributed optimizer step.
          dist_optim.step(context_id)

  def run_process(rank, world_size):
      dst_rank = (rank + 1) % world_size
      _run_process(rank, dst_rank, world_size)
      rpc.shutdown()

  if __name__ == '__main__':
    # Run world_size workers
    world_size = 2
    mp.spawn(run_process, args=(world_size,), nprocs=world_size)

.. _RFC: https://github.com/pytorch/pytorch/issues/23110