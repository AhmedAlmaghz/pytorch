.. _quantization-doc:

التمثيل الكمي
============

.. automodule:: torch.ao.quantization
.. automodule:: torch.ao.quantization.fx

.. تحذير ::
     تعتبر التكميم في مرحلة البيتا وقد يطرأ عليها تغييرات.

مقدمة في التكميم
------------

يشير التكميم إلى تقنيات لأداء الحسابات وتخزين
الموترات في عرض نطاق ترددي أقل من دقة النقطة العائمة. ينفذ النموذج المكّمي بعض العمليات أو كلها على موترات ذات دقة منخفضة بدلاً من
القيم ذات الدقة الكاملة (ذات النقطة العائمة). يسمح هذا بتمثيل نموذج أكثر إحكاما واستخدام
عمليات متجه عالية الأداء على العديد من منصات الأجهزة.
يدعم PyTorch تكميم INT8 مقارنة بنماذج FP32 النموذجية مما يسمح
انخفاض 4x في حجم النموذج وانخفاض 4x في متطلبات عرض النطاق الترددي للذاكرة.
عادة ما يكون الدعم الثابت لحسابات INT8 أسرع بمقدار مرتين إلى 4 مرات مقارنة بحساب FP32.
التكميم هو أساسا تقنية ل
تسريع الاستدلال ويتم دعم تمريرة للأمام فقط لمشغلي التكميم.

يدعم PyTorch عدة طرق لتمثيل نموذج التعلم العميق. في
معظم الحالات، يتم تدريب النموذج في FP32 ثم يتم تحويل النموذج إلى
INT8. بالإضافة إلى ذلك، يدعم PyTorch أيضًا التدريب على دراية التكميم، والذي
نموذج أخطاء التكميم في كل من التمريرات الأمامية والخلفية باستخدام
وحدات التكميم المزيفة. لاحظ أن الحساب بأكمله يتم تنفيذه في
النقطة العائمة. في نهاية التدريب على دراية التكميم، يوفر PyTorch
وظائف التحويل لتحويل النموذج المدرب إلى دقة أقل.

على مستوى أقل، يوفر PyTorch طريقة لتمثيل الموترات المكّمة
وأداء العمليات باستخدامها. يمكن استخدامها لبناء نماذج مباشرة
التي تؤدي كل أو جزء من الحساب في دقة أقل. يتم توفير واجهات برمجة التطبيقات عالية المستوى
التي تتضمن سير عمل نموذجي لتحويل نموذج FP32 إلى
دقة أقل مع الحد الأدنى من فقدان الدقة.

ملخص واجهة برمجة التطبيقات للتكميم
-------------------------

يوفر PyTorch ثلاث طرق مختلفة للتحويل الكمي: التحويل الكمي في الوضع الفوري، والتحويل الكمي في وضع الرسم البياني FX (في طور الصيانة)، والتحويل الكمي لتصدير PyTorch 2.

التحويل الكمي في الوضع الفوري هو ميزة تجريبية. يجب على المستخدم القيام بالدمج وتحديد مكان حدوث التحويل الكمي وإلغاء التحويل الكمي يدويًا، كما أنه يدعم الوحدات فقط وليس الوظائف.

التحويل الكمي في وضع الرسم البياني FX هو سير عمل التحويل الكمي الآلي في PyTorch، وهو حاليًا ميزة تجريبية، وهو في وضع الصيانة منذ أن أصبح لدينا التحويل الكمي لتصدير PyTorch 2. إنه يحسن التحويل الكمي في الوضع الفوري من خلال إضافة دعم للوظائف وتلقائية عملية التحويل الكمي، على الرغم من أن الأشخاص قد يحتاجون إلى إعادة هيكلة النموذج لجعله متوافقًا مع التحويل الكمي في وضع الرسم البياني FX (قابل للتتبع بشكل رمزي باستخدام "torch.fx"). لاحظ أن التحويل الكمي في وضع الرسم البياني FX لا يُتوقع أن يعمل على نماذج عشوائية لأن النموذج قد لا يكون قابلًا للتتبع بشكل رمزي، حيث سنقوم بدمجه في مكتبات المجال مثل torchvision وسيكون المستخدمون قادرين على تحويل نماذج كمية مماثلة لتلك الموجودة في مكتبات المجال المدعومة باستخدام التحويل الكمي في وضع الرسم البياني FX. وبالنسبة للنماذج العشوائية، سنقدم مبادئ توجيهية عامة، ولكن لجعلها تعمل بالفعل، قد يحتاج المستخدمون إلى الإلمام بـ "torch.fx"، خاصةً كيفية جعل النموذج قابلًا للتتبع بشكل رمزي.

التحويل الكمي لتصدير PyTorch 2 هو سير عمل التحويل الكمي الكامل الجديد للرسم البياني، والذي تم إصداره كميزة تجريبية في PyTorch 2.1. مع PyTorch 2، ننتقل إلى حل أفضل للالتقاط الكامل للبرنامج (torch.export) لأنه يمكنه التقاط نسبة مئوية أعلى (88.8% على 14000 نموذج) من النماذج مقارنة بـ torch.fx.symbolic_trace (72.7% على 14000 نموذج)، وهو حل التقاط البرنامج المستخدم بواسطة التحويل الكمي في وضع الرسم البياني FX. لا تزال torch.export بها قيود حول بعض البنيات البرمجية Python وتتطلب مشاركة المستخدم لدعم الديناميكية في النموذج المصدر، ولكن بشكل عام، فهي تمثل تحسنًا عن حل التقاط البرنامج السابق. تم تصميم التحويل الكمي لتصدير PyTorch 2 للنماذج التي تم التقاطها بواسطة torch.export، مع مراعاة المرونة والإنتاجية لكل من مستخدمي النمذجة ومطوري backends. الميزات الرئيسية هي:

1. واجه برمجة قابلة للبرمجة لتكوين كيفية تحويل نموذج إلى صيغة كمية يمكن أن تتوسع إلى العديد من حالات الاستخدام الأخرى.
2. تجربة مستخدم مبسطة لمستخدمي النمذجة ومطوري backends حيث يحتاجون فقط إلى التفاعل مع كائن واحد (محول) للتعبير عن نية المستخدم بشأن كيفية تحويل نموذج ما والدعم الخلفي.
3. تمثيل مرجعي اختياري لنموذج كمي يمكنه تمثيل الحسابات الكمية باستخدام العمليات الصحيحة التي ترتبط ارتباطًا وثيقًا بالحسابات الكمية الفعلية التي تحدث في الأجهزة.

يُشجع المستخدمون الجدد للتحويل الكمي على تجربة التحويل الكمي لتصدير PyTorch 2 أولاً، وإذا لم يعمل بشكل جيد، فيمكن للمستخدم تجربة التحويل الكمي في الوضع الفوري.

يقارن الجدول التالي الاختلافات بين التحويل الكمي في الوضع الفوري، والتحويل الكمي في وضع الرسم البياني FX، والتحويل الكمي لتصدير PyTorch 2:

+-----------------+-------------------+-------------------+-------------------------+
|                 |التحويل الكمي في   |التحويل الكمي في   |التحويل الكمي لتصدير     |
|                 |الوضع الفوري       |وضع الرسم البياني   |PyTorch 2                |
|                 |                   |FX                  |                         |
+-----------------+-------------------+-------------------+-------------------------+
|الإصدار          |تجريبي            |تجريبي             |تجريبي                   |
|الحالة           |                   |(صيانة)            |                         |
+-----------------+-------------------+-------------------+-------------------------+
|دمج المشغل      |يدوي              |تلقائي            |تلقائي                   |
|                 |                   |                   |                         |
+-----------------+-------------------+-------------------+-------------------------+
|وضع التحويل/    |يدوي              |تلقائي            |تلقائي                   |
|إلغاء التحويل    |                   |                   |                         |
|الكمي            |                   |                   |                         |
+-----------------+-------------------+-------------------+-------------------------+
|الوحدات الكمية   |مدعوم             |مدعوم              |مدعوم                    |
|                 |                   |                   |                         |
+-----------------+-------------------+-------------------+-------------------------+
|الوظائف/عمليات  |يدوي              |تلقائي            |مدعوم                    |
|Torch الكمية     |                   |                   |                         |
+-----------------+-------------------+-------------------+-------------------------+
|الدعم للتخصيص   |دعم محدود        |مدعوم بالكامل      |مدعوم بالكامل             |
|                 |                   |                   |                         |
+-----------------+-------------------+-------------------+-------------------------+
|وضع التحويل الكمي|التحويل الكمي بعد  |التحويل الكمي بعد  |محدد بواسطة backend     |
|الدعم           |التدريب: التحويل   |التدريب: التحويل   |الخاص                     |
|                 |الكمي الثابت،      |الكمي الثابت،      |                         |
|                 |التحويل الكمي      |التحويل الكمي      |                         |
|                 |الديناميكي،        |الديناميكي،        |                         |
|                 |الوزن فقط         |الوزن فقط         |                         |
|                 |                   |                   |                         |
|                 |التحويل الكمي      |التحويل الكمي      |                         |
|                 |الواعي بالتدريب:  |الواعي بالتدريب:  |                         |
|                 |ثابت              |ثابت              |                         |
+-----------------+-------------------+-------------------+-------------------------+
|نوع النموذج      |torch.nn.Module   |torch.nn.Module   |torch.fx.GraphModule    |
|الدخل/الخرج      |                   |(قد يحتاج إلى بعض |(تم التقاطه بواسطة     |
|                 |                   |إعادة الهيكلة     |torch.export)           |
|                 |                   |لجعل النموذج      |                         |
|                 |                   |متوافقًا مع التحويل|                         |
|                 |                   |الكمي في وضع الرسم|                         |
|                 |                   |البياني FX         |                         |
+-----------------+-------------------+-------------------+-------------------------+



هناك ثلاثة أنواع من التحويل الكمي المدعومة:

1. التحويل الكمي الديناميكي (يتم تحويل الأوزان مع الاحتفاظ بالتنشيطات في النقطة العائمة وتحويلها للحساب)
2. التحويل الكمي الثابت (يتم تحويل الأوزان والتنشيطات، ويتم طلب المعايرة بعد التدريب)
3. التحويل الكمي الثابت الواعي بالتدريب (يتم تحويل الأوزان والتنشيطات، ويتم نمذجة الأرقام الكمية أثناء التدريب)

يرجى الاطلاع على منشور المدونة الخاص بنا "مقدمة عن التحويل الكمي في PyTorch" للحصول على نظرة عامة أكثر شمولاً عن المقايضات بين أنواع التحويل الكمي هذه.

تختلف تغطية المشغل بين التحويل الكمي الديناميكي والثابت ويتم التقاطها في الجدول أدناه.

+---------------------------+-------------------+--------------------+
|                           |التحويل الكمي      |التحويل الكمي       |
|                           |الثابت            |الديناميكي          |
+---------------------------+-------------------+--------------------+
| | nn.Linear               | | نعم              | | نعم               |
| | nn.Conv1d/2d/3d         | | نعم              | | لا                |
+---------------------------+-------------------+--------------------+
| | nn.LSTM                 | | نعم (من خلال     | | نعم               |
| |                         | | وحدات مخصصة)   | |                   |
| | nn.GRU                  | | لا              | | نعم               |
+---------------------------+-------------------+--------------------+
| | nn.RNNCell              | | لا              | | نعم               |
| | nn.GRUCell              | | لا              | | نعم               |
| | nn.LSTMCell             | | لا              | | نعم               |
+---------------------------+-------------------+--------------------+
|nn.EmbeddingBag            | نعم (التنشيطات    |                    |
|                           |في fp32)          | نعم                |
+---------------------------+-------------------+--------------------+
|nn.Embedding               | نعم                | نعم                |
+---------------------------+-------------------+--------------------+
| nn.MultiheadAttention     | نعم (من خلال      | غير مدعوم         |
|                           |وحدات مخصصة)     |                    |
+---------------------------+-------------------+--------------------+
| التنشيطات               | مدعوم بشكل واسع  | دون تغيير،         |
|                           |                   | تظل الحسابات      |
|                           |                   | في fp32            |
+---------------------------+-------------------+--------------------+


التحويل الكمي في الوضع الفوري
^^^^^^^^^^^^^^^^^^^^^^^
للحصول على مقدمة عامة عن سير عمل التحويل الكمي، بما في ذلك الأنواع المختلفة من التحويل الكمي، يرجى الاطلاع على "سير عمل التحويل الكمي العام"_.

التحويل الكمي الديناميكي بعد التدريب
~~~~~~~~~~~~~~~~~~~~~~~~~

هذا هو أبسط أشكال التحويل الكمي للتطبيق حيث يتم تحويل الأوزان مسبقًا، ولكن يتم تحويل التنشيطات ديناميكيًا أثناء الاستدلال. يتم استخدام هذا الوضع في الحالات التي يهيمن فيها وقت تنفيذ النموذج على تحميل الأوزان من الذاكرة بدلاً من إجراء عمليات الضرب في المصفوفة. ينطبق هذا على نماذج LSTM و Transformer مع حجم دفعة صغير.

مخطط::

  # النموذج الأصلي
  # جميع المصفوفات والحسابات في النقطة العائمة
  previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32
                   /
  linear_weight_fp32

  # النموذج المحول كميًا ديناميكيًا
  # الأوزان الخطية والـ LSTM في int8
  previous_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32
                       /
     linear_weight_int8

مثال على واجه برمجة التطبيقات PTDQ::

  import torch

  # حدد نموذج النقطة العائمة
  class M(torch.nn.Module):
      def __init__(self):
          super().__init__()
          self.fc = torch.nn.Linear(4, 4)

      def forward(self, x):
          x = self.fc(x)
          return x

  # إنشاء مثيل للنموذج
  model_fp32 = M()
  # إنشاء مثيل للنموذج المحول كميًا
  model_int8 = torch.ao.quantization.quantize_dynamic(
      model_fp32,  # النموذج الأصلي
      {torch.nn.Linear},  # مجموعة من الطبقات لتحويلها كميًا ديناميكيًا
      dtype=torch.qint8)  # نوع البيانات المستهدف للأوزان المحولة كميًا

لمعرفة المزيد حول التحويل الكمي الديناميكي، يرجى الاطلاع على البرنامج التعليمي الخاص بنا "التحويل الكمي الديناميكي
<https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html>`_.

التحويل الكمي الثابت بعد التدريب
~~~~~~~~~~~~~~~~~~~~~~~

يُحوّل التحويل الكمي الثابت بعد التدريب (PTQ static) أوزان النموذج وتنشيطاته. يقوم بدمج التنشيطات في الطبقات السابقة حيثما أمكن ذلك. يتطلب معايرة بمجموعة بيانات تمثيلية لتحديد معلمات التحويل الكمي المثلى للتنشيطات. يتم استخدام التحويل الكمي الثابت بعد التدريب عادةً عندما تكون كل من توفير عرض النطاق الترددي للذاكرة وعمليات الحساب مهمة، مع كون الشبكات العصبية التلافيفية هي حالة الاستخدام النموذجية.

قد نحتاج إلى تعديل النموذج قبل تطبيق التحويل الكمي الثابت بعد التدريب. يرجى الاطلاع على "إعداد النموذج للتحويل الكمي الثابت في الوضع الفوري"_.

مخطط::

    # النموذج الأصلي
    # جميع المصفوفات والحسابات في النقطة العائمة
    previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32
                        /
        linear_weight_fp32

    # النموذج المحول كميًا بشكل ثابت
    # الأوزان والتنشيطات في int8
    previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8
                        /
      linear_weight_int8

مثال على واجه برمجة التطبيقات PTSQ::

  import torch
# تحديد نموذج النقطة العائمة حيث يمكن أن تكون بعض الطبقات ذات كمية ثابتة

.. class:: M(torch.nn.Module)

   .. method:: __init__(self)

      :نطاق: M

      إنشاء مثيل للطبقات.

      .. code-block:: python

         super().__init__()
         # QuantStub يحول المنسوجات من نقطة عائمة إلى كمية
         self.quant = torch.ao.quantization.QuantStub()
         self.conv = torch.nn.Conv2d(1, 1, 1)
         self.relu = torch.nn.ReLU()
         # DeQuantStub يحول المنسوجات من كمية إلى نقطة عائمة
         self.dequant = torch.ao.quantization.DeQuantStub()

   .. method:: forward(self, x)

      :نطاق: M

      تعريف الحسابات الأمامية.

      .. code-block:: python

         # حدد يدويًا المكان الذي سيتم فيه تحويل المنسوجات من نقطة عائمة إلى كمية
         # في النموذج الكمي
         x = self.quant(x)
         x = self.conv(x)
         x = self.relu(x)
         # حدد يدويًا المكان الذي سيتم فيه تحويل المنسوجات من كمية
         # إلى نقطة عائمة في النموذج الكمي
         x = self.dequant(x)
         return x

إنشاء مثيل للنموذج::

   model_fp32 = M()

يجب تعيين النموذج إلى وضع التقييم للمنطق الكمي الثابت للعمل::

   model_fp32.eval()

قم بتعليق تكوين qconfig العالمي، والذي يحتوي على معلومات حول نوع المراقبين المرفقين. استخدم "x86" للتنبؤ بالخادم و"qnnpack" للتنبؤ المحمول. يمكن أيضًا تحديد تكوينات الكم الأخرى مثل اختيار الكم المتماثل أو غير المتماثل وتقنيات المعايرة MinMax أو L2Norm هنا.

ملاحظة: لا يزال "fbgemm" القديم متاحًا ولكن "x86" هو الافتراضي الموصى به للتنبؤ بالخادم.

.. code-block:: python

   # model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')
   model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')

دمج التنشيطات في الطبقات السابقة، حيثما ينطبق ذلك.

يحتاج هذا إلى أن يتم يدويًا اعتمادًا على بنية النموذج.

تشمل عمليات الدمج الشائعة "conv + relu" و"conv + batchnorm + relu"::

   model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv', 'relu']])

إعداد النموذج للكمية الثابتة. يقوم هذا بإدراج المراقبين في النموذج الذين سيراقبون المنسوجات التنشيط أثناء المعايرة::

   model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)

معايرة النموذج المعد لتحديد معلمات الكم للتنشيطات

في الإعداد الواقعي، سيتم إجراء المعايرة باستخدام مجموعة بيانات تمثيلية::

   input_fp32 = torch.randn(4, 1, 4, 4)
   model_fp32_prepared(input_fp32)

تحويل النموذج المراقب إلى نموذج كمي. يقوم هذا بالعديد من الأشياء:

- الكميات الأوزان
- يحسب ويخزن قيمة النطاق والانحياز التي سيتم استخدامها مع كل منسوج التنشيط
- استبدل المشغلين الرئيسيين بتنفيذ كمي::

   model_int8 = torch.ao.quantization.convert(model_fp32_prepared)

تشغيل النموذج، ستحدث الحسابات ذات الصلة في int8::

   res = model_int8(input_fp32)

لمعرفة المزيد حول الكم الثابت، يرجى الاطلاع على `التدريب الثابت للكمية
<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_.

تدريب الكم على الوعي بالكمية للكمية الثابتة
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

يحدد تدريب الوعي بالكمية (QAT) آثار الكم أثناء التدريب
السماح بمزيد من الدقة مقارنة بطرق الكم الأخرى. يمكننا القيام بـ QAT للكمية الثابتة أو الديناميكية أو الوزن فقط. أثناء
التدريب، تتم جميع الحسابات في نقطة عائمة، مع وحدات fake_quant التي
نموذج آثار الكم عن طريق القطع والتقريب لمحاكاة
آثار INT8. بعد تحويل النموذج، يتم تكميم الأوزان والتنشيطات، ويتم دمج التنشيطات في الطبقة السابقة
حيثما أمكن. يتم استخدامه بشكل شائع مع CNNs ويعطي دقة أعلى
مقارنة بالكمية الثابتة.

قد نحتاج إلى تعديل النموذج قبل تطبيق الكم الثابت بعد التدريب. يرجى الاطلاع على `إعداد النموذج للكمية الثابتة لوضع Eager`_.

مخطط::

   # النموذج الأصلي
   # جميع المنسوجات والحسابات في نقطة عائمة
   previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32
                             /
      linear_weight_fp32

   # نموذج مع fake_quants لمحاكاة الكميات أثناء التدريب
   previous_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32
                                  /
      linear_weight_fp32 -- fq

   # نموذج كمي
   # الأوزان والتنشيطات في int8
   previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8
                        /
      linear_weight_int8

مثال على واجهة برمجة تطبيقات QAT::

   import torch

   # تحديد نموذج نقطة عائمة حيث يمكن أن تستفيد بعض الطبقات من QAT
   .. class:: M(torch.nn.Module)

      .. method:: __init__(self)

         :نطاق: M

         إنشاء مثيل للطبقات.

         .. code-block:: python

            super().__init__()
            # QuantStub يحول المنسوجات من نقطة عائمة إلى كمية
            self.quant = torch.ao.quantization.QuantStub()
            self.conv = torch.nn.Conv2d(1, 1, 1)
            self.bn = torch.nn.BatchNorm2d(1)
            self.relu = torch.nn.ReLU()
            # DeQuantStub يحول المنسوجات من كمية إلى نقطة عائمة
            self.dequant = torch.ao.quantization.DeQuantStub()

      .. method:: forward(self, x)

         :نطاق: M

         تعريف الحسابات الأمامية.

         .. code-block:: python

            x = self.quant(x)
            x = self.conv(x)
            x = self.bn(x)
            x = self.relu(x)
            x = self.dequant(x)
            return x

   # إنشاء مثيل للنموذج::

      model_fp32 = M()

   يجب تعيين النموذج إلى التقييم للاندماج::

      model_fp32.eval()

   قم بتعليق تكوين qconfig العالمي، والذي يحتوي على معلومات حول نوع المراقبين المرفقين. استخدم "x86" للتنبؤ بالخادم و"qnnpack" للتنبؤ المحمول. يمكن أيضًا تحديد تكوينات الكم الأخرى مثل اختيار الكم المتماثل أو غير المتماثل وتقنيات المعايرة MinMax أو L2Norm هنا.

   ملاحظة: لا يزال "fbgemm" القديم متاحًا ولكن "x86" هو الافتراضي الموصى به للتنبؤ بالخادم.

   .. code-block:: python

      # model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')
      model_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')

   دمج التنشيطات في الطبقات السابقة، حيثما ينطبق ذلك

   يحتاج هذا إلى أن يتم يدويًا اعتمادًا على بنية النموذج

   .. code-block:: python

      model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32,
          [['conv', 'bn', 'relu']])

   إعداد النموذج لـ QAT. يقوم هذا بإدراج المراقبين والfake_quants في
   # يحتاج النموذج إلى أن يكون في وضع التدريب للعمل المنطقي لـ QAT
   # النموذج الذي سيراقب وزن المنسوجات والتنشيط أثناء المعايرة.

   .. code-block:: python

      model_fp32_prepared = torch.ao.quantization.prepare_qat(model_fp32_fused.train())

   # تشغيل حلقة التدريب (غير موضحة)
   # حلقة التدريب (غير موضحة)

   .. code-block:: python

      # Convert the observed model to a quantized model. This does several things:
      # quantizes the weights, computes and stores the scale and bias value to be
      # used with each activation tensor, fuses modules where appropriate,
      # and replaces key operators with quantized implementations.
      model_fp32_prepared.eval()
      model_int8 = torch.ao.quantization.convert(model_fp32_prepared)

   # تشغيل النموذج، ستحدث الحسابات ذات الصلة في int8::

      res = model_int8(input_fp32)

لمعرفة المزيد حول تدريب الوعي بالكمية، يرجى الاطلاع على `QAT
التدريب
<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_.

إعداد النموذج للكمية الثابتة لوضع Eager
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

من الضروري حاليًا إجراء بعض التعديلات على تعريف النموذج
قبل الكم في وضع Eager. ويرجع ذلك إلى أن الكم يعمل حاليًا على أساس وحدة نمطية
بواسطة وحدة نمطية الأساس. على وجه التحديد، بالنسبة لجميع تقنيات الكم، يحتاج المستخدم إلى:

1. تحويل أي عمليات تتطلب إعادة الكم (ولها بالتالي
   معلمات إضافية) من الدوال إلى شكل الوحدة النمطية (على سبيل المثال،
   استخدام ``torch.nn.ReLU`` بدلاً من ``torch.nn.functional.relu``).
2. تحديد أجزاء النموذج التي تحتاج إلى الكم إما عن طريق تعيين
   سمات ``.qconfig`` على الوحدات الفرعية أو عن طريق تحديد ``qconfig_mapping``.
   على سبيل المثال، يعني تعيين ``model.conv1.qconfig = None`` أن
   لن يتم تكميم طبقة "model.conv"، وتعيين
   ``model.linear1.qconfig = custom_qconfig`` يعني أن إعدادات الكم لـ ``model.linear1``
   سيتم استخدام "custom_qconfig" بدلاً من qconfig العالمي.

بالنسبة لتقنيات الكم الثابت التي تقوم بكمية التنشيطات، يحتاج المستخدم
للقيام بما يلي بالإضافة إلى ذلك:

1. حدد المكان الذي يتم فيه تكميم التنشيطات وإلغاء الكم. يتم ذلك باستخدام
   :class: `~ torch.ao.quantization.QuantStub` و
   :class: `~ torch.ao.quantization.DeQuantStub` الوحدات.
2. استخدم :class: `~ torch.ao.nn.quantized.FloatFunctional` لتغليف عمليات المنسوج التي
   تتطلب معالجة خاصة للكم في الوحدات. أمثلة
   العمليات مثل "add" و"cat" التي تتطلب معالجة خاصة لتحديد
   معلمات الكم لإخراج.
3. دمج الوحدات: قم بدمج العمليات/الوحدات في وحدة نمطية واحدة للحصول على
   دقة وأداء أعلى. يتم ذلك باستخدام
   :func: `~ torch.ao.quantization.fuse_modules.fuse_modules` API، والذي يأخذ قوائم الوحدات
   ليتم دمجها. نحن ندعم حاليًا عمليات الدمج التالية:
   [Conv، Relu]، [Conv، BatchNorm]، [Conv، BatchNorm، Relu]، [Linear، Relu]

(النموذج الأولي - وضع الصيانة) وضع كمية الرسم البياني FX
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

هناك أنواع متعددة من الكم في الكم بعد التدريب (الوزن فقط، والديناميكي والثابت) ويتم التكوين من خلال `qconfig_mapping` (حجة وظيفة `prepare_fx`).

مثال على واجهة برمجة تطبيقات FXPTQ::

   import torch
   from torch.ao.quantization import (
     get_default_qconfig_mapping،
     get_default_qat_qconfig_mapping،
     QConfigMapping،
   )
   import torch.ao.quantization.quantize_fx as quantize_fx
   import copy

   model_fp = UserModel()

   #
   # الكم الديناميكي/الوزن فقط بعد التدريب
   #

   # نحن بحاجة إلى deepcopy إذا كنا لا نزال نريد الاحتفاظ بـ model_fp دون تغيير بعد الكم لأن واجهات برمجة تطبيقات الكم تغير نموذج الإدخال
   model_to_quantize = copy.deepcopy(model_fp)
   model_to_quantize.eval()
   qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)
   # مطلوب إدخال مثال واحد أو أكثر لتتبع النموذج
   example_inputs = (input_fp32)
   # إعداد
   model_prepared = quantize_fx.prepare_fx(model_to_quantize، qconfig_mapping، example_inputs)
   # لا يلزم المعايرة عند وجود الكم الديناميكي/الوزن فقط
   # الكم
   model_quantized = quantize_fx.convert_fx(model_prepared)

   #
   # الكم الثابت بعد التدريب
   #

   model_to_quantize = copy.deepcopy(model_fp)
   qconfig_mapping = get_default_qconfig_mapping("qnnpack")
   model_to_quantize.eval()
   # إعداد
   model_prepared = quantize_fx.prepare_fx(model_to_quantize، qconfig_mapping، example_inputs)
   # المعايرة (غير موضحة)
   # الكم
   model_quantized = quantize_fx.convert_fx(model_prepared)

   #
   # تدريب الوعي بالكمية للكمية الثابتة
   #

   model_to_quantize = copy.deepcopy(model_fp)
   qconfig_mapping = get_default_qat_qconfig_mapping("qnnpack")
   model_to_quantize.train()
   # إعداد
   model_prepared = quantize_fx.prepare_qat_fx(model_to_quantize، qconfig_mapping، example_inputs)
   # حلقة التدريب (غير موضحة)
   # الكم
   model_quantized = quantize_fx.convert_fx(model_prepared)

   #
   # الانصهار
   #
   model_to_quantize = copy.deepcopy(model_fp)
   model_fused = quantize_fx.fuse_fx(model_to_quantize)

يرجى اتباع البرامج التعليمية أدناه لمعرفة المزيد حول وضع كمية الرسم البياني FX:

- `دليل المستخدم حول استخدام وضع كمية الرسم البياني FX <https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html>`_
- `وضع الرسم البياني FX بعد التدريب الكمي الثابت <https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html>`_
- `وضع الرسم البياني FX بعد التدريب الديناميكي الكمي <https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_dynamic.html>`_

(النموذج الأولي) PyTorch 2 تصدير الكم
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
مثال على واجهة برمجة التطبيقات::

   import torch
   from torch.ao.quantization.quantize_pt2e import prepare_pt2e
   from torch._export import capture_pre_autograd_graph
   from torch.ao.quantization.quantizer import (
       XNNPACKQuantizer،
       get_symmetric_quantization_config،
   )
.. class:: M(torch.nn.Module):

   قم بتعريف فئة باسم "M" كنموذج وحدة نمطية من PyTorch.

   .. method:: __init__(self):

      قم بتهيئة الكائن M.

      :param self: كائن من النوع M.

   .. method:: forward(self, x):

      تنفيذ التقديم إلى الأمام للشبكة العصبية.

      :param self: كائن من النوع M.
      :param x: إدخال طبقة الخطية.
      :return: إخراج طبقة الخطية.

   # قم بتهيئة نموذج بنقطة عائمة
   float_model = M().eval()

   # حدد دالة المعايرة
   def calibrate(model, data_loader):
      ضع النموذج في وضع التقييم
      مع تعطيل الاشتقاق
      قم بالمرور عبر البيانات وطبق النموذج

      :param model: نموذج PyTorch الذي سيتم معايرته.
      :param data_loader: كائن DataLoader لتغذية البيانات إلى النموذج.

   # الخطوة 1. التقاط البرنامج
   # ملاحظة: سيتم تحديث هذا الـ API إلى torch.export API في المستقبل، ولكن النتيجة التي تم التقاطها
   # يجب أن تبقى كما هي
   m = capture_pre_autograd_graph(m, *example_inputs)
   # نحصل على نموذج مع عمليات aten

   # الخطوة 2. التكميم
   # سيقوم مطورو backend بكتابة برامج التكميم الخاصة بهم وتعريض الأساليب للسماح
   # للمستخدمين بالتعبير عن كيفية قيامهم بالتكميم للنموذج
   quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())
   # أو prepare_qat_pt2e للتدريب على الوعي بالتكميم
   m = prepare_pt2e(m, quantizer)

   # تشغيل المعايرة
   # calibrate(m, sample_inference_data)
   m = convert_pt2e(m)

   # الخطوة 3. الخفض
   # خفض إلى backend المستهدف

يرجى اتباع هذه البرامج التعليمية للبدء في تكميم PyTorch 2 Export:

نمذجة المستخدمين:

- `تكميم PyTorch 2 Export Post Training <https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html>`_
- `تكميم PyTorch 2 Export Post Training مع Backend X86 من خلال Inductor <https://pytorch.org/tutorials/prototype/pt2e_quant_ptq_x86_inductor.html>`_
- `تدريب الوعي بالتكميم في PyTorch 2 Export <https://pytorch.org/tutorials/prototype/pt2e_quant_qat.html>`_

مطورو backends (يرجى الاطلاع على جميع وثائق نمذجة المستخدمين أيضًا):

- `كيفية كتابة برنامج تكميم لـ PyTorch 2 Export Quantization <https://pytorch.org/tutorials/prototype/pt2e_quantizer.html>`_

كومة التكميم
----------
التخطين هو عملية تحويل نموذج النقطة العائمة إلى نموذج مخطط. لذلك، يمكن تقسيم مجموعة التخطين على مستوى عالٍ إلى جزأين: 1). اللبنات الأساسية أو التجريدات لنموذج مخطط 2). اللبنات الأساسية أو التجريدات لتدفق التخطين الذي يحول نموذج نقطة عائمة إلى نموذج مخطط

النموذج المخطط
^^^^^^^^^^^
المخطط Tensor
~~~~~~~~~~~~~~~
لإجراء التخطين في PyTorch، نحتاج إلى القدرة على تمثيل البيانات المخططة في المخططات. يسمح Tensor المخطط بتخزين البيانات المخططة (الممثلة كـ int8/uint8/int32) جنبًا إلى جنب مع معلمات التخطين مثل المقياس ونقطة الصفر. تسمح المخططات Tensor بالعديد من العمليات المفيدة مما يجعل الحساب المخطط سهلاً، بالإضافة إلى السماح بتهيئة البيانات بتنسيق مخطط.

يدعم PyTorch التخطين المتماثل وغير المتماثل لكل من Tensor والقناة. يعني لكل Tensor أن جميع القيم داخل المخطط يتم تخطيطها بنفس الطريقة وبنفس معلمات التخطين. يعني لكل قناة أنه بالنسبة لكل بُعد، عادةً بُعد القناة للمخطط، يتم تخطيط القيم في المخطط بمعلمات تخطيط مختلفة. يسمح ذلك بخطأ أقل في تحويل المخططات إلى قيم مخططة حيث أن القيم الشاذة ستؤثر فقط على القناة التي توجد فيها، بدلاً من المخطط بأكمله.

يتم تنفيذ الخريطة من خلال تحويل المخططات العائمة باستخدام المعادلة التالية:

.. image:: math-quantizer-equation.png
   :width: 40%

لاحظ أننا نتأكد من أن الصفر في النقطة العائمة يتم تمثيله بدون خطأ بعد التخطين، مما يضمن أن العمليات مثل الحشو لا تسبب خطأ إضافيًا في التخطين.

فيما يلي بعض السمات الرئيسية للمخطط Tensor:

* QScheme (torch.qscheme): enum الذي يحدد طريقة تخطيط المخطط

  * torch.per_tensor_affine
  * torch.per_tensor_symmetric
  * torch.per_channel_affine
  * torch.per_channel_symmetric

* dtype (torch.dtype): نوع البيانات للمخطط Tensor

  * torch.quint8
  * torch.qint8
  * torch.qint32
  * torch.float16

* معلمات التخطين (تختلف حسب QScheme): معلمات لطريقة التخطين المختارة

  * سيكون لدى torch.per_tensor_affine معلمات التخطين التالية

    * المقياس (float)
    * zero_point (int)
  * سيكون لدى torch.per_channel_affine معلمات التخطين التالية

    * per_channel_scales (قائمة من float)
    * per_channel_zero_points (قائمة من int)
    * axis (int)

التخطيط وإلغاء التخطين
~~~~~~~~~~~~~~~~~
يكون الإدخال والإخراج للنموذج عبارة عن مخططات عائمة، ولكن التنشيطات في النموذج المخطط تكون مخططة، لذلك نحتاج إلى مشغلات لتحويلها من النقطة العائمة إلى المخططات وبالعكس.

* التخطين (float -> quantized)

  * torch.quantize_per_tensor(x، scale، zero_point، dtype)
  * torch.quantize_per_channel(x، scales، zero_points، axis، dtype)
  * torch.quantize_per_tensor_dynamic(x، dtype، reduce_range)
  * to(torch.float16)

* إلغاء التخطين (quantized -> float)

  * quantized_tensor.dequantize() - يؤدي استدعاء إلغاء التخطين على Tensor من النوع torch.float16 إلى تحويل المخطط مرة أخرى إلى النوع torch.float
  * torch.dequantize(x)

المشغل المخطط/الوحدات النمطية
~~~~~~~~~~~~~~~~~~~~~~~
* المشغل المخطط هو المشغل الذي يأخذ المخطط Tensor كإدخال، وينتج مخطط Tensor.
* الوحدات النمطية المخططة هي وحدات PyTorch التي تقوم بعمليات مخططة. يتم تعريفها عادة للعمليات ذات الأوزان مثل العمليات الخطية والمتناقصة.

محرك التخطين
~~~~~~~~~~~~~~~~
عندما يتم تنفيذ نموذج مخطط، يحدد qengine (torch.backends.quantized.engine) backend الذي سيتم استخدامه للتنفيذ. من المهم التأكد من أن qengine متوافق مع النموذج المخطط من حيث نطاق القيم للتنشيطات والأوزان المخططة.

تدفق التخطين
^^^^^^^^^^

المراقب و FakeQuantize
~~~~~~~~~~~~~~~~~~~~~~~
* المراقب هو وحدات PyTorch المستخدمة لما يلي:

  * جمع إحصائيات المخطط مثل القيمة الدنيا والقيمة القصوى للمخطط الذي يمر عبر المراقب
  * حساب معلمات التخطين بناءً على إحصائيات المخطط التي تم جمعها
* FakeQuantize هي وحدات PyTorch المستخدمة لما يلي:

  * محاكاة التخطين (أداء التخطين/إلغاء التخطين) لمخطط في الشبكة
  * يمكنه حساب معلمات التخطين بناءً على الإحصائيات التي تم جمعها من المراقب، أو يمكنه تعلم معلمات التخطين أيضًا

QConfig
~~~~~~~~~
* QConfig هو namedtuple من فئة المراقب أو FakeQuantize Module التي يمكن تكوينها باستخدام qscheme وdtype وما إلى ذلك. يتم استخدامه لتكوين كيفية مراقبة المشغل

  * تكوين التخطين للمشغل/الوحدة النمطية

    * أنواع مختلفة من المراقب/FakeQuantize
    * dtype
    * qscheme
    * quant_min/quant_max: يمكن استخدامها لمحاكاة مخططات ذات دقة أقل
  * يدعم حاليًا التكوين للتنشيط والوزن
  * نقوم بإدراج مراقب الإدخال/الوزن/الإخراج بناءً على qconfig الذي تم تكوينه لمشغل أو وحدة نمطية معينة

تدفق التخطين العام
~~~~~~~~~~~~~~
بشكل عام، يكون التدفق على النحو التالي

* الإعداد

  * إدراج وحدات المراقب/FakeQuantize بناءً على qconfig المحدد من قبل المستخدم

* المعايرة/التدريب (اعتمادًا على تخطيط ما بعد التدريب أو التدريب على التخطين)

  * السماح للمراقبين بجمع الإحصائيات أو وحدات FakeQuantize لتعلم معلمات التخطين

* تحويل

  * تحويل نموذج معاير/مدرب إلى نموذج مخطط

هناك طرق مختلفة للتخطين، يمكن تصنيفها بطريقتين:

من حيث المكان الذي نطبق فيه تدفق التخطين، لدينا:

1. تخطيط ما بعد التدريب (تطبيق التخطين بعد التدريب، يتم حساب معلمات التخطين بناءً على بيانات المعايرة)
2. التدريب على التخطين (محاكاة التخطين أثناء التدريب حتى يمكن تعلم معلمات التخطين مع النموذج باستخدام بيانات التدريب)

ومن حيث كيفية تخطيطنا للمشغلين، يمكننا أن نستخدم:

- تخطيط الوزن فقط (يتم تخطيط الوزن فقط بشكل ثابت)
- التخطين الديناميكي (يتم تخطيط الوزن بشكل ثابت، ويتم تخطيط التنشيط ديناميكيًا)
- التخطين الثابت (يتم تخطيط كل من الوزن والتنشيطات بشكل ثابت)

يمكننا خلط طرق مختلفة لتخطيط المشغلين في نفس تدفق التخطين. على سبيل المثال، يمكننا أن يكون لدينا تخطيط ما بعد التدريب الذي يحتوي على مشغلين مخططين بشكل ثابت وديناميكي.

مصفوفة دعم التخطين
هذا النص يشرح أوضاع التقريب المدعومة في PyTorch، وهو إطار عمل للتعلم الآني.

--------------------------------------
دعم وضع التقريب
^^^^^^^^^^^^^^^

+-----------------------------+------------------------------------------------------+----------------+----------------+------------+-----------------+
|                             | وضع التقريب                                          | متطلبات مجموعة البيانات         | أفضل ما يناسب لـ | الدقة   | ملاحظات      |
|                             | وضع                                                  |                            |                |            |                 |
+-----------------------------+---------------------------------+--------------------+----------------+----------------+------------+-----------------+
| التقريب بعد التدريب   | ديناميكي/تقريب الأوزان فقط | التنشيط          | لا شيء            | LSTM، MLP،      | جيد        | سهل الاستخدام،     |
|                             |                                 | ديناميكيًا         |                | Embedding،      |            | قريب من التقريب الثابت  |
|                             |                                 | (fp16،    |                | Transformer     |            | عند أداء العمليات الحسابية أو |
|                             |                                 | int8) أو لا        |                |                |            | الذاكرة المحدودة بسبب الأوزان |
|                             |                                 | يتم تقريبها بشكل ثابت (fp16، int8، in4)   |                |                |            |
|                             +---------------------------------+--------------------+----------------+----------------+------------+-----------------+
|                             | التقريب الثابت              | التنشيط والأوزان يتم تقريبهما بشكل ثابت  | معايرة مجموعة البيانات         | CNN             | جيد        | يوفر أفضل أداء، وقد يكون له   |
|                             |                                 | (int8)    |                |                |            | تأثير كبير على الدقة، وهو جيد   |
|                             |                                 |                    |                |                |            | للأجهزة التي تدعم فقط |
|                             |                                 |                    |                |                |            | العمليات الحسابية int8 |
+-----------------------------+---------------------------------+--------------------+----------------+----------------+------------+-----------------+
|                             | التقريب الديناميكي             | التنشيط والوزن مزيفان   | ضبط دقيق لمجموعة البيانات         | MLP، Embedding  | الأفضل        | دعم محدود  |
|                             |                                 | يتم تقريبهما           |                |                |            | في الوقت الحالي          |
|                             +---------------------------------+--------------------+----------------+----------------+------------+-----------------+
|                             | التقريب الثابت              | التنشيط والوزن مزيفان   | ضبط دقيق لمجموعة البيانات         | CNN، MLP،       | الأفضل        | يستخدم عادةً   |
|                             |                                 | يتم تقريبهما           |                | Embedding       |            | عندما يؤدي التقريب الثابت      |
|                             |                                 |                    |                |                |            | إلى انخفاض الدقة، ويستخدم لإغلاق فجوة الدقة |
| تقريب التدريب الواعي         |                                 |                    |                |                |            |                 |
+-----------------------------+---------------------------------+--------------------+----------------+----------------+------------+-----------------+

يرجى الاطلاع على منشور مدونتنا "مقدمة عن التقريب في بايتورتش
<https://pytorch.org/blog/introduction-to-quantization-on-pytorch/>`_ للحصول على نظرة شاملة حول المزايا والعيوب بين أنواع التقريب هذه.

دعم تدفق التقريب
^^^^^^^^^^^^^^^^

يوفر PyTorch وضعين للتقريب: وضع التقريب Eager ووضع التقريب FX Graph.

وضع التقريب Eager هو ميزة تجريبية. يحتاج المستخدم إلى إجراء الاندماج وتحديد مكان حدوث التقريب وإلغاء التقريب يدويًا، كما أنه يدعم الوحدات فقط وليس الوظائف.

وضع التقريب FX Graph هو إطار عمل للتقريب التلقائي في PyTorch، وهو حاليًا ميزة تجريبية. إنه يحسن وضع التقريب Eager من خلال إضافة دعم للوظائف والتشغيل التلقائي لعملية التقريب، على الرغم من أنه قد يتعين على الأشخاص إعادة هيكلة النموذج لجعله متوافقًا مع وضع التقريب FX Graph (قابل للتتبع الرمزي باستخدام "torch.fx"). لاحظ أن وضع التقريب FX Graph غير متوقع للعمل على النماذج التعسفية نظرًا لأن النموذج قد لا يكون قابلًا للتتبع الرمزي، وسندمجه في مكتبات المجال مثل torchvision وسيكون المستخدمون قادرين على تقريب النماذج المماثلة للنماذج الموجودة في مكتبات المجال المدعومة باستخدام وضع التقريب FX Graph. وبالنسبة للنماذج التعسفية، سنقدم مبادئ توجيهية عامة، ولكن لجعلها تعمل بالفعل، قد يحتاج المستخدمون إلى الإلمام بـ "torch.fx"، خاصةً كيفية جعل النموذج قابلًا للتتبع الرمزي.

يُشجع المستخدمون الجدد للتقريب على تجربة وضع التقريب FX Graph أولاً، وإذا لم يعمل، يمكن للمستخدم أن يحاول اتباع المبادئ التوجيهية لـ "استخدام وضع التقريب FX Graph <https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html>`_ أو العودة إلى وضع التقريب Eager.

يقارن الجدول التالي الاختلافات بين وضعي التقريب Eager ووضع التقريب FX Graph:

+-----------------+-------------------+-------------------+
|                 | وضع التقريب Eager         | وضع التقريب FX           |
|                 |                   |                     |
|                 |                   |                     |
+-----------------+-------------------+-------------------+
| الإصدار          | تجريبي               | نموذج أولي          |
| الحالة           |                   |                   |
+-----------------+-------------------+-------------------+
| دمج المشغل         | يدوي             | تلقائي          |
|                   |                   |                   |
+-----------------+-------------------+-------------------+
| وضع التقريب/إلغاء التقريب    | يدوي             | تلقائي          |
|                   |                   |                   |
+-----------------+-------------------+-------------------+
| الوحدات          | مدعوم          | مدعوم          |
| التقريب                |                   |                   |
+-----------------+-------------------+-------------------+
| الوظائف/عمليات Torch| يدوي             | تلقائي          |
| التقريب                |                   |                   |
+-----------------+-------------------+-------------------+
| دعم التخصيص    | دعم محدود    | مدعوم بالكامل          |
|                   |                   |                   |
+-----------------+-------------------+-------------------+
| دعم وضع التقريب| التقريب بعد التدريب      | التقريب بعد التدريب      |
|                 | وضع التقريب:      | وضع التقريب:      |
|                 | ديناميكي، ثابت،   | ديناميكي، ثابت،   |
|                 | تقريب الأوزان فقط        | تقريب الأوزان فقط        |
|                 |                   |                   |
|                 | التدريب الواعي بالتقريب: | التدريب الواعي بالتقريب: |
|                 | ثابت             | ثابت             |
+-----------------+-------------------+-------------------+
| نوع نموذج الإدخال/الإخراج     | ``torch.nn.Module``| ``torch.nn.Module``|
|                 |                   | (قد يحتاج إلى بعض     |
|                 |                   | التعديلات لجعل  |
|                 |                   | النموذج          |
|                 |                   | متوافق مع وضع التقريب FX |
|                 |                   |                     |
|                 |                   | Graph Mode         |
|                 |                   |                     |
+-----------------+-------------------+-------------------+

دعم البنية الخلفية/الأجهزة
^^^^^^^^^^^^^^^^^^

+-----------------+---------------+------------+------------+------------+
| الأجهزة         | مكتبة النواة | وضع التقريب Eager  | وضع التقريب FX    | وضع التقريب|
|                 |               |                     |                     |            |
|                 |               |                         |                     |            |
+-----------------+---------------+------------+------------+------------+
| وحدة المعالجة المركزية للخادم       | fbgemm/onednn  | مدعوم                | الكل         |
|                 |               |                         | مدعوم   |
+-----------------+---------------+                         |            +
| وحدة المعالجة المركزية للهاتف المحمول       | qnnpack/xnnpack|                         |            |
|                 |               |                         |            |
+-----------------+---------------+------------+------------+------------+
| وحدة معالجة الرسوميات للخادم       | TensorRT (نموذج أولي مبكر)| لا تدعم هذا الأمر | مدعوم   | ثابت      |
|                 |               | يتطلب رسمًا       |            |            |
|                 |               |                     |            |            |
+-----------------+---------------+------------+------------+------------+

اليوم، يدعم PyTorch البنيات الخلفية التالية لتشغيل المشغلين المتقاربين بكفاءة:

* وحدات المعالجة المركزية x86 مع دعم AVX2 أو أعلى (بدون AVX2، يكون لبعض العمليات تنفيذاً غير فعال)، عبر `x86` التي تم تحسينها بواسطة `fbgemm <https://github.com/pytorch/FBGEMM>`_ و `onednn <https://github.com/oneapi-src/oneDNN>`_ (راجع التفاصيل في `RFC <https://github.com/pytorch/pytorch/issues/83888>`_)
* وحدات المعالجة المركزية ARM (توجد عادةً في الأجهزة المحمولة/المدمجة)، عبر `qnnpack <https://github.com/pytorch/pytorch/tree/main/aten/src/ATen/native/quantized/cpu/qnnpack>`_
* (نموذج أولي مبكر) دعم وحدات معالجة الرسوميات من Nvidia عبر `TensorRT <https://developer.nvidia.com/tensorrt>`_ من خلال `fx2trt` (سيتم إصداره مفتوح المصدر)


ملاحظة حول البنيات الخلفية لوحدات المعالجة المركزية الأصلية
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

نحن نعرض كل من `x86` و`qnnpack` مع نفس مشغلات PyTorch المتقاربة الأصلية، لذلك نحتاج إلى علم إضافي للتمييز بينهما. يتم اختيار التنفيذ المقابل لـ `x86` و`qnnpack` تلقائيًا بناءً على وضع البناء PyTorch، على الرغم من أن المستخدمين لديهم خيار تجاوز هذا عن طريق تعيين `torch.backends.quantization.engine` إلى `x86` أو `qnnpack`.

عند إعداد نموذج متقارب، من الضروري التأكد من أن qconfig
والمحرك المستخدم لحسابات التقريب متوافقان مع البنية الخلفية التي
سيتم تنفيذ النموذج عليها. يتحكم qconfig في نوع المراقبين المستخدمين
أثناء تمريرات التقريب. يتحكم qengine في ما إذا كان يتم استخدام وظيفة التعبئة المحددة لـ `x86` أو `qnnpack`
عند تعبئة الأوزان لوظائف
وحدات الخطي والضرب المنقط. على سبيل المثال:

الإعدادات الافتراضية لـ x86::
هذا هو النص المترجم إلى اللغة العربية بتنسيق ReStructuredText:

# تعيين qconfig لتقنية PTQ
# ملاحظة: الإصدار القديم 'fbgemm' ما زال متاحًا ولكن 'x86' هو الإعداد الافتراضي الموصى به لمعالجات x86
qconfig = torch.ao.quantization.get_default_qconfig('x86')
# أو، قم بتعيين qconfig لتقنية QAT
qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')
# قم بتعيين qengine للتحكم في ضغط الأوزان
torch.backends.quantized.engine = 'x86'

الإعدادات الافتراضية لـ qnnpack::

# قم بتعيين qconfig لتقنية PTQ
qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')
# أو، قم بتعيين qconfig لتقنية QAT
qconfig = torch.ao.quantization.get_default_qat_qconfig('qnnpack')
# قم بتعيين qengine للتحكم في ضغط الأوزان
torch.backends.quantized.engine = 'qnnpack'

دعم المشغل
^^^^^^^^

تختلف تغطية المشغل بين التقنين الديناميكي والثابت ويتم التقاطها في الجدول أدناه.
ملاحظة: بالنسبة لوضع تقنين مخطط FX، يتم أيضًا دعم الدوال الوظيفية المقابلة.

+---------------------------+-------------------+--------------------+
|                           | التقنين الثابت    | التقنين الديناميكي |
|                           |                   |                    |
+---------------------------+-------------------+--------------------+
| | nn.Linear               | | نعم              | | نعم               |
| | nn.Conv1d/2d/3d         | | نعم              | | لا                |
+---------------------------+-------------------+--------------------+
| | nn.LSTM                 | | لا               | | نعم               |
| | nn.GRU                  | | لا               | | نعم               |
+---------------------------+-------------------+--------------------+
| | nn.RNNCell              | | لا               | | نعم               |
| | nn.GRUCell              | | لا               | | نعم               |
| | nn.LSTMCell             | | لا               | | نعم               |
+---------------------------+-------------------+--------------------+
|nn.EmbeddingBag            | نعم (التنشيطات    |                    |
|                           | في fp32)          | نعم                |
+---------------------------+-------------------+--------------------+
|nn.Embedding               | نعم                | نعم                 |
+---------------------------+-------------------+--------------------+
|nn.MultiheadAttention      | غير مدعوم         | غير مدعوم          |
+---------------------------+-------------------+--------------------+
|Activations                | مدعوم على نطاق واسع| دون تغيير،         |
|                           |                   | الحسابات          |
|                           |                   | تبقى في fp32       |
+---------------------------+-------------------+--------------------+

ملاحظة: سيتم تحديث هذا بمساعدة بعض المعلومات المولدة من backend_config_dict الأصلي قريبًا.

مرجع واجهة برمجة التطبيقات للتقنين
---------------------------

تحتوي مرجع واجهة برمجة التطبيقات الكمية :doc: `<quantization-support>` على وثائق
لواجهات برمجة التطبيقات الكمية، مثل تمريرات الكم، وعمليات التنسور الكمية،
والوحدات والوظائف الكمية المدعومة.

.. toctree::
    :hidden:

    quantization-support

تكوين backend الكمي
----------------

يحتوي تكوين backend الكمي :doc: `<quantization-backend-configuration>` على وثائق
حول كيفية تكوين سير عمل الكم لمختلف backends.

.. toctree::
    :hidden:

    quantization-backend-configuration

تصحيح دقة الكم
-------------

يحتوي تصحيح دقة الكم :doc: `<quantization-accuracy-debugging>` على وثائق
حول كيفية تصحيح دقة الكم.

.. toctree::
    :hidden:

    quantization-accuracy-debugging

التخصيصات الكمية
--------------

في حين يتم توفير التطبيقات الافتراضية للمراقبين لاختيار عامل المقياس والانحياز
بناءً على بيانات tensor التي تمت ملاحظتها، يمكن للمطورين توفير وظائف الكم الخاصة بهم. يمكن تطبيق الكم بشكل انتقائي على أجزاء مختلفة
من النموذج أو تكوينها بشكل مختلف لأجزاء مختلفة من النموذج.

كما نقدم الدعم للكم لكل قناة لـ **conv1d()**، **conv2d()**،
**conv3d()** و **linear()**.

تعمل سير عمل الكم عن طريق إضافة (على سبيل المثال، إضافة المراقبين كـ
``.observer`` submodule) أو استبدال (على سبيل المثال، تحويل ``nn.Conv2d`` إلى
``nn.quantized.Conv2d``) الوحدات الفرعية في التسلسل الهرمي للوحدة النمطية للنموذج. وهذا
يعني أن النموذج يظل مثيلًا منتظمًا لـ ``nn.Module`` طوال العملية وبالتالي يمكنه العمل مع بقية واجهات برمجة تطبيقات PyTorch.

واجهة برمجة التطبيقات النمطية المخصصة للوحدات النمطية الكمية
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

يوفر كل من وضع Eager ووضع FX graph mode APIs للكم خطافًا للمستخدم
لتحديد الوحدة النمطية المحددة بالطريقة المخصصة، مع منطق المستخدم المحدد للملاحظة والكم. يحتاج المستخدم إلى تحديد ما يلي:

1. نوع Python للوحدة النمطية float32 المصدر (الموجودة في النموذج)
2. نوع Python للوحدة النمطية التي تمت ملاحظتها (يقدمها المستخدم). يجب أن تقوم هذه الوحدة النمطية بتعريف وظيفة `from_float` التي تحدد كيفية إنشاء الوحدة النمطية التي تمت ملاحظتها من الوحدة النمطية float32 الأصلية.
3. نوع Python للوحدة النمطية الكمية (يقدمها المستخدم). يجب أن تقوم هذه الوحدة النمطية بتعريف وظيفة `from_observed` التي تحدد كيفية إنشاء الوحدة النمطية الكمية من الوحدة النمطية التي تمت ملاحظتها.
4. تكوين يصف (1)، (2)، (3) أعلاه، يتم تمريره إلى واجهات برمجة تطبيقات الكم.


بعد ذلك، سيقوم الإطار بما يلي:

1. أثناء عمليات استبدال الوحدة النمطية "التحضير"، سيقوم بتحويل كل وحدة نمطية من النوع
   المحدد في (1) إلى النوع المحدد في (2)، باستخدام وظيفة `from_float` للصف
   في (2).
2. أثناء عمليات استبدال الوحدة النمطية "التحويل"، سيقوم بتحويل كل وحدة نمطية من النوع
   المحدد في (2) إلى النوع المحدد في (3)، باستخدام وظيفة `from_observed`
   من الفئة في (3).

حاليًا، هناك متطلب بأن يكون لدى `ObservedCustomModule` إخراج Tensor واحد، وسيقوم الإطار (وليس المستخدم)
بإضافة مراقب على هذا الإخراج. سيتم تخزين المراقب في إطار مفتاح `activation_post_process`
كسمة مثيل الوحدة النمطية المخصصة. قد يتم تخفيف هذه القيود في وقت لاحق.

مثال على واجهة برمجة التطبيقات المخصصة::

  import torch
  import torch.ao.nn.quantized as nnq
  from torch.ao.quantization import QConfigMapping
  import torch.ao.quantization.quantize_fx

  # original fp32 module to replace
  class CustomModule(torch.nn.Module):
      def __init__(self):
          super().__init__()
          self.linear = torch.nn.Linear(3, 3)

      def forward(self, x):
          return self.linear(x)

  # custom observed module, provided by user
  class ObservedCustomModule(torch.nn.Module):
      def __init__(self, linear):
          super().__init__()
          self.linear = linear

      def forward(self, x):
          return self.linear(x)

      @classmethod
      def from_float(cls, float_module):
          assert hasattr(float_module, 'qconfig')
          observed = cls(float_module.linear)
          observed.qconfig = float_module.qconfig
          return observed

  # custom quantized module, provided by user
  class StaticQuantCustomModule(torch.nn.Module):
      def __init__(self, linear):
          super().__init__()
          self.linear = linear

      def forward(self, x):
          return self.linear(x)

      @classmethod
      def from_observed(cls, observed_module):
          assert hasattr(observed_module, 'qconfig')
          assert hasattr(observed_module, 'activation_post_process')
          observed_module.linear.activation_post_process = \
              observed_module.activation_post_process
          quantized = cls(nnq.Linear.from_float(observed_module.linear))
          return quantized

  #
  # مثال على مكالمة واجهة برمجة التطبيقات (وضع الكم Eager)
  #

  m = torch.nn.Sequential(CustomModule()).eval()
  prepare_custom_config_dict = {
      "float_to_observed_custom_module_class": {
          CustomModule: ObservedCustomModule
      }
  }
  convert_custom_config_dict = {
      "observed_to_quantized_custom_module_class": {
          ObservedCustomModule: StaticQuantCustomModule
      }
  }
  m.qconfig = torch.ao.quantization.default_qconfig
  mp = torch.ao.quantization.prepare(
      m, prepare_custom_config_dict=prepare_custom_config_dict)
  # المعايرة (غير موضحة)
  mq = torch.ao.quantization.convert(
      mp, convert_custom_config_dict=convert_custom_config_dict)
  #
  # مثال على مكالمة واجهة برمجة التطبيقات (وضع الكم FX graph)
  #
  m = torch.nn.Sequential(CustomModule()).eval()
  qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_qconfig)
  prepare_custom_config_dict = {
      "float_to_observed_custom_module_class": {
          "static": {
              CustomModule: ObservedCustomModule,
          }
      }
  }
  convert_custom_config_dict = {
      "observed_to_quantized_custom_module_class": {
          "static": {
              ObservedCustomModule: StaticQuantCustomModule,
          }
      }
  }
  mp = torch.ao.quantization.quantize_fx.prepare_fx(
      m, qconfig_mapping, torch.randn(3,3), prepare_custom_config=prepare_custom_config_dict)
  # المعايرة (غير موضحة)
  mq = torch.ao.quantization.quantize_fx.convert_fx(
      mp, convert_custom_config=convert_custom_config_dict)

أفضل الممارسات
--------------

1. إذا كنت تستخدم backend ``x86``، فيجب علينا استخدام 7 بتات بدلاً من 8 بتات. تأكد من تقليل النطاق لـ ``quant_min``، ``quant_max``، على سبيل المثال
إذا كان ``dtype`` هو ``torch.quint8``، فتأكد من تعيين ``quant_min`` مخصص إلى ``0`` و ``quant_max`` إلى ``127`` (``255`` / ``2``)
إذا كان ``dtype`` هو ``torch.qint8``، فتأكد من تعيين ``quant_min`` مخصص إلى ``-64`` (``-128`` / ``2``) و ``quant_max`` إلى ``63`` (``127`` / ``2``)، لقد قمنا بالفعل بتعيين هذا بشكل صحيح إذا
قمت بالاتصال بوظيفة `torch.ao.quantization.get_default_qconfig(backend)` أو `torch.ao.quantization.get_default_qat_qconfig(backend)` للحصول على ``qconfig`` الافتراضي
لـ ``x86`` أو backends ``qnnpack``.

2. إذا تم تحديد backend ``onednn``، فسيتم استخدام 8 بتات للتنشيط في خريطة qconfig الافتراضية ``torch.ao.quantization.get_default_qconfig_mapping('onednn')``
و qconfig الافتراضي ``torch.ao.quantization.get_default_qconfig('onednn')``. يوصى باستخدامه على وحدات المعالجة المركزية التي تدعم تعليمات Vector Neural Network (VNNI). وإلا، قم بتعيين ``reduce_range`` إلى True لمراقب التنشيط للحصول على دقة أفضل على وحدات المعالجة المركزية التي لا تدعم VNNI.

الأسئلة الشائعة
-----------

1. كيف يمكنني إجراء الاستدلال الكمي على GPU؟:

   لا يوجد لدينا دعم رسمي لـ GPU حتى الآن، ولكن هذا مجال تطوير نشط، يمكنك العثور على مزيد من المعلومات
   `هنا <https://github.com/pytorch/pytorch/issues/87395>`_

2. أين يمكنني الحصول على دعم ONNX لنموذجي الكمي؟

   إذا واجهت أخطاءً عند تصدير النموذج (باستخدام واجهات برمجة التطبيقات الموجودة أسفل ``torch.onnx``)، فيمكنك فتح مشكلة في مستودع PyTorch. أضف بادئة إلى عنوان المشكلة بـ ``[ONNX]`` وقم بوسم المشكلة باسم ``module: onnx``.

   إذا واجهتك مشكلات مع ONNX Runtime، فقم بفتح مشكلة في `GitHub - microsoft/onnxruntime <https://github.com/microsoft/onnxruntime/issues/>`_.

3. كيف يمكنني استخدام الكم مع LSTM's؟:

   يتم دعم LSTM من خلال واجهة برمجة التطبيقات النمطية المخصصة في كل من وضع Eager ووضع FX graph mode quantization. يمكن العثور على الأمثلة في
   وضع Eager: `pytorch/test_quantized_op.py TestQuantizedOps.test_custom_module_lstm <https://github.com/pytorch/pytorch/blob/9b88dcf248e717ca6c3f8c5e11f600825547a561/test/quantization/core/test_quantized_op.py#L2782>`_
   وضع FX Graph: `pytorch/test_quantize_fx.py TestQuantizeFx.test_static_lstm <https://github.com/pytorch/pytorch/blob/9b88dcf248e717ca6c3f8c5e11f600825547a561/test/quantization/fx/test_quantize_fx.py#L4116>`_

الأخطاء الشائعة
------------

تمرير Tensor غير كمي إلى نواة كمية
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

إذا رأيت خطأً مشابهًا لما يلي::

  RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...

هذا يعني أنك تحاول تمرير Tensor غير كمي إلى نواة كمية. تتمثل إحدى طرق المعالجة الشائعة في استخدام ``torch.ao.quantization.QuantStub``
لكمي Tensor. يجب القيام بذلك يدويًا في وضع التهيئة Eager. مثال من البداية إلى النهاية::

  class M(torch.nn.Module):
      def __init__(self):
          super().__init__()
          self.quant = torch.ao.quantization.QuantStub()
          self.conv = torch.nn.Conv2d(1, 1, 1)

      def forward(self, x):
          # خلال خطوة التحويل، سيتم استبدال هذا
          # باستدعاء `quantize_per_tensor`
          x = self.quant(x)
          x = self.conv(x)
          return x

تمرير Tensor الكمي إلى نواة غير كمية
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

إذا رأيت خطأً مشابهًا لما يلي::

  RuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend.

هذا يعني أنك تحاول تمرير Tensor الكمي إلى نواة غير كمية. تتمثل إحدى طرق المعالجة الشائعة في استخدام ``torch.ao.quantization.DeQuantStub``
لفك كمية Tensor. يجب القيام بذلك يدويًا في وضع التهيئة Eager. مثال من البداية إلى النهاية::

  class M(torch.nn.Module):
      def __init__(self):
          super().__init__()
          self.quant = torch.ao.quantization.QuantStub()
          self.conv1 = torch.nn.Conv2d(1, 1, 1)
          # لن يتم كمية هذه الوحدة (راجع منطق `qconfig = None` أدناه)
          self.conv2 = torch.nn.Conv2d(1, 1, 1)
          self.dequant = torch.ao.quantization.DeQuantStub()

      def forward(self, x):
          # خلال خطوة التحويل، سيتم استبدال هذا
          # باستدعاء `quantize_per_tensor`
          x = self.quant(x)
          x = self.conv1(x)
          # خلال خطوة التحويل، سيتم استبدال هذا
          # باستدعاء `dequantize`
          x = self.dequant(x)
          x = self.conv2(x)
          return x

  m = M()
  m.qconfig = some_qconfig
  # إيقاف تشغيل الكمية لـ conv2
  m.conv2.qconfig = None

حفظ وتحميل النماذج الكمية
^^^^^^^^^^^^^^^^^^^

عند استدعاء ``torch.load`` على نموذج كمي، إذا رأيت خطأً مثل::

  AttributeError: 'LinearPackedParams' object has no attribute '_modules'

يرجع ذلك إلى أن حفظ وتحميل نموذج كمي مباشرةً باستخدام ``torch.save`` و ``torch.load``
غير مدعوم. لحفظ/تحميل النماذج الكمية، يمكن استخدام الطرق التالية:

1. حفظ/تحميل حالة النموذج الكمي dict

مثال::

  class M(torch.nn.Module):
      def __init__(self):
          super().__init__()
          self.linear = nn.Linear(5, 5)
          self.relu = nn.ReLU()

      def forward(self, x):
          x = self.linear(x)
          x = self.relu(x)
          return x

  m = M().eval()
  prepare_orig = prepare_fx(m, {'' : default_qconfig})
  prepare_orig(torch.rand(5, 5))
  quantized_orig = convert_fx(prepare_orig)

  # حفظ/تحميل باستخدام state_dict
  b = io.BytesIO()
  torch.save(quantized_orig.state_dict(), b)

  m2 = M().eval()
  prepared = prepare_fx(m2, {'' : default_qconfig})
  quantized = convert_fx(prepared)
  b.seek(0)
  quantized.load_state_dict(torch.load(b))

2. حفظ/تحميل النماذج الكمية المكتوبة نصًا باستخدام ``torch.jit.save`` و ``torch.jit.load``

مثال::

  # ملاحظة: استخدام نفس النموذج M من المثال السابق
  m = M().eval()
  prepare_orig = prepare_fx(m, {'' : default_qconfig})
  prepare_orig(torch.rand(5, 5))
  quantized_orig = convert_fx(prepare_orig)

  # حفظ/تحميل النموذج المكتوب نصًا
  scripted = torch.jit.script(quantized_orig)
  b = io.BytesIO()
  torch.jit.save(scripted, b)
  b.seek(0)
  scripted_quantized = torch.jit.load(b)

خطأ التعقب الرمزي عند استخدام وضع كمية مخطط FX
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
إن إمكانية التعقب الرمزي هي متطلب لـ `(Prototype - maintenance mode) FX Graph Mode Quantization`_، لذلك إذا قمت بتمرير نموذج PyTorch غير قابل للتعقب رمزيًا إلى `torch.ao.quantization.prepare_fx` أو `torch.ao.quantization.prepare_qat_fx`، فقد نرى خطأً مشابهًا لما يلي::

  torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow

يرجى الاطلاع على `Limitations of Symbolic Tracing <https://pytorch.org/docs/2.0/fx.html#limitations-of-symbolic-tracing>`_ و - `User Guide on Using FX Graph Mode Quantization <https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html>`_ للالتفاف حول المشكلة.
.. torch.ao يفتقر إلى التوثيق. نظرًا لأنه تمت الإشارة إلى جزء منه هنا، فقد أضفته هنا الآن.
.. إنها هنا لأغراض التتبع حتى يتم إصلاحها بشكل دائم.
.. py:module:: torch.ao
.. py:module:: torch.ao.nn
.. py:module:: torch.ao.nn.quantizable
.. py:module:: torch.ao.nn.quantizable.modules
.. py:module:: torch.ao.nn.quantized
.. py:module:: torch.ao.nn.quantized.reference
.. py:module:: torch.ao.nn.quantized.reference.modules
.. py:module:: torch.ao.nn.sparse
.. py:module:: torch.ao.nn.sparse.quantized
.. py:module:: torch.ao.nn.sparse.quantized.dynamic
.. py:module:: torch.ao.ns
.. py:module:: torch.ao.ns.fx
.. py:module:: torch.ao.quantization.backend_config
.. py:module:: torch.ao.pruning
.. py:module:: torch.ao.pruning.scheduler
.. py:module:: torch.ao.pruning.sparsifier
.. py:module:: torch.ao.nn.intrinsic.modules.fused
.. py:module:: torch.ao.nn.intrinsic.qat.modules.conv_fused
.. py:module:: torch.ao.nn.intrinsic.qat.modules.linear_fused
.. py:module:: torch.ao.nn.intrinsic.qat.modules.linear_relu
.. py:module:: torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu
.. py:module:: torch.ao.nn.intrinsic.quantized.modules.bn_relu
.. py:module:: torch.ao.nn.intrinsic.quantized.modules.conv_add
.. py:module:: torch.ao.nn.intrinsic.quantized.modules.conv_relu
.. py:module:: torch.ao.nn.intrinsic.quantized.modules.linear_relu
.. py:module:: torch.ao.nn.qat.dynamic.modules.linear
.. py:module:: torch.ao.nn.qat.modules.conv
.. py:module:: torch.ao.nn.qat.modules.embedding_ops
.. py:module:: torch.ao.nn.qat.modules.linear
.. py:module:: torch.ao.nn.quantizable.modules.activation
.. py:module:: torch.ao.nn.quantizable.modules.rnn
.. py:module:: torch.ao.nn.quantized.dynamic.modules.conv
.. py:module:: torch.ao.nn.quantized.dynamic.modules.linear
.. py:module:: torch.ao.nn.quantized.dynamic.modules.rnn
.. py:module:: torch.ao.nn.quantized.modules.activation
.. py:module:: torch.ao.nn.quantized.modules.batchnorm
.. py:module:: torch.ao.nn.quantized.modules.conv
.. py:module:: torch.ao.nn.quantized.modules.dropout
.. py:module:: torch.ao.nn.quantized.modules.embedding_ops
.. py:module:: torch.ao.nn.quantized.modules.functional_modules
.. py:module:: torch.ao.nn.quantized.modules.linear
.. py:module:: torch.ao.nn.quantized.modules.normalization
.. py:module:: torch.ao.nn.quantized.modules.rnn
.. py:module:: torch.ao.nn.quantized.modules.utils
.. py:module:: torch.ao.nn.quantized.reference.modules.conv
.. py:module:: torch.ao.nn.quantized.reference.modules.linear
.. py:module:: torch.ao.nn.quantized.reference.modules.rnn
.. py:module:: torch.ao.nn.quantized.reference.modules.sparse
.. py:module:: torch.ao.nn.quantized.reference.modules.utils
.. py:module:: torch.ao.nn.sparse.quantized.dynamic.linear
.. py:module:: torch.ao.nn.sparse.quantized.linear
.. py:module:: torch.ao.nn.sparse.quantized.utils
.. py:module:: torch.ao.ns.fx.graph_matcher
.. py:module:: torch.ao.ns.fx.graph_passes
.. py:module:: torch.ao.ns.fx.mappings
.. py:module:: torch.ao.ns.fx.n_shadows_utils
.. py:module:: torch.ao.ns.fx.ns_types
.. py:module:: torch.ao.ns.fx.pattern_utils
.. py:module:: torch.ao.ns.fx.qconfig_multi_mapping
.. py:module:: torch.ao.ns.fx.utils
.. py:module:: torch.ao.ns.fx.weight_utils
.. py:module:: torch.ao.pruning.scheduler.base_scheduler
.. py:module:: torch.ao.pruning.scheduler.cubic_scheduler
.. py:module:: torch.ao.pruning.scheduler.lambda_scheduler
.. py:module:: torch.ao.pruning.sparsifier.base_sparsifier
.. py:module:: torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier
.. py:module:: torch.ao.pruning.sparsifier.utils
.. py:module:: torch.ao.pruning.sparsifier.weight_norm_sparsifier
.. py:module:: torch.ao.quantization.backend_config.backend_config
.. py:module:: torch.ao.quantization.backend_config.executorch
.. py:module:: torch.ao.quantization.backend_config.fbgemm
.. py:module:: torch.ao.quantization.backend_config.native
.. py:module:: torch.ao.quantization.backend_config.observation_type
.. py:module:: torch.ao.quantization.backend_config.onednn
.. py:module:: torch.ao.quantization.backend_config.qnnpack
.. py:module:: torch.ao.quantization.backend_config.tensorrt
.. py:module:: torch.ao.quantization.backend_config.utils
.. py:module:: torch.ao.quantization.backend_config.x86
.. py:module:: torch.ao.quantization.fake_quantize
.. py:module:: torch.ao.quantization.fuser_method_mappings
.. py:module:: torch.ao.quantization.fuse_modules
.. py:module:: torch.ao.quantization.fx.convert
.. py:module:: torch.ao.quantization.fx.custom_config
.. py:module:: torch.ao.quantization.fx.fuse
.. py:module:: torch.ao.quantization.fx.fuse_handler
.. py:module:: torch.ao.quantization.fx.graph_module
.. py:module:: torch.ao.quantization.fx.lower_to_fbgemm
.. py:module:: torch.ao.quantization.fx.lower_to_qnnpack
.. py:module:: torch.ao.quantization.fx.lstm_utils
.. py:module:: torch.ao.quantization.fx.match_utils
.. py:module:: torch.ao.quantization.fx.pattern_utils
.. py:module:: torchMultiplier
.. py:module:: torch.ao.quantization.fx.prepare
.. py:module:: torch.ao.quantization.fx.qconfig_mapping_utils
.. py:module:: torch.ao.quantization.fx.quantize_handler
.. py:module:: torch.ao.quantization.fx.tracer
.. py:module:: torch.ao.quantization.fx.utils
.. py:module:: torch.ao.quantization.observer
.. py:module:: torch.ao.quantization.pt2e.duplicate_dq_pass
.. py:module:: torch.ao.quantization.pt2e.export_utils
.. py:module:: torch.ao.quantization.pt2e.graph_utils
.. py:module:: torch.ao.quantization.pt2e.port_metadata_pass
.. py:module:: torch.ao.quantization.pt2e.prepare
.. py:module:: torch.ao.quantization.pt2e.qat_utils
.. py:module:: torch.ao.quantization.pt2e.representation.rewrite
.. py:module:: torch.ao.quantization.pt2e.utils
.. py:module:: torch.ao.quantization.qconfig
.. py:module:: torch.ao.quantization.qconfig_mapping
.. py:module:: torch.ao.quantization.quant_type
.. py:module:: torch.ao.quantization.quantization_mappings
.. py:module:: torch.ao.quantization.quantize_fx
.. py:module:: torch.ao.quantization.quantize_jit
.. py:module:: torch.ao.quantization.quantize_pt2e
.. py:module:: torch.ao.quantization.quantizer.composable_quantizer
.. py:module:: torch.ao.quantization.quantizer.embedding_quantizer
.. py:module:: torch.ao.quantization.quantizer.quantizer
.. py:module:: torch.ao.quantization.quantizer.utils
.. py:module:: torch.ao.quantization.quantizer.x86_inductor_quantizer
.. py:module:: torch.ao.quantization.quantizer.xnnpack_quantizer
.. py:module:: torch.ao.quantization.quantizer.xnnpack_quantizer_utils
.. py:module:: torch.ao.quantization.stubs
.. py:module:: torch.ao.quantization.utils
.. py:module:: torch.nn.intrinsic.modules.fused
.. py:module:: torch.nn.intrinsic.qat.modules.conv_fused
.. py:module:: torch.nn.intrinsic.qat.modules.linear_fused
.. py:module:: torch.nn.intrinsic.qat.modules.linear_relu
.. py:module:: torch.nn.intrinsic.quantized.dynamic.modules.linear_relu
.. py:module:: torch.nn.intrinsic.quantized.modules.bn_relu
.. py:module:: torch.nn.intrinsic.quantized.modules.conv_relu
.. py:module:: torch.nn.intrinsic.quantized.modules.linear_relu
.. py:module:: torch.nn.qat.dynamic.modules.linear
.. py:module:: torch.nn.qat.modules.conv
.. py:module:: torch.nn.qat.modules.embedding_ops
.. py:module:: torch.nn.qat.modules.linear
.. py:module:: torch.nn.quantizable.modules.activation
.. py:module:: torch.nn.quantizable.modules.rnn
.. py:module:: torch.nn.quantized.dynamic.modules.conv
.. py:module:: torch.nn.quantized.dynamic.modules.linear
.. py:module:: torch.nn.quantized.dynamic.modules.rnn
.. py:module:: torch.nn.quantized.functional
.. py:module:: torch.nn.quantized.modules.activation
.. py:module:: torch.nn.quantized.modules.batchnorm
.. py:module:: torch.nn.quantized.modules.conv
.. py:module:: torch.nn.quantized.modules.dropout
.. py:module:: torch.nn.quantized.modules.embedding_ops
.. py:module:: torch.nn.quantized.modules.functional_modules
.. py:module:: torch.nn.quantized.modules.linear
.. py:module:: torch.nn.quantized.modules.normalization
.. py:module:: torch.nn.quantized.modules.rnn
.. py:module:: torch.nn.quantized.modules.utils
.. py:module:: torch.quantization.fake_quantize
.. py:module:: torch.quantization.fuse_modules
.. py:module:: torch.quantization.fuser_method_mappings
.. py:module:: torch.quantization.fx.convert
.. py:module:: torch.quantization.fx.fuse
.. py:module:: torch.quantization.fx.fusion_patterns
.. py:module:: torch.quantization.fx.graph_module
.. py:module:: torch.quantization.fx.match_utils
.. py:module:: torch.quantization.fx.pattern_utils
.. py:module:: torch.quantization.fx.prepare
.. py:module:: torch.quantization.fx.quantization_patterns
.. py:module:: torch.quantization.fx.quantization_types
.. py:module:: torch.quantization.fx.utils
.. py:module:: torch.quantization.observer
.. py:module:: torch.quantization.qconfig
.. py:module:: torch.quantization.quant_type
.. py:module:: torch.quantization.quantization_mappings
.. py:module:: torch.quantization.quantize
.. py:module:: torch.quantization.quantize_fx
.. py:module:: torch.quantization.quantize_jit
.. py:module:: torch.quantization.stubs
.. py:module:: torch.quantization.utils