.. currentmodule:: torch.func

.. _ux-limitations:

قيود تجربة المستخدم
==============

يحتوي torch.func، مثل `JAX <https://github.com/google/jax>`_، على قيود حول ما يمكن تحويله. بشكل عام، تتمثل قيود JAX في أن التحويلات تعمل فقط مع الوظائف البحتة: أي الوظائف التي يتم تحديدها تمامًا بواسطة الإدخال والتي لا تنطوي على تأثيرات جانبية (مثل الطفرة).

لدينا ضمان مشابه: تعمل تحويلاتنا بشكل جيد مع الوظائف البحتة. ومع ذلك، فإننا ندعم بعض العمليات في المكان. من ناحية، قد يتضمن كتابة التعليمات البرمجية المتوافقة مع تحويلات الوظائف تغيير طريقة كتابة التعليمات البرمجية لـ PyTorch، ومن ناحية أخرى، قد تجد أن تحويلاتنا تسمح لك بالتعبير عن أشياء كان من الصعب التعبير عنها سابقًا في PyTorch.

القيود العامة
---------

تشترك جميع تحويلات torch.func في القيد المتمثل في أنه لا ينبغي للوظيفة تعيين متغيرات عالمية. بدلاً من ذلك، يجب إرجاع جميع المخرجات من الدالة من الدالة. يأتي هذا القيد من كيفية تنفيذ torch.func: يقوم كل تحويل بتغليف إدخالات Tensor في فئات فرعية خاصة لـ Tensor torch.func التي تسهل التحويل.

لذلك، بدلاً من التالي:

::

  import torch
  from torch.func import grad

  # لا تفعل هذا
  intermediate = None

  def f(x):
    global intermediate
    intermediate = x.sin()
    z = intermediate.sin()
    return z

  x = torch.randn([])
  grad_x = grad(f)(x)

أعد كتابة ``f`` لإرجاع ``intermediate``:

::

  def f(x):
    intermediate = x.sin()
    z = intermediate.sin()
    return z, intermediate

  grad_x, intermediate = grad(f, has_aux=True)(x)

واجهات برمجة التطبيقات torch.autograd
------------------------------------

إذا كنت تحاول استخدام واجهة برمجة تطبيقات ``torch.autograd`` مثل ``torch.autograd.grad`` أو ``torch.autograd.backward`` داخل دالة يتم تحويلها بواسطة :func:`vmap` أو أحد تحويلات AD في torch.func (:func:`vjp`، :func:`jvp`، :func:`jacrev`، :func:`jacfwd`)، فقد لا يتمكن التحويل من التحويل عليه. إذا لم يتمكن من ذلك، فستتلقى رسالة خطأ.

هذا قيد تصميم أساسي في كيفية تنفيذ دعم AD في PyTorch والسبب في تصميم مكتبة torch.func. يرجى استخدام مكافئات torch.func لواجهات برمجة تطبيقات ``torch.autograd`` بدلاً من ذلك:
- ``torch.autograd.grad``، ``Tensor.backward`` -> ``torch.func.vjp`` أو ``torch.func.grad``
- ``torch.autograd.functional.jvp`` -> ``torch.func.jvp``
- ``torch.autograd.functional.jacobian`` -> ``torch.func.jacrev`` أو ``torch.func.jacfwd``
- ``torch.autograd.functional.hessian`` -> ``torch.func.hessian``

قيود vmap
---------

.. note::
  :func:`vmap` هو أكثر تحويلاتنا تقييدًا.
  لا تحتوي التحويلات المتعلقة بـ grad (:func:`grad`، :func:`vjp`، :func:`jvp`) على هذه القيود. :func:`jacfwd` (و :func:`hessian`، والذي يتم تنفيذه باستخدام :func:`jacfwd`) هو تركيبة من :func:`vmap` و:func:`jvp` لذلك فهو يحتوي أيضًا على هذه القيود.

``vmap(func)`` هو تحويل يعيد دالة تقوم بتعيين ``func`` عبر بعض الأبعاد الجديدة لكل إدخال Tensor. نموذج التفكير لـ vmap هو أنه يشبه تشغيل حلقة for: بالنسبة للوظائف البحتة (أي في غياب التأثيرات الجانبية)، ``vmap(f)(x)`` مكافئ لما يلي:

::

  torch.stack([f(x_i) for x_i in x.unbind(0)])

الطفرة: طفرة تعسفية لهياكل البيانات Python
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

في وجود التأثيرات الجانبية، :func:`vmap` لم يعد يتصرف كما لو كان يقوم بتشغيل
حلقة for. على سبيل المثال، ستطبع الدالة التالية:

::

  def f(x, list):
    list.pop()
    print("hello!")
    return x.sum(0)

  x = torch.randn(3, 1)
  lst = [0, 1, 2, 3]

  result = vmap(f, in_dims=(0, None))(x, lst)

"مرحبًا!" مرة واحدة فقط وستقوم بإلغاء عنصر واحد فقط من ``lst``.

:func:`vmap` ينفذ ``f`` مرة واحدة، لذلك تحدث جميع التأثيرات الجانبية مرة واحدة فقط.

هذا نتيجة لكيفية تنفيذ vmap. يحتوي torch.func على فئة BatchedTensor داخلية خاصة. ``vmap(f)(*inputs)`` يأخذ جميع إدخالات Tensor، ويحولها إلى BatchedTensors، وينادي ``f(*batched_tensor_inputs)``. يلغي BatchedTensor واجهة برمجة تطبيقات PyTorch لإنتاج سلوك معالج بالدفعات (أي معالج متجهي) لكل مشغل PyTorch.

الطفرة: عمليات PyTorch في المكان
^^^^^^^^^^^^^^^^^^^^^^^^^^

قد تكون هنا بسبب تلقي خطأ حول عمليات في المكان غير المتوافقة مع vmap. :func:`vmap` سيرفع خطأ إذا واجه عملية في المكان غير مدعومة في PyTorch وسيتمكن من ذلك بخلاف ذلك. العمليات غير المدعومة هي تلك التي من شأنها أن تتسبب في كتابة Tensor بمزيد من العناصر إلى Tensor بعدد أقل من العناصر. فيما يلي مثال على كيفية حدوث ذلك:

::

  def f(x, y):
    x.add_(y)
    return x

  x = torch.randn(1)
  y = torch.randn(3, 1)  # عند vmapped فوق، يبدو أنه له الشكل [1]

  # يرفع خطأ لأن 'x' لديه عدد أقل من العناصر من 'y'.
  vmap(f, in_dims=(None, 0))(x, y)

``x`` عبارة عن Tensor بعنصر واحد، و``y`` عبارة عن Tensor بثلاثة عناصر.
يحتوي ``x + y`` على ثلاثة عناصر (بسبب البث)، ولكن محاولة كتابة
ثلاثة عناصر مرة أخرى في ``x``، الذي يحتوي فقط على عنصر واحد، يرفع خطأ
بسبب محاولة كتابة ثلاثة عناصر في Tensor بعنصر واحد.

لا توجد مشكلة إذا كان Tensor الذي تتم الكتابة إليه مجمعًا في
:func:`~torch.vmap` (أي أنه يتم تشغيل vmap عليه).

::

  def f(x, y):
    x.add_(y)
    return x

  x = torch.randn(3, 1)
  y = torch.randn(3, 1)
  expected = x + y

  # لا يرفع خطأ لأن 'x' يتم تشغيل vmap عليه.
  vmap(f, in_dims=(0, 0))(x, y)
  assert torch.allclose(x, expected)

هناك إصلاح شائع لهذا وهو استبدال المكالمات إلى وظائف المصنع بما يعادلها "new_*"
على سبيل المثال:

- استبدل :func:`torch.zeros` بـ :meth:`Tensor.new_zeros`
- استبدل :func:`torch.empty` بـ :meth:`Tensor.new_empty`

ولرؤية سبب ذلك، ضع في اعتبارك ما يلي.

::

  def diag_embed(vec):
    assert vec.dim() == 1
    result = torch.zeros(vec.shape[0], vec.shape[0])
    result.diagonal().copy_(vec)
    return result

  vecs = torch.tensor([[0., 1, 2], [3., 4, 5]])

  # RuntimeError: vmap: inplace arithmetic(self, *extra_args) is not possible ...
  vmap(diag_embed)(vecs)

داخل :func:`~torch.vmap`، ``result`` عبارة عن Tensor من الشكل [3، 3].
ومع ذلك، على الرغم من أن ``vec`` يبدو أنه له الشكل [3]، فإن ``vec`` يحتوي
في الواقع على الشكل الأساسي [2، 3].
من غير الممكن نسخ ``vec`` إلى ``result.diagonal()``، الذي له
شكل [3]، لأنه يحتوي على عناصر كثيرة جدًا.

::

  def diag_embed(vec):
    assert vec.dim() == 1
    result = vec.new_zeros(vec.shape[0], vec.shape[0])
    result.diagonal().copy_(vec)
    return result

  vecs = torch.tensor([[0., 1, 2], [3., 4, 5]])
  vmap(diag_embed)(vecs)

يؤدي استبدال :func:`torch.zeros` بـ :meth:`Tensor.new_zeros` إلى أن يكون لـ ``result``
Tensor أساسي من الشكل [2، 3، 3]، لذلك من الممكن الآن
لنسخ ``vec``، الذي له شكل أساسي [2، 3]، في ``result.diagonal()``.

الطفرة: عمليات PyTorch باستخدام out=
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
:func:`vmap` لا يدعم وسيط ``out=`` في عمليات PyTorch.
سيؤدي إلى خطأ بشكل أنيق إذا واجه ذلك في التعليمات البرمجية الخاصة بك.

هذا ليس قيدًا أساسيًا؛ يمكننا من الناحية النظرية دعم هذا في
المستقبل ولكننا اخترنا عدم القيام بذلك الآن.

تدفق التحكم المعتمد على البيانات في Python
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
لا ندعم بعد تشغيل ``vmap`` عبر تدفق التحكم المعتمد على البيانات. تدفق التحكم المعتمد على البيانات هو عندما يكون شرط عبارة if أو حلقة while أو for عبارة Tensor التي يتم تشغيل vmap عليها. على سبيل المثال، سيرفع ما يلي رسالة خطأ:

::

  def relu(x):
    if x > 0:
      return x
    return 0

  x = torch.randn(3)
  vmap(relu)(x)

ومع ذلك، فإن أي تدفق تحكم لا يعتمد على القيم في تنسورات "vmap"ed
سيعمل:

::

  def custom_dot(x):
    if x.dim() == 1:
      return torch.dot(x, x)
    return (x * x).sum()

  x = torch.randn(3)
  vmap(custom_dot)(x)

تدعم JAX تحويل
`تدفق التحكم المعتمد على البيانات <https://jax.readthedocs.io/en/latest/jax.lax.html#control-flow-operators>`_
باستخدام مشغلي تدفق تحكم خاصين (مثل ``jax.lax.cond``، ``jax.lax.while_loop``).
نحن نحقق في إضافة مكافئات لتلك الموجودة في PyTorch.

العمليات المعتمدة على البيانات (.item())
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
لا ندعم (ولن ندعم) تشغيل vmap عبر دالة معرفة من قبل المستخدم تستدعي
``.item()`` على Tensor. على سبيل المثال، سيرفع ما يلي رسالة خطأ:

::

  def f(x):
    return x.item()

  x = torch.randn(3)
  vmap(f)(x)

يرجى محاولة إعادة كتابة التعليمات البرمجية الخاصة بك لعدم استخدام مكالمات ``.item()``.

قد تواجه أيضًا رسالة خطأ حول استخدام ``.item()`` ولكن قد
لم تستخدمها. في تلك الحالات، من الممكن أن PyTorch داخليًا
استدعاء ``.item()`` - يرجى إرسال مشكلة على GitHub وسنقوم بإصلاح
داخليات PyTorch.

عمليات الشكل الديناميكي (nonzero وما شابه)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
يتطلب ``vmap(f)`` أن تقوم ``f`` بتطبيق كل "مثال" في الإدخال الخاص بك
إرجاع Tensor بنفس الشكل. لا يتم دعم العمليات مثل ``torch.nonzero``،
``torch.is_nonzero`` وسينتج عنها خطأ نتيجة لذلك.

ولرؤية السبب، ضع في اعتبارك المثال التالي:

::

  xs = torch.tensor([[0, 1, 2]، [0، 0، 3]])
  vmap(torch.nonzero)(xs)

``torch.nonzero(xs[0])`` يعيد Tensor من الشكل 2؛
لكن ``torch.nonzero(xs[1])`` يعيد Tensor من الشكل 1.
نحن غير قادرين على بناء Tensor واحد كإخراج؛
ستحتاج الإخراج إلى Tensor متفرق (ولم يتم بعد مفهوم PyTorch
من Tensor متفرق).

العشوائية
------
يمكن أن تكون نية المستخدم عند استدعاء عملية عشوائية غير واضحة. على وجه التحديد، قد يريد بعض المستخدمين
أن يكون السلوك العشوائي هو نفسه عبر الدفعات في حين قد يريدها آخرون أن يختلفوا عبر الدفعات.
للتصدي لهذا، يأخذ "vmap" علم العشوائية.

يمكن تمرير العلم فقط إلى vmap ويمكن أن يأخذ 3 قيم، "error" أو "different" أو "same"، افتراضيًا
إلى الخطأ. في وضع "الخطأ"، ستؤدي أي مكالمة إلى دالة عشوائية إلى حدوث خطأ يطلب من المستخدم استخدام
واحدة من العلامتين الأخريين بناءً على حالة الاستخدام الخاصة بهم.

في ظل العشوائية "المختلفة"، تنتج العناصر في الدفعة قيمًا عشوائية مختلفة. على سبيل المثال،

::

  def add_noise(x):
    y = torch.randn(())  # ستكون y مختلفة عبر الدفعة
    return x + y

  x = torch.ones(3)
  result = vmap(add_noise، randomness="different")(x)  # نحصل على 3 قيم مختلفة

في ظل العشوائية "نفس"، تنتج العناصر في الدفعة نفس القيم العشوائية. على سبيل المثال،

::

  def add_noise(x):
    y = torch.randn(())  # ستكون y هي نفسها عبر الدفعة
    return x + y

  x = torch.ones(3)
  result = vmap(add_noise، randomness="same")(x)  # نحصل على نفس القيمة، مكررة 3 مرات

.. warning::
    لا يحدد نظامنا سوى سلوك العشوائية لمشغلي PyTorch ولا يمكنه التحكم في
    سلوك المكتبات الأخرى، مثل numpy. هذا مشابه لقيود JAX مع حلولهم

.. note::
    لن تنتج مكالمات vmap المتعددة باستخدام أي من نوعي العشوائية المدعومة
    نفس النتائج. مثل PyTorch القياسي، يمكن للمستخدم الحصول على قابلية تكرار العشوائية من خلال
    إما استخدام ``torch.manual_seed()`` خارج vmap أو باستخدام المولدات.

.. note::
    وأخيرًا، تختلف العشوائية لدينا عن JAX لأننا لا نستخدم PRNG عديم الحالة، جزئيًا لأن PyTorch
    لا يدعم PRNG عديم الحالة بالكامل. بدلاً من ذلك، لقد قدمنا نظامًا للعلامات للسماح بأشكال العشوائية الأكثر شيوعًا التي نراها. إذا لم تتناسب حالة الاستخدام الخاصة بك مع أشكال العشوائية هذه، فيرجى إرسال مشكلة.