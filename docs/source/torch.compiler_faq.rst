.. _only::

الأسئلة المتكررة
===========

هل يدعم ``torch.compile`` التدريب؟
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

يدعم ``torch.compile`` التدريب، باستخدام AOTAutograd لالتقاط الخلفيات:

1. تتم عملية التقاط مخطط ``.forward()`` و ``optimizer.step()`` بواسطة واجهة
   Python ``evalframe`` الخاصة بـ TorchDynamo.
2. لكل جزء من ``.forward()`` الذي يلتقطه torchdynamo، فإنه يستخدم
   AOTAutograd لتوليد جزء مخطط خلفي.
3. يتم (اختياريًا) تقسيم كل زوج من المخططات الأمامية والخلفية إلى الحد الأدنى
   لحفظ الحالة الدنيا بين الأمام والخلف.
4. يتم لف أزواج المخططات الأمامية والخلفية في وحدات ``autograd.function``.
5. لا يزال كود المستخدم الذي يستدعي\ ``.backward()`` يؤدي إلى تشغيل محرك
   autograd الخاص بـ eager، والذي يقوم بتشغيل كل مخطط خلفي مجمع كما لو كان
   عملية واحدة، كما يقوم بتشغيل وظائف ``.backward()`` لأي عمليات eager غير
   مجمعة.

هل تدعم التعليمات البرمجية الموزعة؟
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

يدعم ``torch.compile`` ``DistributedDataParallel`` (DDP).
ويتم النظر في دعم مكتبات التدريب الموزعة الأخرى.

والسبب الرئيسي في أن التعليمات البرمجية الموزعة تشكل تحديًا مع dynamo هو
أن AOTAutograd تفك كل من المرور الأمامي والخلفي وتقدم
مخططين للواجهات الخلفية للتحسين. هذه مشكلة للرمز الموزع لأننا
نود أن نتداخل في عمليات الاتصال مع الحسابات. تنجز PyTorch
المتحمسة ذلك بطرق مختلفة لـ DDP/FSDP - باستخدام خطافات autograd،
وخطافات الوحدات، وتعديلات/طفرات حالات الوحدات. في تطبيق بسيط لـ
dynamo، قد يتم تأخير الخطافات التي يجب تشغيلها مباشرة بعد عملية
أثناء الخلفيات حتى بعد منطقة العمليات الخلفية المجمعة بأكملها، وذلك
بسبب كيفية تفاعل وظائف AOTAutograd المجمّعة مع خطافات الموزع.

تتمثل الاستراتيجية الأساسية لتحسين DDP باستخدام Dynamo في
`distributed.py <https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/backends/distributed.py>`__
حيث تتمثل الفكرة الرئيسية في كسر المخطط على `حدود دلو
DDP <https://pytorch.org/docs/stable/notes/ddp.html#internal-design>`__.

عندما تحتاج كل عقدة في DDP إلى مزامنة أوزانها مع العقد الأخرى،
فهي تنظم تدرجاتها وبارامتراتها في دلاء تقلل من أوقات الاتصال
وتسمح للعقدة ببث جزء من تدرجاتها إلى العقد الأخرى التي تنتظر.

تعني كسور المخطط في التعليمات البرمجية الموزعة أنه يمكنك توقع
قيام dynamo وواجهاتها الخلفية بتحسين عبء العمل الحسابي لبرنامج
موزع ولكن ليس عبئه التواصلي. قد تتعارض كسور المخطط مع تسريع
التجميع، إذا حرم حجم المخطط المخفض المجمع من فرص الانصهار.
ومع ذلك، هناك عوائد متناقصة مع زيادة حجم المخطط نظرًا لأن معظم
تحسين الحساب الحالي عبارة عن عمليات انصهار محلية. لذا فقد يكون
هذا النهج كافيًا في الممارسة العملية.

هل ما زلت بحاجة إلى تصدير الرسوم البيانية الكاملة؟
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

بالنسبة لأغلبية النماذج، فمن المحتمل ألا تحتاج إلى ذلك، ويمكنك استخدام
``torch.compile()`` كما هو، ولكن هناك بعض الحالات التي تكون فيها
الرسوم البيانية الكاملة ضرورية، ويمكنك التأكد من اكتمال الرسم البياني
ببساطة عن طريق تشغيل ``torch.compile(..., fullgraph=True)``. تشمل هذه
الحالات ما يلي:

* تشغيلات التدريب واسعة النطاق، مثل 250 ألفًا+ التي تتطلب موازاة الأنابيب
  واستراتيجيات التجزئة المتقدمة الأخرى.

* أدوات تحسين الاستدلال مثل `TensorRT <https://github.com/pytorch/TensorRT>`__
  أو `AITemplate <https://github.com/facebookincubator/AITemplate>`__ التي
  تعتمد على الانصهار بشكل أكثر عدوانية من أدوات تحسين التدريب.

* التدريب أو الاستدلال المحمول.

سيشمل العمل المستقبلي تتبع عمليات الاتصال في الرسوم البيانية،
وتنسيق هذه العمليات مع تحسينات الحساب، وتحسين عمليات الاتصال.

لماذا تتحطم الشفرة الخاصة بي؟
~~~~~~~~~~~~~~~~~~~~~~~~

إذا كان كودك يعمل بشكل جيد دون ``torch.compile`` وبدأ في التعطل بمجرد
تمكينه، فإن الخطوة الأولى والأهم هي معرفة الجزء من المكدس الذي حدث
فيه الفشل. لإصلاح ذلك، اتبع الخطوات أدناه وحاول فقط الخطوة التالية إذا
نجحت الخطوة السابقة.

1. ``torch.compile(..., backend="eager")`` الذي يقوم بتشغيل عملية التقاط
   مخطط TorchDynamo الأمامي فقط ثم يقوم بتشغيل المخطط الذي تم التقاطه
   باستخدام PyTorch. إذا فشل هذا، فهناك مشكلة في TorchDynamo.

2. ``torch.compile(..., backend="aot_eager")``
   الذي يقوم بتشغيل TorchDynamo لالتقاط مخطط أمامي، ثم AOTAutograd
   لتتبع مخطط خلفي دون أي خطوات مجمعة إضافية من واجهة برمجة
   التطبيقات. سيتم بعد ذلك استخدام PyTorch eager لتشغيل المخططات الأمامية
   والخلفية. إذا فشل هذا، فهناك مشكلة في AOTAutograd.

3. ``torch.compile(..., backend="inductor")`` الذي يقوم بتشغيل TorchDynamo
   لالتقاط مخطط أمامي، ثم AOTAutograd لتتبع مخطط خلفي مع مجمع
   TorchInductor. إذا فشل هذا، فهناك مشكلة في TorchInductor

لماذا التجميع بطيء؟
~~~~~~~~~~~~~~~~~~~~~~~~

* **تجميع دينامو** - لدى TorchDynamo إحصائيات مدمجة لجمع وعرض الوقت
  المستغرق في كل مرحلة من مراحل التجميع. يمكن الوصول إلى هذه الإحصائيات
  عن طريق استدعاء ``torch._dynamo.utils.compile_times()`` بعد تنفيذ
  ``torch._dynamo``. بشكل افتراضي، تقوم هذه الدالة بإرجاع تمثيل سلسلة
  لأوقات التجميع المستغرقة في كل دالة TorchDynamo حسب الاسم.

* **تجميع المحث** - لدى TorchInductor إحصائيات مدمجة ووظيفة تتبع لعرض
  الوقت المستغرق في كل مرحلة من مراحل التجميع، ورمز الإخراج، وتصور
  المخطط وإغراق IR. ``env TORCH_COMPILE_DEBUG=1 python repro.py``.
  هذه أداة تصحيح مصممة لتسهيل تصحيح أخطاء/فهم داخليات TorchInductor
  مع إخراج سيشبه
  `هذا <https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396>`__
  يمكن تمكين/تعطيل كل ملف في تتبع التصحيح هذا عبر
  ``torch._inductor.config.trace.*``. يتم تعطيل كل من المخطط والرسوم
  البيانية بشكل افتراضي نظرًا لأنهما مكلفان لإنشائهما. راجع
  `إخراج دليل التصحيح
  <https://gist.github.com/jansel/f4af078Singolare.com/openai.com/blog/triton/>`__
  للحصول على مزيد من الأمثلة.

* **إعادة التجميع المفرط**
  عندما يقوم TorchDynamo بتجميع دالة (أو جزء منها)، فإنه يقوم بعمل افتراضات
  معينة حول المحليات والعالميات للسماح بتحسين المجمع، ويعبر عن هذه
  الافتراضات على أنها حراس يتحققون من القيم في وقت التشغيل. إذا فشل أي
  من هذه الحراس، فسيقوم Dynamo بإعادة تجميع تلك الدالة (أو الجزء) حتى
  ``torch._dynamo.config.cache_size_limit`` مرات. إذا وصل برنامجك إلى حد
  ذاكرة التخزين المؤقت، فستحتاج أولاً إلى تحديد الحارس الذي يفشل وما
  الجزء من برنامجك الذي يفعله. `الملف الشخصي لإعادة التجميع
  <#recompilation-profiler>`__ يقوم بتشغيل عملية أتمتة تعيين حد ذاكرة
  التخزين المؤقت لـ TorchDynamo إلى 1 وتشغيل برنامجك في إطار "مجمع"
  للمراقبة فقط يقوم بتسجيل أسباب أي فشل في الحارس. يجب أن تتأكد من
  تشغيل برنامجك لمدة لا تقل عن (عدد التكرارات) كما كنت تفعل عندما واجهت
  مشكلة، وسيقوم الملف الشخصي بتراكم الإحصائيات خلال هذه المدة.

.. code-block:: python

   from torch._dynamo.utils import CompileProfiler

   def my_model():
       ...

   with CompileProfiler() as prof:
       profiler_model = torch.compile(my_model, backend=prof)
       profiler_model()
       print(prof.report())

لماذا تقوم بإعادة التجميع في الإنتاج؟
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

في بعض الحالات، قد لا ترغب في إجراء عمليات التجميع غير المتوقعة بعد
تسخين البرنامج. على سبيل المثال، إذا كنت تقوم بتشغيل حركة المرور
الإنتاجية في تطبيق حساس للاتصالات. يوفر TorchDynamo وضعًا بديلاً
يتم فيه استخدام الرسوم البيانية المجمعة مسبقًا، ولكن لا يتم إنشاؤها
جديدة:

.. code-block:: python

   frozen_toy_example = dynamo.run(toy_example)
   frozen_toy_example(torch.randn(10)، torch.randn(10))

كيف تقوم بتسريع كودي؟
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

هناك 3 طرق رئيسية لتسريع كود PyTorch:

1. دمج النواة من خلال عمليات الانصهار الرأسية التي تدمج العمليات المتتالية
   لتجنب القراءات/الكتابات المفرطة. على سبيل المثال، يؤدي دمج اثنين من
   عمليات cosine اللاحقة إلى قراءة واحدة وكتابة واحدة بدلاً من قراءتين
   وكتابتين و2. الانصهار الأفقي: أبسط مثال على ذلك هو الدفعات، حيث يتم
   ضرب مصفوفة واحدة بمثال دفعي، ولكن السيناريو العام هو GEMM مجمعة
   حيث يتم جدولة مجموعة من ضربات المصفوفة معًا

2. التنفيذ خارج الترتيب: تحسين عام لمجمعات، بالنظر إلى الأمام إلى
   تبعيات البيانات الدقيقة داخل الرسم البياني، يمكننا تحديد الوقت
   الأنسب لتنفيذ عقدة وإعادة استخدام المخازن المؤقتة.

3. التنسيب التلقائي للعمل: مماثل لنقطة تنفيذ خارج الترتيب، ولكن عن طريق
   مطابقة العقد في الرسم البياني للموارد مثل الأجهزة المادية أو الذاكرة،
   يمكننا تصميم جدول مناسب

ما سبق هي مبادئ عامة لتسريع كود PyTorch ولكن الواجهات الخلفية المختلفة
ستقوم كل منها بمقايضات مختلفة حول ما يجب تحسينه. على سبيل المثال،
يحرص المحث أولاً على دمج كل ما يمكنه ثم يقوم بتوليد نوى
`Triton <https://openai.com/blog/triton/>`__.

بالإضافة إلى ذلك، توفر Triton تسريعًا بسبب تجميع الذاكرة التلقائي،
وإدارة الذاكرة، والجدولة داخل كل وحدة معالجة تدفق متعددة
(SM)، وقد تم تصميمها للتعامل مع الحسابات المبلطة.

ومع ذلك، بغض النظر عن الواجهة الخلفية التي تستخدمها، من الأفضل استخدام
نهج المعايير القياسية لذا جرب ملف تعريف PyTorch، وقم بالتفتيش البصري
على النواة المولدة، وحاول أن ترى ما يحدث بنفسك.

لماذا لا أرى تسريع؟
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _torch.compiler_graph_breaks:

كسور المخطط
------------

السبب الرئيسي في أنك لن ترى تسريع السرعة التي تريدها باستخدام dynamo
هو كسور المخطط المفرطة. لذا فما هو كسر المخطط؟

بالنظر إلى برنامج مثل:

.. code-block:: python

   def some_fun(x):
       ...

   torch.compile(some_fun)(x)
   ...

سيحاول Torchdynamo تجميع جميع عمليات torch/tensor داخل ``some_fun()``
في رسم بياني FX واحد، ولكنه قد يفشل في التقاط كل شيء في رسم بياني
واحد.

بعض أسباب كسر المخطط لا يمكن التغلب عليها بالنسبة إلى TorchDynamo مثل
الاستدعاء إلى امتداد C بخلاف PyTorch غير مرئي لـ TorchDynamo، ويمكنه
القيام بأشياء عشوائية دون أن يتمكن TorchDynamo من تقديم حراس
ضروريين لضمان أن البرنامج المجمع سيكون آمنًا لإعادة استخدامه.

   لتحقيق الأداء الأمثل، من المهم أن يكون لديك أقل عدد ممكن من كسور
   المخطط.

تحديد سبب كسر المخطط
--------------------------------------

لتحديد جميع كسور المخطط في برنامج وأسباب كسور المخطط المرتبطة،
يمكن استخدام ``torch._dynamo.explain``. تقوم هذه الأداة بتشغيل
TorchDynamo على الدالة المقدمة وتجميع كسور المخطط التي تتم مواجهتها.
فيما يلي مثال على الاستخدام:

.. code-block:: python

   import torch
   import torch._dynamo as dynamo
   def toy_example(a, b):
       x = a / (torch.abs(a) + 1)
       print("woo")
       if b.sum() < 0:
           b = b * -1
       return x * b
   explanation = dynamo.explain(toy_example)(torch.randn(10), torch.randn(10))
   print(explanation)
   """
   Graph Count: 3
   Graph Break Count: 2
   Op Count: 5
   Break Reasons:
     Break Reason 1:
       Reason: builtin: print [<class 'torch._dynamo.variables.constant.ConstantVariable'>] False
       User Stack:
         <FrameSummary file foo.py, line 5 in toy_example>
     Break Reason 2:
       Reason: generic_jump TensorVariable()
       User Stack:
         <FrameSummary file foo.py, line 6 in torch_dynamo_resume_in_toy_example_at_5>
   Ops per Graph:
     ...
   Out Guards:
     ...
   """

لإلقاء خطأ عند أول كسر مخطط يتم مواجهته، يمكنك تعطيل عمليات الاستدعاء
الخلفية لـ Python باستخدام ``fullgraph=True``، والتي يجب أن تكون
مألوفة إذا كنت قد عملت مع مجمعات قائمة على التصدير.

.. code-block:: python

   def toy_example(a, b):
      ...

   torch.compile(toy_example, fullgraph=True, backend=<compiler>)(a, b)

لماذا لم تتم إعادة تجميع التعليمات البرمجية الخاصة بي عندما قمت بتغييرها؟
هذا هو النص المترجم إلى اللغة العربية بتنسيق ReStructuredText:

-----------------------------------------------

إذا قمت بتفعيل الأشكال الديناميكية عن طريق ضبط
``env TORCHDYNAMO_DYNAMIC_SHAPES=1 python model.py`` فإن كودك لن يعاد تجميعه عند تغير الأشكال. لقد أضفنا دعمًا للأشكال الديناميكية
التي تتجنب إعادة التجميع في حالة اختلاف الأشكال بأقل من عامل 2. وهذا مفيد بشكل خاص في السيناريوهات التي تختلف فيها أحجام الصور في الرؤية الحاسوبية أو طول التسلسل المتغير في معالجة اللغات الطبيعية. في سيناريوهات الاستدلال، غالبًا ما يكون من المستحيل معرفة حجم الدفعة مسبقًا لأنك تأخذ ما يمكنك الحصول عليه من تطبيقات العميل المختلفة.

بشكل عام، يحاول TorchDynamo جاهدًا عدم إعادة تجميع الأشياء
دون داعٍ، لذلك إذا وجد TorchDynamo، على سبيل المثال، 3 رسومات بيانية ولم يتغير تعديلك سوى رسم بياني واحد، فسيتم إعادة تجميع هذا الرسم البياني فقط. لذلك، فإن إحدى النصائح الأخرى لتجنب أوقات التجميع البطيئة المحتملة هي تسخين النموذج عن طريق تجميعه مرة واحدة، والتي ستكون التجميعات اللاحقة بعدها أسرع بكثير. وقت التجميع البارد عند بدء التشغيل لا يزال مقياسًا نتتبعه بشكل واضح.

لماذا أحصل على نتائج غير صحيحة؟
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

يمكن أيضًا تقليل مشكلات الدقة إذا قمت بتعيين متغير البيئة
``TORCHDYNAMO_REPRO_LEVEL=4``، فهو يعمل بنموذج git bisect مشابه وقد يكون إعادة الإنشاء الكامل شيئًا مثل
``TORCHDYNAMO_REPRO_AFTER="aot" TORCHDYNAMO_REPRO_LEVEL=4`` والسبب
نحن بحاجة إلى هذا هو أن المترجمين القادمين سيقومون بتوليد التعليمات البرمجية سواء كان ذلك
رمز Triton أو backend C++، ويمكن أن تختلف الأرقام من هذه المترجمات النهائية بطرق دقيقة ولكنها قد يكون لها تأثير كبير على استقرار التدريب الخاص بك. لذلك، فإن مصحح الدقة مفيد جدًا بالنسبة لنا
لاكتشاف الأخطاء في توليد التعليمات البرمجية الخاصة بنا أو مع مترجم backend.

إذا كنت ترغب في التأكد من أن توليد الأرقام العشوائية هو نفسه عبر كل من torch
و triton، فيمكنك تمكين ``torch._inductor.config.fallback_random = True``

لماذا أحصل على OOMs؟
~~~~~~~~~~~~~~~~~~~~~~

Dynamo لا يزال منتجًا تجريبيًا، لذلك هناك بضعة مصادر لـ OOMs وإذا
كنت ترى OOM، جرب تعطيل التكوينات التالية بهذا
الترتيب ثم قم بفتح مشكلة على GitHub حتى نتمكن من حل المشكلة الأساسية
1. إذا كنت تستخدم الأشكال الديناميكية، فحاول تعطيلها، لقد قمنا بتعطيلها
بشكل افتراضي: ``env TORCHDYNAMO_DYNAMIC_SHAPES=0 python model.py`` 2.
تمكين الرسوم البيانية CUDA مع Triton بشكل افتراضي في المحرك، ولكن إزالتها قد تخفف بعض مشكلات OOM: ``torch._inductor.config.triton.cudagraphs = False``.

هل يعمل ``torch.func`` مع ``torch.compile`` (لتحويلات `grad` و` vmap`)؟
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

تطبيق تحويل ``torch.func`` على دالة تستخدم ``torch.compile``
يعمل:

.. code-block:: python

    import torch

    @torch.compile
    def f(x):
        return torch.sin(x)

    def g(x):
        return torch.grad(f)(x)

    x = torch.randn(2, 3)
    g(x)

استدعاء تحويل ``torch.func`` داخل دالة تمت معالجتها باستخدام ``torch.compile``
------------------------------------------------------------------------------------


تجميع ``torch.func.grad`` مع ``torch.compile``
----------------------------------------------------

.. code-block:: python

    import torch

    def wrapper_fn(x):
        return torch.func.grad(lambda x: x.sin().sum())(x)

    x = torch.randn(3, 3, 3)
    grad_x = torch.compile(wrapper_fn)(x)

تجميع ``torch.vmap`` مع ``torch.compile``
-----------------------------------------------

.. code-block:: python

    import torch

    def my_fn(x):
        return torch.vmap(lambda x: x.sum(1))(x)

    x = torch.randn(3, 3, 3)
    output = torch.compile(my_fn)(x)


تجميع الدوال بخلاف تلك المدعومة (مخرج الطوارئ)
-----------------------------------------------------------------------

كحل بديل للتحويلات الأخرى، استخدم ``torch._dynamo.allow_in_graph``

``allow_in_graph`` هو مخرج للطوارئ. إذا لم يعمل كودك مع
``torch.compile``، الذي يفحص بايتكود بايثون، ولكنك تعتقد أنه
سيعمل عبر نهج التتبع الرمزي (مثل ``jax.jit``)، ثم استخدم
``allow_in_graph``.

من خلال استخدام ``allow_in_graph`` لوضع علامة على دالة، يجب التأكد من
تلبية كودك للمتطلبات التالية:

- تعتمد جميع المخرجات في دالتك فقط على المدخلات
  ولا تعتمد على أي تنسيقات محتجزة.
- دالتك وظيفية. أي أنه لا يقوم بتغيير أي حالة. قد
  يتم الاسترخاء؛ في الواقع، نحن ندعم الدوال التي تبدو وظيفية من
الخارج: قد يكون لديهم عمليات PyTorch في المكان، ولكن قد لا يقومون بتعديل حالة عالمية أو مدخلات للدالة.
- لا ترفع دالتك أخطاء تعتمد على البيانات.

.. code-block:: python

    import torch

    @torch.compile
    def f(x):
        return torch._dynamo.allow_in_graph(torch.vmap(torch.sum))(x)

    x = torch.randn(2, 3)
    f(x)

من الأخطاء الشائعة استخدام ``allow_in_graph`` لوضع علامة على دالة
التي تستدعي ``nn.Module``. هذا لأن المخرجات تعتمد الآن على
معلمات ``nn.Module``. لجعل هذا يعمل، استخدم
``torch.func.functional_call`` لاستخراج حالة الوحدة النمطية.

هل يعمل NumPy مع ``torch.compile``؟
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

بدءًا من الإصدار 2.1، يفهم ``torch.compile`` برامج NumPy الأصلية التي
تعمل على صفيفات NumPy، وبرامج PyTorch-NumPy المختلطة التي تحول من PyTorch
إلى NumPy والعكس باستخدام ``x.numpy()``، ``torch.from_numpy``، والوظائف ذات الصلة.

.. _nonsupported-numpy-feats:

أي ميزات NumPy يدعمها ``torch.compile``؟
----------------------------------------------------

يتبع NumPy داخل ``torch.compile`` إصدار NumPy 2.0 قبل الإصدار.

بشكل عام، يمكن لـ ``torch.compile`` تتبع معظم الإنشاءات NumPy،
وعندما لا يستطيع، فإنه ينتقل إلى الوضع التواقتي ويدع NumPy ينفذ ذلك الجزء من
الشفرة. حتى بعد ذلك، هناك بعض الميزات التي تنحرف فيها دلالة ``torch.compile``
بصورة طفيفة عن تلك الخاصة بـ NumPy:

- المصفوفات العددية لـ NumPy: نحن نعتبرها مصفوفات أحادية البعد. أي أن ``np.float32(3)`` يعيد
  مصفوفة أحادية البعد في ظل ``torch.compile``. لتجنب كسر الرسم البياني، من الأفضل استخدام هذه المصفوفة أحادية البعد. إذا كسر هذا كودك، فيمكنك حل هذه المشكلة عن طريق قولبة المصفوفة العددية لـ NumPy
  إلى نوع السلسلة المناسب ``bool/int/float``.

- الخطوات السلبية: يعيد ``np.flip`` والشرائح مع خطوة سلبية نسخة.

- الترقية النوعية: ستتغير قواعد الترقية النوعية لـ NumPy في NumPy 2.0. القواعد الجديدة
  موصوفة في `NEP 50 <https://numpy.org/neps/nep-0050-scalar-promotion.html)>`__.
  ينفذ ``torch.compile`` NEP 50 بدلاً من القواعد الحالية التي سيتم إيقافها قريبًا.

- ``{tril، triu} _indices_from / {tril، triu} _indices`` تعيد المصفوفات بدلاً من مجموعة من المصفوفات.

وهناك ميزات أخرى لا ندعم فيها التتبع، وننتقل بسلاسة
العودة إلى NumPy لتنفيذها:

- الأنواع غير الرقمية مثل التواريخ والأوقات، والسلاسل النصية، والمحارف، والأنواع الفارغة، والمصفوفات المهيكلة، ومصفوفات recarrays.

- الأنواع الطويلة ``np.float128/np.complex256`` وبعض الأنواع غير الموقعة ``np.uint16/np.uint32/np.uint64``.

- فئات فرعية ``ndarray``.

- المصفوفات المقنعة.

- آلات ufunc الغامضة مثل ``axes=[(n، k)، (k، m)-> (n، m)]`` وطرق ufunc (على سبيل المثال، ``np.add.reduce``).

- فرز / ترتيب المصفوفات ``complex64/complex128``.

- الدوالية ``np.poly1d`` و ``np.polynomial``.

- المواضع ``out1، out2`` في الدوال ذات الإرجاع 2 أو أكثر (يعمل ``out=tuple``).

- ``__array_function__``، ``__array_interface__`` و ``__array_wrap__``.

- سمة ``ndarray.ctypes``.

هل يمكنني تجميع كود NumPy باستخدام ``torch.compile``؟
-------------------------------------------------

بالطبع يمكنك ذلك! يفهم ``torch.compile`` كود NumPy بشكل أصلي، ويعامله
كما لو كان كود PyTorch. للقيام بذلك، قم ببساطة بتغليف كود NumPy باستخدام الديكور ``torch.compile``.

.. code-block:: python

   import torch
   import numpy as np

   @torch.compile
   def numpy_fn(X: np.ndarray، Y: np.ndarray) -> np.ndarray:
       return np.sum(X[:, :, None] * Y[:, None, :])، axis=(-2، -1))

   X = np.random.randn(1024، 64)
   Y = np.random.randn(1024، 64)
   Z = numpy_fn(X، Y)
   تأكيد isinstance (Z، np.ndarray)

عند تنفيذ هذا المثال باستخدام متغير البيئة ``TORCH_LOGS=output_code``، يمكننا أن نرى
أن ``torch.compile`` كان قادرًا على دمج الضرب والمجموع في نواة C++ واحدة.
كما تمكن من تنفيذها بالتوازي باستخدام OpenMP (NumPy الأصلي أحادي الخيط).
يمكن أن يجعل هذا كود NumPy الخاص بك أسرع "n" مرة، حيث "n" هو عدد النوى
في معالجك!

يدعم تتبع كود NumPy بهذه الطريقة أيضًا كسور الرسم البياني داخل الكود المجمّع.

هل يمكنني تنفيذ كود NumPy على CUDA وحساب التدرجات عبر ``torch.compile``؟
نعم، يمكنك ذلك! للقيام بذلك، يمكنك ببساطة تنفيذ كودك داخل سياق ``torch.device("cuda")``. على سبيل المثال:

.. code-block:: python

   import torch
   import numpy as np

   @torch.compile
   def numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray:
       return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))

   X = np.random.randn(1024, 64)
   Y = np.random.randn(1024, 64)
   with torch.device("cuda"):
       Z = numpy_fn(X, Y)
   assert isinstance(Z, np.ndarray)

في هذا المثال، سيتم تنفيذ ``numpy_fn`` في CUDA. ولجعل ذلك ممكناً، يقوم ``torch.compile`` تلقائيًا بنقل ``X`` و ``Y`` من CPU إلى CUDA، ثم ينقل النتيجة ``Z`` من CUDA إلى CPU. إذا كنا نقوم بتنفيذ هذه الدالة عدة مرات في نفس تشغيل البرنامج، فقد نرغب في تجنب جميع عمليات نسخ الذاكرة المكلفة هذه. للقيام بذلك، نحتاج فقط إلى تعديل دالة ``numpy_fn`` بحيث تقبل Tensor CUDA وتعيد Tensor. يمكننا القيام بذلك باستخدام ``torch.compiler.wrap_numpy``:

.. code-block:: python

   @torch.compile(fullgraph=True)
   @torch.compiler.wrap_numpy
   def numpy_fn(X, Y):
       return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))

   X = torch.randn(1024, 64, device="cuda")
   Y = torch.randn(1024, 64, device="cuda")
   Z = numpy_fn(X, Y)
   assert isinstance(Z, torch.Tensor)
   assert Z.device.type == "cuda"

هنا، نقوم بإنشاء Tensor بشكل صريح في ذاكرة CUDA، ونمررها إلى الدالة، والتي تقوم بجميع الحسابات على جهاز CUDA. وتكون ``wrap_numpy`` مسؤولة عن وضع علامة على أي مدخلات ``torch.Tensor`` كمدخلات ذات دلالة ``np.ndarray`` على مستوى ``torch.compile``. إن وضع علامات على Tensor داخل المحول البرمجي هو عملية رخيصة للغاية، لذلك لا يحدث أي نسخ أو نقل للبيانات أثناء وقت التشغيل.

باستخدام هذا الديكور، يمكننا أيضًا التمييز من خلال كود NumPy!

.. code-block:: python

   @torch.compile(fullgraph=True)
   @torch.compiler.wrap_numpy
   def numpy_fn(X, Y):
       return np.mean(np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1)))

   X = torch.randn(1024, 64, device="cuda", requires_grad=True)
   Y = torch.randn(1024, 64, device="cuda")
   Z = numpy_fn(X, Y)
   assert isinstance(Z, torch.Tensor)
   Z.backward()
   # X.grad الآن يحتوي على تدرج الحساب
   print(X.grad)

لقد كنا نستخدم ``fullgraph=True`` لأن كسور الجرافيك تسبب مشاكل في هذا السياق. عندما يحدث كسر في الرسم البياني، نحتاج إلى تجسيد صفائف NumPy. نظرًا لأن صفائف NumPy لا تحتوي على مفهوم "الجهاز" أو "requires_grad"، تضيع هذه المعلومات أثناء كسر الرسم البياني.

لا يمكننا تمرير التدرجات عبر كسر في الرسم البياني، لأن كود كسر الرسم البياني قد ينفذ تعليمات برمجية تعسفية لا نعرف كيفية التمييز بينها. من ناحية أخرى، في حالة التنفيذ CUDA، يمكننا حل هذه المشكلة كما فعلنا في المثال الأول، باستخدام مدير سياق ``torch.device("cuda")``:

.. code-block:: python

   @torch.compile
   @torch.compiler.wrap_numpy
   def numpy_fn(X, Y):
       prod = X[:, :, None] * Y[:, None, :]
       print("oops, a graph break!")
       return np.sum(prod, axis=(-2, -1))

   X = torch.randn(1024, 64, device="cuda")
   Y = torch.randn(1024, 64, device="cuda")

   with torch.device("cuda"):
       Z = numpy_fn(X, Y)
   assert isinstance(Z, torch.Tensor)
   assert Z.device.type == "cuda"

أثناء كسر الرسم البياني، لا تزال هناك حاجة إلى نقل Tensor الوسيطة إلى CPU، ولكن عندما يتم استئناف التتبع بعد كسر الرسم البياني، يتم تتبع بقية الرسم البياني لا يزال على CUDA. بالنظر إلى هذه الحركة CUDA <> CPU وCPU <> CUDA، فإن كسور الرسم البياني مكلفة إلى حد ما في سياق NumPy ويجب تجنبها، ولكنها تسمح على الأقل بتتبعها من خلال قطع من التعليمات البرمجية المعقدة.

كيف يمكنني تصحيح كود NumPy تحت ``torch.compile``؟
--------------------------------------------------

تصحيح الأخطاء في الكود المترجم JIT أمر صعب، نظرًا لتعقيد المحولات البرمجية الحديثة والأخطاء المخيفة التي تثيرها.
يحتوي "البرنامج التعليمي حول كيفية تشخيص الأخطاء أثناء التشغيل داخل torch.compile" على بعض النصائح والحيل حول كيفية أداء هذه المهمة.

إذا لم يكن ما سبق كافيًا لتحديد أصل المشكلة، فلا تزال هناك بعض الأدوات المحددة لـ NumPy والتي يمكننا استخدامها. يمكننا التمييز بين ما إذا كان الخطأ موجودًا بالكامل في كود PyTorch عن طريق تعطيل التتبع من خلال دالات NumPy:

.. code-block:: python

   from torch._dynamo import config
   config.trace_numpy = False

إذا كان الخطأ موجودًا في كود NumPy الذي تم تتبعه، فيمكننا تنفيذ كود NumPy بحماس (بدون ``torch.compile``) باستخدام PyTorch كخلفية عن طريق استيراد ``import torch._numpy as np``.
لا ينبغي استخدام هذا إلا لأغراض **التصحيح** وهو ليس بديلاً عن واجهة برمجة التطبيقات PyTorch بأي حال من الأحوال، لأنه **أقل أداءً بكثير**، وباعتباره واجهة برمجة تطبيقات خاصة، **قد يتغير دون إشعار**. وعلى أي حال، فإن ``torch._numpy`` هو تنفيذ Python لـ NumPy من حيث PyTorch ويستخدمه ``torch.compile`` داخليًا لتحويل كود NumPy إلى كود PyTorch. من السهل جدًا قراءته وتعديله، لذا إذا وجدت أي خطأ فيه، فلا تتردد في تقديم طلب سحب لإصلاحه أو ببساطة فتح مشكلة.

إذا كان البرنامج يعمل عند استيراد ``torch._numpy as np``، فمن المحتمل أن يكون الخطأ في TorchDynamo. إذا كان الأمر كذلك، يرجى فتح مشكلة مع "منتج قابل للتكرار بحد أدنى".

قمت بتجميع بعض كود NumPy باستخدام ``torch.compile`` ولم ألاحظ أي تسريع.
-------------------------------------------------------------------

أفضل مكان للبدء هو
"البرنامج التعليمي الذي يقدم المشورة العامة حول كيفية تصحيح مشكلات torch.compile هذه".

قد تحدث بعض كسور الرسم البياني بسبب استخدام ميزات غير مدعومة. راجع :ref: `nonsupported-numpy-feats`. بشكل عام، من المفيد أن نتذكر أن بعض ميزات NumPy المستخدمة على نطاق واسع لا تتوافق مع المحولات البرمجية. على سبيل المثال، تجعل التعديلات في المكان من الصعب الاستدلال داخل المحول البرمجي وغالبا ما تعطي أداء أسوأ من نظرائهم خارج المكان. لذلك، من الأفضل تجنبها. وينطبق الشيء نفسه على استخدام معلمة ``out=``. بدلاً من ذلك، يفضل استخدام العمليات خارج المكان والسماح لـ ``torch.compile`` بتحسين استخدام الذاكرة. وينطبق الشيء نفسه على العمليات المعتمدة على البيانات مثل الفهرسة المقنعة من خلال الأقنعة الثنائية، أو التحكم في التدفق المعتمد على البيانات مثل عبارات ``if`` أو ``while``.

أي واجهة برمجة تطبيقات يجب استخدامها للتتبع الدقيق؟
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

في بعض الحالات، قد تحتاج إلى استبعاد أجزاء صغيرة من الكود الخاص بك من تجميعات ``torch.compile``. يقدم هذا القسم بعض الإجابات ويمكنك العثور على مزيد من المعلومات في :ref: `torchdynamo_fine_grain_tracing`.

كيف أقوم بكسر الرسم البياني على دالة؟
-----------------------------------

كسر الرسم البياني على دالة غير كافٍ للتعبير عما تريده من PyTorch القيام به. تحتاج إلى أن تكون أكثر تحديدًا بشأن حالتك الاستخدام. فيما يلي بعض حالات الاستخدام الأكثر شيوعًا التي قد ترغب في مراعاتها:

* إذا كنت تريد تعطيل التجميع في هذا إطار الدالة والأطر المستدعاة بشكل متكرر، فاستخدم ``torch._dynamo.disable``.

* إذا كنت تريد أن يستخدم مشغل معين، مثل ``fbgemm`` الوضع الحريص، فاستخدم ``torch._dynamo.disallow_in_graph``.

بعض حالات الاستخدام غير الشائعة تشمل:

* إذا كنت تريد تعطيل TorchDynamo على إطار الدالة ولكن إعادة تمكينه على الأطر المستدعاة بشكل متكرر - استخدم ``torch._dynamo.disable(recursive=False)``.

* إذا كنت تريد منع دمج دالة إطار - استخدم ``torch._dynamo.graph_break`` في بداية الدالة التي تريد منع دمجها.

ما الفرق بين ``torch._dynamo.disable`` و ``torch._dynamo.disallow_in_graph``؟
-----------------------------------------------------------------------------------------------

يعمل Disallow-in-graph على مستوى المشغلات، أو بشكل أكثر تحديدًا، المشغلات التي تشاهدها في الرسوم البيانية المستخرجة من TorchDynamo.

يعمل Disable على مستوى إطار الدالة ويقرر ما إذا كان يجب على TorchDynamo البحث في إطار الدالة أم لا.

ما الفرق بين ``torch._dynamo.disable`` و ``torch._dynamo_skip``؟
----------------------------------------------------------------------------------

.. note::
   ``torch._dynamo_skip`` مهمل.

من المحتمل أن تحتاج إلى ``torch._dynamo.disable``. ولكن في سيناريو غير محتمل، قد تحتاج إلى مزيد من التحكم الدقيق. لنفترض أنك تريد تعطيل التتبع على دالة ``a_fn`` فقط، ولكنك تريد مواصلة التتبع مرة أخرى في ``aa_fn`` و ``ab_fn``. توضح الصورة أدناه حالة الاستخدام هذه:

.. figure:: _static/img/fine_grained_apis/call_stack_diagram.png
   :alt: رسم تخطيطي لـ torch.compile + disable(a_fn، recursive=False)

في هذه الحالة، يمكنك استخدام ``torch._dynamo.disable(recursive=False)``.
في الإصدارات السابقة، تم توفير هذه الوظيفة بواسطة ``torch._dynamo.skip``.
يتم الآن دعم ذلك بواسطة علم ``recursive`` داخل ``torch._dynamo.disable``.