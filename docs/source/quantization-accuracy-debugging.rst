تحديد أخطاء دقة التكميم وإصلاحها
----------------------

تقدم هذه الوثيقة استراتيجيات عامة لتحسين دقة التكميم. إذا كان هناك خطأ في النموذج المكّمم مقارنة بالنموذج الأصلي، فيمكننا تصنيف الخطأ إلى:

1. **خطأ غير حساس للبيانات** - ينتج عن خطأ تكميم متأصل في النموذج، حيث يكون لجزء كبير من بيانات الإدخال خطأ كبير.
2. **خطأ حساس للبيانات** - ينتج عن بيانات إدخال شاذة، حيث يكون لجزء صغير من بيانات الإدخال خطأ كبير.
3. **خطأ في التنفيذ** - عندما لا يتطابق الكيرنل المكّمم مع التنفيذ المرجعي.

الخطأ غير الحساس للبيانات
~~~~~~~~~~~~~~~~~~

نصائح عامة
^^^^^^^^

1. بالنسبة لتقنية PTQ، تأكد من أن البيانات التي تقوم بمعايرتها تمثيلية لمجموعة بياناتك. على سبيل المثال، بالنسبة لمشكلة التصنيف، تتمثل الإرشادات العامة في وجود عينات متعددة في كل فئة، وأن يكون العدد الإجمالي للعينات 100 على الأقل. لا توجد عقوبة للمعايرة باستخدام المزيد من البيانات بخلاف وقت المعايرة.
2. إذا كان نموذجك يحتوي على أنماط Conv-BN أو Linear-BN، ففكر في دمجها. إذا كنت تستخدم وضع الرسم البياني FX للتكميم، فإن سير العمل يقوم بذلك تلقائيًا. إذا كنت تستخدم وضع Eager للتكميم، فيمكنك القيام بذلك يدويًا باستخدام واجهة برمجة التطبيقات ``torch.ao.quantization.fuse_modules``.
3. زيادة دقة النوع dtype للعمليات المشكلة. عادة، سيكون لـ fp32 أعلى دقة، يليه fp16، ثم التكميم الديناميكي int8، ثم التكميم الثابت int8.

   1. ملاحظة: هذا يتطلب موازنة بين الأداء والدقة.
   2. ملاحظة: قد تختلف توفر الكيرنلات لكل نوع ولعملية حسابية لكل backend.
   3. ملاحظة: تضيف تحويلات النوع dtype تكلفة أداء إضافية. على سبيل المثال،
      ``fp32_op -> quant -> int8_op -> dequant -> fp32_op -> quant -> int8_op -> dequant``
      سيكون لها عقوبة أداء مقارنة بـ
      ``fp32_op -> fp32_op -> quant -> int8_op -> int8_op -> dequant``
      بسبب ارتفاع عدد تحويلات النوع dtype المطلوبة.

4. إذا كنت تستخدم PTQ، ففكر في استخدام QAT لاسترداد بعض فقدان الدقة الناتج عن التكميم.

نصائح تكميم int8
^^^^^^^^^^^^^^

1. إذا كنت تستخدم تكميم وزن per-tensor، ففكر في استخدام تكميم وزن per-channel.
2. إذا كنت تقوم بالاستدلال على `fbgemm`، فتأكد من تعيين وسيط `reduce_range` إلى `False` إذا كانت وحدة المعالجة المركزية الخاصة بك Cooperlake أو أحدث، وإلى `True` في الحالات الأخرى.
3. تدقيق توزيع تنشيط الإدخال عبر عينات مختلفة. إذا كان هذا التباين مرتفعًا، فقد تكون الطبقة مناسبة للتكميم الديناميكي ولكن ليس للتكميم الثابت.

الخطأ الحساس للبيانات
~~~~~~~~~~~~~~~

إذا كنت تستخدم التكميم الثابت ومجموعة صغيرة من بيانات الإدخال لديك تؤدي إلى خطأ تكميم مرتفع، فيمكنك تجربة ما يلي:

1. ضبط مجموعة بيانات المعايرة الخاصة بك لجعلها أكثر تمثيلاً لمجموعة بيانات الاستدلال الخاصة بك.
2. التفتيش اليدوي (باستخدام Numeric Suite) للطبقات التي تحتوي على خطأ تكميم مرتفع. بالنسبة لهذه الطبقات، ضع في اعتبارك تركها في النقطة العائمة أو ضبط إعدادات المراقب لتحديد مقياس أفضل ونقطة الصفر.

خطأ في التنفيذ
~~~~~~~~~~

إذا كنت تستخدم تكميم PyTorch مع backend الخاص بك، فقد ترى اختلافات بين التنفيذ المرجعي لعملية ما (مثل ``dequant -> op_fp32 -> quant``) وتنفيذ العملية المكّممة (مثل ``op_int8``) على الأجهزة المستهدفة. قد يعني هذا أحد أمرين:

1. الاختلافات (عادة ما تكون صغيرة) متوقعة بسبب السلوك المحدد لنواة الهدف على الأجهزة المستهدفة مقارنة بـ fp32/cpu. أحد أمثلة ذلك هو التراكم في نوع بيانات صحيح. ما لم تضمن النواة التطابق التام مع التنفيذ المرجعي، فإن هذا متوقع.
2. تحتوي النواة على الأجهزة المستهدفة على مشكلة في الدقة. في هذه الحالة، تواصل مع مطور النواة.

أدوات التصحيح العددي (النموذج الأولي)
---------------------------

.. toctree::
    :hidden:

    torch.ao.ns._numeric_suite
    torch.ao.ns._numeric_suite_fx

.. warning ::
     أدوات التصحيح العددي هي نموذج أولي مبكر وقد تتغير.

* :ref:`torch_ao_ns_numeric_suite`
  مجموعة الأرقام في وضع Eager
* :ref:`torch_ao_ns_numeric_suite_fx`
  مجموعة FX الرقمية