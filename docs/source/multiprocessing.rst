:orphan:

.. _multiprocessing-doc:

حزمة معالجة متعددة - torch.multiprocessing
=========================================

.. automodule:: torch.multiprocessing
.. currentmodule:: torch.multiprocessing

.. warning::

    إذا خرجت العملية الرئيسية فجأة (على سبيل المثال، بسبب إشارة واردة)،
    ففي بعض الأحيان تفشل Python's ``multiprocessing`` في تنظيف عملياتها الفرعية.
    إنه عيب معروف، لذا إذا كنت ترى أي تسرب للموارد بعد
    مقاطعة المفسر، فهذا يعني على الأرجح أن هذا حدث
    لك.

إدارة الاستراتيجية
------------

.. autofunction:: get_all_sharing_strategies
.. autofunction:: get_sharing_strategy
.. autofunction:: set_sharing_strategy


.. _multiprocessing-cuda-sharing-details:

مشاركة ناقلات CUDA
--------------------

يتم دعم مشاركة ناقلات CUDA بين العمليات فقط في Python 3، باستخدام
أساليب "spawn" أو "forkserver" للبدء.


على عكس ناقلات وحدة المعالجة المركزية (CPU)، يجب على العملية المرسلة الاحتفاظ بالناقلة الأصلية
طالما تحتفظ العملية المستقبلة بنسخة من الناقلة. يتم تنفيذ العد المرجعي تحت الغطاء ولكنه يتطلب من المستخدمين اتباع أفضل الممارسات التالية.

.. warning::
    إذا توقفت عملية المستهلك بشكل غير طبيعي بسبب إشارة خطيرة، فقد يتم الاحتفاظ بالناقلة المشتركة
    إلى الأبد في الذاكرة طالما أن العملية المرسلة قيد التشغيل.


1. حرر الذاكرة في المستهلك في أقرب وقت ممكن.

::

    ## جيد
    x = queue.get()
    # قم بشيء ما مع x
    del x

::

    ## سيء
    x = queue.get()
    # قم بشيء ما مع x
    # افعل كل شيء آخر (يجب على المنتج الاحتفاظ بـ x في الذاكرة)

2. استمر في تشغيل عملية المنتج حتى تخرج جميع المستهلكين. سيمنع هذا
الوضع الذي تقوم فيه عملية المنتج بتحرير الذاكرة التي لا تزال قيد الاستخدام
بواسطة المستهلك.

::

    ## المنتج
    # إرسال الناقلات، قم بشيء ما
    event.wait()


::

    ## المستهلك
    # استقبال الناقلات واستخدامها
    event.set()

3. لا تمرر الناقلات المستلمة.

::

    # لن ينجح
    x = queue.get()
    queue_2.put(x)


::

    # تحتاج إلى إنشاء نسخة محلية من العملية
    x = queue.get()
    x_clone = x.clone()
    queue_2.put(x_clone)


::

    # من المحتمل أن يؤدي وضع الناقلة والحصول عليها من نفس الطابور في نفس العملية إلى تعطل
    queue.put(tensor)
    x = queue.get()


استراتيجيات المشاركة
------------------

يقدم هذا القسم نظرة عامة موجزة حول كيفية عمل استراتيجيات المشاركة المختلفة. لاحظ أن هذا ينطبق فقط على ناقلات وحدة المعالجة المركزية (CPU) - ستستخدم ناقلات CUDA دائمًا
واجهة برمجة التطبيقات CUDA، حيث أنها الطريقة الوحيدة التي يمكن بها مشاركتها.

وصف الملف - ``file_descriptor``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


.. note::

    هذه هي الاستراتيجية الافتراضية (باستثناء macOS وOS X حيث لا يتم
    دعمها).

ستستخدم هذه الاستراتيجية مؤشرات الملفات كمقابض للذاكرة المشتركة. كلما تم نقل
التخزين إلى الذاكرة المشتركة، يتم تخزين مؤشر الملف الذي تم الحصول عليه من ``shm_open``
مع الكائن، وعندما يتم إرساله إلى عمليات أخرى،
سيتم نقل مؤشر الملف (على سبيل المثال، عبر مقابس UNIX) إليه. سيقوم المستقبل أيضًا بتخزين مؤشر الملف في ذاكرة التخزين المؤقتة وسيقوم بـ ``mmap``، للحصول على عرض مشترك لبيانات التخزين.

لاحظ أنه إذا كان هناك الكثير من الناقلات المشتركة، فستحتفظ هذه الاستراتيجية
بعدد كبير من مؤشرات الملفات المفتوحة معظم الوقت. إذا كان نظامك يحتوي على حدود منخفضة
لعدد مؤشرات الملفات المفتوحة، ولا يمكنك رفعها، فيجب عليك استخدام استراتيجية "file_system".

نظام الملفات - ``file_system``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

ستستخدم هذه الاستراتيجية أسماء الملفات المقدمة إلى ``shm_open`` لتحديد مناطق الذاكرة المشتركة. تتمثل ميزة ذلك في عدم الحاجة إلى تنفيذ ذاكرة التخزين المؤقت لمؤشرات الملفات التي تم الحصول عليها منها، ولكنه في نفس الوقت عرضة لتسرب الذاكرة المشتركة. لا يمكن حذف الملف فور إنشائه، لأن العمليات الأخرى تحتاج إلى الوصول إليه لفتح طرق العرض الخاصة بها. إذا تعطلت العمليات أو قُتلت بشكل مفاجئ، ولم تستدعي دالات التدمير للتخزين، فستظل الملفات موجودة في النظام. هذا أمر خطير للغاية، لأنها تستمر في استخدام الذاكرة حتى يتم إعادة تشغيل النظام، أو تحريرها يدويًا.

لمكافحة مشكلة تسرب ملف الذاكرة المشتركة، سينشئ :mod:`torch.multiprocessing`
عملية شيطان تسمى ``torch_shm_manager`` التي ستنفصل عن
مجموعة العمليات الحالية، وستقوم بتتبع جميع مخصصات الذاكرة المشتركة. بمجرد خروج جميع العمليات المتصلة بها، فستنتظر لحظة للتأكد من عدم وجود
اتصالات جديدة، وستقوم بالتحقق من جميع ملفات الذاكرة المشتركة
المخصصة بواسطة المجموعة. إذا وجد أن أي منها لا يزال موجودًا، فسيتم إلغاء تخصيصها. لقد اختبرنا هذه الطريقة وقد ثبت أنها قوية في مواجهة مختلف
الأعطال. ومع ذلك، إذا كان لدى نظامك حدود عالية بما فيه الكفاية، و
"file_descriptor" هي استراتيجية مدعومة، فنحن لا نوصي بالتبديل إلى هذا.

إنشاء العمليات الفرعية
---------------------

.. note::

   متاح لـ Python >= 3.4.

   يعتمد هذا على طريقة "spawn" للبدء في حزمة "multiprocessing" في Python.

يمكن إنشاء عدد من العمليات الفرعية لأداء بعض الوظائف
من خلال إنشاء مثيلات "Process" واستدعاء "join" للانتظار
اكتمالها. يعمل هذا النهج بشكل جيد عند التعامل مع عملية فرعية واحدة ولكنه يمثل مشكلات محتملة عند التعامل مع عمليات متعددة.

بمعنى، فإن الانضمام إلى العمليات بشكل متسلسل يعني أنها ستنهي
بالتسلسل. إذا لم يحدث ذلك، ولم تنته العملية الأولى،
سيتم تجاهل إنهاء العملية. أيضًا، لا توجد مرافق أصلية
لانتشار الأخطاء.

تعالج دالة "spawn" أدناه هذه المخاوف وتعتني بنشر الأخطاء،
إنهاء خارج الترتيب، وسوف ينهي العمليات بنشاط
عند اكتشاف خطأ في أحدها.

.. automodule:: torch.multiprocessing.spawn
.. currentmodule:: torch.multiprocessing.spawn

.. autofunction:: spawn

.. currentmodule:: torch.multiprocessing


.. class:: SpawnContext

   يتم إرجاعه بواسطة :func:`~spawn` عند استدعائه باستخدام ``join=False``.

   .. automethod:: join


.. تحتاج هذه الوحدة إلى توثيق. إضافته هنا في الوقت الحالي
.. لأغراض التتبع
.. py:module:: torch.multiprocessing.pool
.. py:module:: torch.multiprocessing.queue
.. py:module:: torch.multiprocessing.reductions