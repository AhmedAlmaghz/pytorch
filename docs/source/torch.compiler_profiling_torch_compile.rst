تحديد ملف التعريف لفهم أداء torch.compile
===================================

ما الذي يستخدم torch.profiler من أجله:
-------------------------------

يساعد torch.profiler على فهم أداء برنامجك على مستوى دقة النواة - على سبيل المثال، يمكنه عرض انقطاع الرسوم البيانية واستخدام GPU على مستوى البرنامج. غالبًا ما تساعد البيانات التي يوفرها المعالج الملف الشخصي المستخدمين على فهم المكان الذي يجب التحقيق فيه بشكل أكبر لفهم أداء النموذج.

لفهم الأداء على مستوى النواة، هناك أدوات أخرى. يمكن استخدام أداة NVIDIA ncu، أو أدوات إنشاء ملفات التعريف في :ref: `inductor <torchinductor-gpu-profiling>`.

راجع أيضًا الدليل العام لمعالج Python profiler <https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html>`_.

أساسيات استخدام torch.profiler وعرض التتبعات
--------------------------------------

**برنامج مثال**: سنستخدم هذا المثال لتحديد ملف تعريف resnet18. لاحظ الأجزاء التالية من برنامج المثال هذا:

* تضمين تشغيل تسخين للانتظار حتى اكتمال التجميع (سيقوم هذا بتسخين الأنظمة مثل مخصص ذاكرة التخزين المؤقت CUDA)
* استخدم سياق :code: `torch.profiler.profile ()` لتحديد ملف تعريف القسم الذي نحن مهتمون به
* استخدم :code: `prof.export_chrome_trace ("trace.json")` لتصدير قطعة التعريف.

.. code-block:: python

    import torch
    from torchvision.models import resnet18

    model = resnet18().cuda()
    inputs = [torch.randn((5, 3, 224, 224), device='cuda') for _ in range(10)]

    model_c = torch.compile(model)

    def fwd_bwd (inp):
        out = model_c (inp)
        out.sum (). backward ()

    # تسخين
    fwd_bwd (مدخلات [0])

    مع torch.profiler.profile () كما بروف:
        للعيون في النطاق (1،4):
            fwd_bwd (مدخلات [i])
            prof.step ()

    prof.export_chrome_trace ("trace.json")

**عرض التتبعات في Chrome**: في متصفح Chrome، افتح chrome://tracing وقم بتحميل ملف json. استخدم المفاتيح "w" و"s" للتكبير والتصغير، واستخدم "a" و"d" للتمرير يمينا ويسارا. سيظهر الضغط على "؟" شاشة "مساعدة" بقائمة من الاختصارات.

.. figure:: _static/img/profiling_torch_compile/basic_chrome_trace.png
    : بديل: مثال على تتبع أساسي، مرئي في عارض chrome://tracing.

هنا، نلاحظ:
* أحداث CompiledFunction و CompiledFunctionBackward، والتي تتوافق مع المناطق المجمعة بالدينامو.
* أحداث وحدة المعالجة المركزية في الأعلى، وأحداث GPU في الأسفل.

**التدفقات بين أحداث وحدة المعالجة المركزية و GPU**

يحدث كل نواة على GPU بعد إطلاقها من قبل التعليمات البرمجية التي تعمل على وحدة المعالجة المركزية. يمكن للمعالج الملف الشخصي رسم اتصالات (أي "التدفقات") بين أحداث وحدة المعالجة المركزية و GPU لإظهار حدث وحدة المعالجة المركزية الذي أطلق نواة GPU. هذا مفيد بشكل خاص لأن نوى GPU يتم إطلاقها بشكل غير متزامن، مع بعض الاستثناءات.

لعرض اتصال التدفق، انقر فوق نواة GPU وانقر فوق "ac2g":

.. figure:: _static/img/profiling_torch_compile/ac2g.png
    : بديل: العرض في عارض chrome://trace، والذي يظهر تدفقًا غير متزامن بين النواة وموقع الإطلاق.

أو، قم بتشغيل جميع التدفقات باستخدام القائمة المنسدلة "أحداث التدفق" في الأعلى.

التغلب على مشكلات ملفات تعريف رسومات CUDA
------------------------------------------

عندما يتم تمكين رسومات CUDA، يمكن لبعض تكوينات CUDA (إصدار برنامج التشغيل أقل من 525.85.12 أو CUDA <12) مواجهة مشكلات بين أدوات التعريف ورسومات CUDA. لإصلاح هذه المشكلات، أضف سياق تعريف فارغ في الجزء العلوي من برنامجك:

.. code-block:: python

    import torch

    torch.profiler._utils._init_for_cuda_graphs()

    # ... بقية البرنامج

فهم وقت التجميع
------------

لفهم سبب استغراق التجميع وقتًا طويلاً، يمكنك تحديد ملف تعريف الاستدعاء الأول لبرنامج torch.compile-ed. ضع في اعتبارك أن آثار تعريف التتبع للتجميعات يمكن أن تكون مشوهة أكثر من التعريف النموذجي، لأن أحمال عمل التجميع يمكن أن تكون مختلفة جدًا عن أحمال عمل PyTorch النموذجية. في بعض الحالات، قد تكون ملفات التتبع كبيرة جدًا أيضًا. يمكن أن يكون من الصعب فتح آثار > 1 غيغابايت باستخدام أداة التتبع Chrome.

ملاحظة: يمكن أيضًا الحصول على نفس المعلومات تقريبًا بتنسيق غير رسومي باستخدام :code: `torch._dynamo.utils.compile_times ()`. لن تعرض هذه الأداة المساعدة وقت حدوث خطوات التجميع، ولكنها ستظهر مقدار الوقت المستغرق في كل خطوة - ولن تتأثر الأوقات بأي تعقيد في التعريف.

انظر المثال أدناه:

.. code-block:: python

    import torch
    from torchvision.models import resnet18

    model = resnet18().cuda()
    inputs = [torch.randn((5, 3, 224, 224), device='cuda') for _ in range(10)]

    model_c = torch.compile(model)

    def fwd_bwd(inp):
        out = model_c(inp)
        out.sum().backward()

    def warmup_compile():
        def fn(x):
            return x.sin().relu()

        x = torch.rand((2, 2)، device='cuda'، requires_grad = True)
        fn_c = torch.compile (fn)
        الخروج = fn_c (x)
        out.sum (). backward ()

    مع torch.profiler.profile () كما بروف:
        مع torch.profiler.record_function ("warmup compile"):
            warmup_compile ()

        مع torch.profiler.record_function ("resnet18 compile"):
            fwd_bwd (مدخلات [0])

    prof.export_chrome_trace ("trace_compile.json")

.. figure:: _static/img/profiling_torch_compile/compilation_profiling.png
    : بديل: عرض في عارض chrome://trace، والذي يظهر خطوات التجميع الدينامو والمحث.

لاحظ بعض الأشياء:

* يجب أن يحدث الاستدعاء الأول *أثناء* التعريف لالتقاط التجميع
* أضف تجميعًا دافئًا لتهيئة أي أنظمة تحتاج إلى تهيئة كسولة.

البحث عن انقطاع الرسم البياني: "منطقة مجمعة بواسطة الشعلة" و "CompiledFunction"
--------------------------------------------------------------------

على الرغم من وجود أدوات تسجيل لتحديد انقطاع الرسم البياني، يوفر المعالج طريقة مرئية سريعة لتحديد :ref: `انقطاع الرسم البياني <torch.compiler_graph_breaks>`. هناك حدثان للمعالج الملف الشخصي للبحث عنهما: **منطقة مجمعة بواسطة الشعلة** و **CompiledFunction**.

**منطقة مجمعة بواسطة الشعلة** - التي تم تقديمها في PyTorch 2.2 - هي حدث المعالج الملف الشخصي الذي يغطي المنطقة المجمعة بأكملها. تبدو انقطاعات الرسم البياني دائمًا متشابهة: أحداث "منطقة مجمعة بواسطة الشعلة" متداخلة.

إذا قمت بتشغيل دالتين منفصلتين مع تطبيق torch.compile () بشكل مستقل على كل منهما، فيجب أن تتوقع بشكل عام رؤية منطقتين مجمعتين بواسطة الشعلة متجاورتين (أي غير مكدستين / متداخلتين). في الوقت نفسه، إذا واجهت انقطاعات في الرسم البياني (أو مناطق معطلة () / تم تخطيها)، فمن المتوقع أن تشاهد أحداث "منطقة مجمعة بواسطة الشعلة" متداخلة.

**CompiledFunction** - التي تم تقديمها في PyTorch 2.0 - هي حدث المعالج الملف الشخصي الذي يظهر عندما تكون الخلاصات مطلوبة لأي مدخلات. سوف يقاطع كل انقطاع في الرسم البياني كتلة CompiledFunction، مما يؤدي إلى انقسامها إلى قسمين. تظهر أحداث CompiledFunction فقط عندما تكون Autograd متضمنة، أي أن بعض تنسيقات الإدخال إلى الرسم البياني لها requires_grad=True.

عندما تظهر CompiledFunction في تتبع، عادة ما تكون مقترنة بـ CompiledFunctionBackward event في تمريرة الخلف. يجب أن يظهر "رابط fwd-bwd" في التتبع الذي يربط بين الاثنين، إذا تم استدعاء الدالة الخلفية.

إذا كانت حالتك الاستخدامية تتضمن رسمًا بيانيًا لا يتطلب grad ولا يتضمن أحداث "منطقة مجمعة بواسطة الشعلة"، فقد يكون من الصعب تحديد ما إذا كان يتم تطبيق torch.compile بشكل صحيح. قد تكون إحدى القرائن هي وجود نوى Triton التي تم إنشاؤها بواسطة المحث.

انظر المثال الاصطناعي أدناه للتوضيح:

.. code-block:: python

    import torch
    import torch._dynamo

    class ModelWithBreaks (torch.nn.Module):
        def __init__ (self):
            super (). __ init __ ()
            def create_sequential ():
                return torch.nn.Sequential (
                    torch.nn.Linear (128، 128)،
                    torch.nn.ReLU ()،
                    torch.nn.Linear (128، 128)،
                    torch.nn.ReLU ()،
                )
            self.mod1 = create_sequential ()
            self.mod2 = create_sequential ()
            self.mod3 = create_sequential ()
            self.mod4 = create_sequential ()

        def forward (self، inp):
            mod1 = self.mod1 (inp)
            torch._dynamo.graph_break ()
            mod2 = self.mod2 (mod1)
            torch._dynamo.graph_break ()
            mod3 = self.mod3 (mod2)
            torch._dynamo.graph_break ()
            mod4 = self.mod4 (mod3)
            return mod4


    model = ModelWithBreaks (). cuda ()
    المدخلات = [torch.randn ((128، 128)، device='cuda') للعيون في النطاق (10)]

    model_c = torch.compile (النموذج)

    def fwd_bwd (inp):
        out = model_c (inp)
        out.sum (). backward ()

    # تسخين
    fwd_bwd (مدخلات [0])

    مع torch.profiler.profile () كما بروف:
        للعيون في النطاق (1،4):
            fwd_bwd (مدخلات [i])
            prof.step ()

    prof.export_chrome_trace ("trace_break.json")

.. figure:: _static/img/profiling_torch_compile/graph_breaks_with_torch_compiled_region.png
    : بديل: عرض في عارض chrome://trace، والذي يظهر أحداث "منطقة مجمعة بواسطة الشعلة" متداخلة وأحداث CompiledFunction متعددة - مما يشير إلى انقطاعات الرسم البياني.

نوى المشغل
---------

عندما يتم إطلاق المشغل، نتوقع أن نرى بعض الأحداث:

1. حدث جانب وحدة المعالجة المركزية
2. إطلاق النواة (إذا كان الأمر يتعلق بنواة GPU)
3. حدث جانب GPU

.. figure:: _static/img/profiling_torch_compile/kernel_launch_labeled.png
    : بديل: عرض في عارض chrome://trace، والذي يظهر الأنواع الثلاثة من الأحداث: حدث جانب وحدة المعالجة المركزية، وإطلاق النواة، وحدث جانب GPU

**نوى Triton التي تم إنشاؤها بواسطة المحث:**
1. يجب أن يظهر **حدث جانب وحدة المعالجة المركزية** كحدث مسبوق بـ "triton_". تحتوي الأحداث حاليًا على معلومات قليلة - اسم النواة والإطلاق، ولكن أقل من معلومات عمليات إطلاق نواة aten النموذجية (التي تحتوي على أشكال الإدخال وأنواعه، وما إلى ذلك).
2. يجب أن يظهر **إطلاق النواة** كـ cuLaunchKernel بدلاً من cudaLaunchKernel (cudaLaunchKernel هو المعتاد لعمليات aten)
3. يجب أن يظهر **حدث جانب GPU**، ويعتمد مدى وصف الاسم على تكوين المحث لـ unique_kernel_names

.. figure:: _static/img/profiling_torch_compile/triton_kernel_launch.png

**نوى Triton غير المولدة بواسطة المحث:**

1. قد لا يظهر **جانب وحدة المعالجة المركزية** في التتبعات؛ حيث يتم حاليًا تنفيذ آلية إدراج حدث المعالج الملف الشخصي تلقائيًا على مستوى المحث، لذلك قد لا تظهر نوى Triton التي تتجاوز المحث في التتبعات، ما لم يقم المستخدمون بوضع علامات عليها يدويًا
2. يجب أن يظهر **إطلاق النواة** كـ cuLaunchKernel بدلاً من cudaLaunchKernel (cudaLaunchKernel هو المعتاد لعمليات aten)
3. يجب أن يظهر **جانب GPU**، ويتم تسمية الحدث بشكل مشابه لنواة triton التي تم تأليفها.

.. figure:: _static/img/profiling_torch_compile/noninductor_triton_kernel.png

**نوى وحدة المعالجة المركزية التي تم إنشاؤها بواسطة المحث:**

1. لن يظهر **حدث جانب وحدة المعالجة المركزية** في التتبعات؛ لم نقم بإضافة التعريف لهذا بعد.
2. لا وجود لإطلاق **النواة** وأحداث **جانب GPU**

من المتوقع أيضًا أن تظهر **نوى غير نوى Triton** (أي نوى aten أو العمليات المخصصة) في بعض الأحيان في التتبعات. في بعض الأحيان، قد يعود المحث إلى تنفيذ العملية الأصلية، وفي هذه الحالة سترى مكالمة إلى عملية aten.


الوقت المستغرق للإطلاق
---------------

تتمثل إحدى المشكلات الشائعة في انخفاض استخدام GPU. تتمثل إحدى الطرق السريعة لتحديد ذلك في وجود فجوات كبيرة بين النوى على GPU:

.. figure:: _static/img/profiling_torch_compile/cpu_bound.png
    : بديل: عرض في عارض chrome://trace، والذي يظهر فجوات كبيرة بين نوى GPU. يشير هذا إلى أن النموذج مقيد بوحدة المعالجة المركزية، على الأرجح بسبب التعقيد أثناء عمليات إطلاق النواة.

غالبًا ما يكون هذا نتيجة لتعقيد وحدة المعالجة المركزية، على سبيل المثال، إذا كان الوقت المستغرق على وحدة المعالجة المركزية بين عمليات إطلاق النواة أكبر من الوقت الذي تستغرقه GPU لمعالجة النوى. تعد هذه المشكلة أكثر شيوعًا لحجم الدفعات الصغيرة.

عند استخدام المحث، يمكن أن يساعد تمكين رسومات CUDA غالبًا في تحسين الأداء عندما يكون الوقت المستغرق للإطلاق مصدر قلق.