.. _ddp:

التدريب المتوازي للبيانات الموزعة
=======================

.. warning::
   يتطور تنفيذ :class:`torch.nn.parallel.DistributedDataParallel`
   مع مرور الوقت. وقد كُتبت هذه الملاحظة التصميمية بناءً على الحالة اعتبارًا من الإصدار 1.4.

:class:`torch.nn.parallel.DistributedDataParallel` (DDP) يقوم بالتدريب المتوازي الموزع للبيانات بشكل شفاف. تصف هذه الصفحة كيفية عمله وتكشف عن تفاصيل التنفيذ.

مثال
^^^^^^^

دعونا نبدأ بمثال بسيط على :class:`torch.nn.parallel.DistributedDataParallel`. يستخدم هذا المثال :class:`torch.nn.Linear` كنموذج محلي، ويغلّفه في DDP، ثم يقوم بتشغيل عملية توجيه واحدة، وعملية تمرير خلفي، وخطوة محسن على نموذج DDP. بعد ذلك، يتم تحديث المعلمات في النموذج المحلي، ويجب أن تكون جميع النماذج في العمليات المختلفة متطابقة تمامًا.

.. code::

   استورد الشعلة
   استورد الشعلة. distributed as dist
   استورد الشعلة. multiprocessing as mp
   استورد الشعلة. nn as nn
   استورد الشعلة. optim as optim
   استورد os
   من الشعلة. nn.parallel استورد DistributedDataParallel as DDP


   def example(rank, world_size):
       # قم بإنشاء مجموعة عمليات افتراضية
       dist.init_process_group("gloo", rank=rank, world_size=world_size)
       # قم بإنشاء نموذج محلي
       النموذج = nn.Linear(10، 10).to(rank)
       # قم ببناء نموذج DDP
       ddp_model = DDP(model, device_ids=[rank])
       # قم بتعريف دالة الخسارة والمحسن
       loss_fn = nn.MSELoss()
       المحسن = optim.SGD(ddp_model.parameters(), lr=0.001)

       # تمرير للأمام
       المخرجات = ddp_model(torch.randn(20, 10).to(rank))
       العلامات = torch.randn(20, 10).to(rank)
       # تمرير الخلفي
       loss_fn(outputs, labels).backward()
       # تحديث المعلمات
       optimizer.step()

   def main():
       world_size = 2
       mp.spawn(example,
           args=(world_size,),
           nprocs=world_size,
           join=True)

   if __name__=="__main__":
       # متغيرات البيئة التي يجب تعيينها
       #
       # عند استخدام وضع "env" الافتراضي في c10d.
       os.environ["MASTER_ADDR"] = "localhost"
       os.environ["MASTER_PORT"] = "29500"
       main()

يعمل DDP مع TorchDynamo. عند استخدامه مع TorchDynamo، قم بتطبيق غلاف نموذج DDP
قبل تجميع النموذج، بحيث يمكن لـ torchdynamo تطبيق ``DDPOptimizer``
(تحسين كسر الرسوم البيانية) بناءً على أحجام دلو DDP. (راجع `TorchDynamo DDPOptimizer <./ddp.html#torchdynamo-ddpoptimizer>`_ لمزيد من المعلومات.)


.. code::

       ddp_model = DDP(model, device_ids=[rank])
       ddp_model = torch.compile(ddp_model)

التصميم الداخلي
^^^^^^^^^^^^^^^

يكشف هذا القسم عن كيفية عمله تحت غطاء المحرك
:class:`torch.nn.parallel.DistributedDataParallel` من خلال الغوص في تفاصيل
كل خطوة في إحدى التكرارات.

- **المتطلب الأساسي**: يعتمد DDP على ``ProcessGroup`` من c10d للاتصالات.
  وبالتالي، يجب على التطبيقات إنشاء مثيلات ``ProcessGroup`` قبل بناء
  DDP.

- **البناء**: يأخذ منشئ DDP مرجعًا للنموذج المحلي،
  ويبث ``state_dict()`` من العملية ذات الترتيب 0 إلى جميع العمليات الأخرى
  في المجموعة للتأكد من أن جميع النسخ المتماثلة للنموذج تبدأ من
  الحالة نفسها بالضبط. بعد ذلك، تقوم كل عملية DDP بإنشاء مثيل محلي لـ ``Reducer``، والذي
  سيتولى لاحقًا رعاية مزامنة التدرجات أثناء التمرير الخلفي. لتحسين كفاءة الاتصال،
  ينظم "المخفض" التدرجات المعلمية في دلاء، ويقلل دلوًا واحدًا في كل مرة. يمكن تكوين حجم الدلو
  من خلال تعيين وسيط `bucket_cap_mb` في منشئ DDP. يتم تحديد تعيين التدرجات المعلمية إلى الدلاء
  في وقت البناء، بناءً على حد حجم الدلو وأحجام المعلمات. يتم تخصيص معلمات النموذج
  في الدلاء بترتيب (تقريبي) عكسي
  من ``Model.parameters()`` من النموذج المُعطى. والسبب في استخدام الترتيب العكسي
  هو أن DDP يتوقع أن تصبح التدرجات جاهزة أثناء التمرير الخلفي
  في هذا الترتيب تقريبًا. يوضح الشكل أدناه مثالًا. لاحظ
  أن "grad0" و"grad1" موجودان في "bucket1"، والتدرجات الأخرى
  في "bucket0". بالطبع، قد لا يكون هذا الافتراض صحيحًا دائمًا، وعندما يحدث ذلك، فقد يؤثر سلبًا على سرعة DDP الخلفية حيث
  لا يمكن لـ "المخفض" بدء الاتصال في أقرب وقت ممكن.
  بالإضافة إلى التجزئة، يقوم "المخفض" أيضًا بتسجيل خطافات autograd أثناء
  البناء، وخطاف واحد لكل معلمة. سيتم تشغيل هذه الخطافات أثناء
  التمرير الخلفي عندما تصبح التدرجات جاهزة.

- **تمرير للأمام**: يأخذ DDP الإدخال ويمرره إلى النموذج المحلي،
  ثم يحلل الإخراج من النموذج المحلي إذا
  تم تعيين ``find_unused_parameters`` إلى ``True``. يسمح هذا الوضع بتشغيل الخلفي على رسم بياني فرعي للنموذج،
  ويكتشف DDP المعلمات المشاركة في التمرير الخلفي عن طريق التنقل في الرسم البياني لـ autograd من إخراج النموذج
  ووضع علامة على جميع المعلمات غير المستخدمة على أنها جاهزة للتخفيض. أثناء
  التمرير الخلفي، سوف ينتظر "المخفض" فقط التدرجات غير الجاهزة، ولكنه
  سيقلل جميع الدلاء على أي حال. لا يساعد وضع علامة على تدرج المعلمة على أنها جاهزة
  DDP على تخطي الدلاء في الوقت الحالي، ولكنه سيمنعه من الانتظار إلى الأبد أثناء التمرير الخلفي
  التدرجات الغائبة. لاحظ أن التنقل في الرسم البياني لـ autograd يقدم نفقات عامة إضافية، لذلك يجب على التطبيقات
  تعيين ``find_unused_parameters`` إلى ``True`` فقط عند الضرورة.

- **تمرير الخلفي**: يتم استدعاء دالة ``backward()`` مباشرة على فقدان
  ``Tensor``، وهو خارج نطاق تحكم DDP، ويستخدم DDP خطافات autograd المسجلة في وقت البناء
  لتشغيل مزامنة التدرجات. عندما تصبح إحدى التدرجات جاهزة، يتم تشغيل خطاف DDP المقابل على ذلك
  سيتم تشغيل مكدس التدرجات، وبعد ذلك سيقوم DDP بوضع علامة على تدرج المعلمة على أنها جاهزة للتخفيض. عندما تكون التدرجات في دلو واحد
  تصبح جاهزة، يقوم "المخفض" بتشغيل "allreduce" غير متزامن على ذلك الدلو لحساب متوسط التدرجات عبر جميع العمليات.
  عندما تكون جميع الدلاء جاهزة،
  ينتظر "المخفض" جميع عمليات "allreduce" حتى تنتهي.
  عندما يتم ذلك، يتم كتابة المتوسطات التدرجية في حقل "param.grad"
  من جميع المعلمات. لذلك، بعد التمرير الخلفي، يجب أن يكون حقل "grad" على نفس
  يجب أن تكون المعلمة المقابلة عبر عمليات DDP المختلفة متطابقة.

- **خطوة المحسن**: من منظور المحسن، فهو يقوم بتحسين نموذج محلي.
  يمكن أن تظل نماذج النسخ المتماثلة على جميع عمليات DDP متزامنة لأنها تبدأ جميعها من نفس الحالة
  ولديهم نفس المتوسطات التدرجية في كل تكرار.


.. image:: https://user-images.githubusercontent.com/16999635/72401724-d296d880-371a-11ea-90ab-737f86543df9.png
    :alt: ddp_grad_sync.png
    :width: 700 بكسل

.. note::
   يتطلب DDP أن تقوم مثيلات "المخفض" على جميع العمليات باستدعاء "allreduce"
   بنفس الترتيب بالضبط، وهو ما يتم عن طريق تشغيل "allreduce" دائمًا
   في ترتيب فهرس الدلو بدلاً من الترتيب الفعلي للدلاء الجاهز. يمكن أن يؤدي عدم تطابق
   ترتيب "allreduce" عبر العمليات إلى نتائج خاطئة أو تعليق DDP الخلفي.

التنفيذ
^^^^^^^^^^^^^^

فيما يلي مؤشرات إلى مكونات تنفيذ DDP. يوضح الرسم البياني المكدس هيكل التعليمات البرمجية.

ProcessGroup
------------

- `ProcessGroup.hpp <https://github.com/pytorch/pytorch/blob/v1.7.0/torch/lib/c10d/ProcessGroup.hpp>`__:
  يحتوي على واجهة برمجة التطبيقات المجردة لجميع عمليات تنفيذ مجموعة العمليات. توفر مكتبة ``c10d``
  3 عمليات تنفيذ خارج الصندوق، وهي
  `ProcessGroupGloo`، و`ProcessGroupNCCL`، و`ProcessGroupMPI`.
  يستخدم ``DistributedDataParallel`` ``ProcessGroup::broadcast()`` لإرسال
  حالات النموذج من العملية ذات الترتيب 0 إلى العمليات الأخرى أثناء التهيئة
  و ``ProcessGroup::allreduce()`` لمجموع التدرجات.


- `Store.hpp <https://github.com/pytorch/pytorch/blob/v1.7.0/torch/lib/c10d/Store.hpp>`__:
  يساعد خدمة النداء لـ مثيلات مجموعة العمليات للعثور على بعضها البعض.

DistributedDataParallel
-----------------------

- `distributed.py <https://github.com/pytorch/pytorch/blob/v1.7.0/torch/nn/parallel/distributed.py>`__:
  هي نقطة الدخول Python لـ DDP. فهو ينفذ خطوات التهيئة ووظيفة "forward" لـ
  وحدة ``nn.parallel.DistributedDataParallel``
  والتي تستدعي مكتبات C++. تقوم وظيفة ``_sync_param`` الخاصة بها بأداء مزامنة المعلمات داخل العملية
  عندما تعمل إحدى عمليات DDP على أجهزة متعددة، كما تقوم ببث مخازن النموذج من العملية ذات الترتيب 0 إلى جميع العمليات الأخرى.
  يحدث التزامن بين العمليات في ``Reducer.cpp``.

- `comm.h <https://github.com/pytorch/pytorch/blob/v1.7.0/torch/csrc/distributed/c10d/comm.h>`__:
  ينفذ دالة المساعدة للبث المجمع التي يتم استدعاؤها لبث حالات النموذج أثناء التهيئة ومزامنة مخازن النموذج
  قبل التمرير للأمام.

- `reducer.h <https://github.com/pytorch/pytorch/blob/v1.7.0/torch/csrc/distributed/c10d/reducer.h>`__:
  يوفر التنفيذ الأساسي لمزامنة التدرجات في التمرير الخلفي. لديها ثلاث وظائف نقطة دخول:

  * ``Reducer``: يتم استدعاء المنشئ في ``distributed.py`` الذي يسجل ``Reducer::autograd_hook()``
    إلى مراكمات التدرجات.
  * يتم استدعاء دالة ``autograd_hook()`` بواسطة محرك autograd عندما
    تصبح إحدى التدرجات جاهزة.
  * يتم استدعاء ``prepare_for_backward()`` في نهاية تمرير DDP للأمام في
    ``distributed.py``. إنه يتنقل في الرسم البياني لـ autograd للعثور على المعلمات غير المستخدمة عندما
    يتم تعيين ``find_unused_parameters`` إلى ``True`` في منشئ DDP.

.. image:: https://user-images.githubusercontent.com/16999635/72313120-4e7c1c80-3658-11ea-9c6d-44336b2daeac.png
    :alt: ddp_code.png
    :width: 400 بكسل


محسن DDP من TorchDynamo
------------------------

تأتي الميزة الأدائية لـ DDP من تداخل عمليات الجمع allreduce مع الحسابات أثناء التمرير الخلفي.
يمنع AotAutograd هذا التداخل عند استخدامه مع TorchDynamo لتجميع رسم بياني للأمامي بالكامل ورسم بياني للخلفي بالكامل،
لأن عمليات الجمع allreduce يتم إطلاقها بواسطة خطافات autograd _بعد_ انتهاء عملية الحساب الخلفي المحسنة بالكامل.

يساعد محسن DDP من TorchDynamo عن طريق كسر الرسم البياني للأمامي عند الحدود المنطقية لدلاء allreduce الخاصة بـ DDP
أثناء التمرير الخلفي. لاحظ: الهدف هو كسر الرسم البياني أثناء التمرير الخلفي، وأبسط تنفيذ هو
كسر الرسوم البيانية للأمامي ثم استدعاء AotAutograd والتجميع على كل قسم. يسمح هذا لـ DDP بإطلاق خطافات allreduce
فيما بين أقسام التمرير الخلفي، وجدولة الاتصالات لتتداخل مع الحساب.

راجع `هذه التدوينة <https://dev-discuss.pytorch.org/t/torchdynamo-update-9-making-ddp-work-with-torchdynamo/860/1>`_ للحصول على
شرح أكثر تعمقًا ونتائج تجريبية، أو اقرأ الوثائق والتعليمات البرمجية في
`torch/_dynamo/optimizations/distributed.py <https://github.com/pytorch/pytorch/blob/bbc39b7bb48d28d67e3253a89cc82df3687ddd1b/torch/_dynamo/backends/distributed.py#L124>`_

لتصحيح أخطاء محسن DDP، قم بتعيين `TORCH_LOGS='ddp_graphs'` لتفريغ الرسوم البيانية الكاملة. للحصول على سجلات بدون رسوم بيانية، أضف أيًا من 'dynamo' أو 'distributed' أو 'dist_ddp' إلى `TORCH_LOGS`
(للحصول على معلومات أساسية حول حدود الدلو). لتعطيل محسن DDP، قم بتعيين `torch._dynamo.config.optimize_ddp=False`.
يجب أن يعمل DDP وTorchDynamo بشكل صحيح دون محسن DDP، ولكن مع تدهور الأداء.