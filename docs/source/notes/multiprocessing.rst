.. _multiprocessing-best-practices:

أفضل الممارسات في المعالجة المتعددة
==================================

:mod:`torch.multiprocessing` هو بديل مباشر لوحدة :mod:`python:multiprocessing` في بايثون. تدعم الوحدة نفس العمليات تمامًا، ولكنها تمددها، بحيث يتم نقل جميع المنسقات التي يتم إرسالها عبر :class:`python:multiprocessing.Queue` إلى الذاكرة المشتركة، ولن يتم إرسال سوى مؤشر إلى عملية أخرى.

.. note::

   عندما يتم إرسال :class:`~torch.Tensor` إلى عملية أخرى، يتم مشاركة بيانات :class:`~torch.Tensor`. إذا لم يكن :attr:`torch.Tensor.grad` ``None``، فإنه يتم مشاركته أيضًا. بعد إرسال :class:`~torch.Tensor` بدون حقل :attr:`torch.Tensor.grad` إلى العملية الأخرى، فإنه يقوم بإنشاء منسق ``grad`` قياسي خاص بالعملية لا يتم مشاركته تلقائيًا عبر جميع العمليات، على عكس بيانات :class:`~torch.Tensor` التي تم مشاركتها.

يسمح ذلك بتنفيذ طرق تدريب مختلفة، مثل Hogwild وA3C، أو أي طرق أخرى تتطلب عملية غير متزامنة.

.. _multiprocessing-cuda-note:

استخدام CUDA في المعالجة المتعددة
------------------------------

لا يدعم وقت تشغيل CUDA طريقة "fork" لبدء التشغيل؛ حيث تتطلب طريقة "spawn" أو "forkserver" لبدء التشغيل لاستخدام CUDA في العمليات الفرعية.

.. note::
   يمكن تعيين طريقة بدء التشغيل إما من خلال إنشاء سياق باستخدام ``multiprocessing.get_context(...)`` أو مباشرة باستخدام ``multiprocessing.set_start_method(...)``.

على عكس المنسقات التي تعمل على وحدة المعالجة المركزية (CPU)، يجب على العملية المرسلة الاحتفاظ بالمنسق الأصلي طالما أن العملية المستقبلة تحتفظ بنسخة من المنسق. يتم تنفيذ ذلك تحت الغطاء، ولكنه يتطلب من المستخدمين اتباع أفضل الممارسات لتشغيل البرنامج بشكل صحيح. على سبيل المثال، يجب أن تبقى العملية المرسلة نشطة طالما أن العملية المستقبلة لديها مراجع للمنسق، ولا يمكن للعد المرجعي أن ينقذك إذا خرجت العملية المستقبلة بشكل غير طبيعي عبر إشارة خطيرة. راجع :ref:`هذا القسم <multiprocessing-cuda-sharing-details>`.

انظر أيضًا: :ref:`cuda-nn-ddp-instead`

أفضل الممارسات والنصائح
-----------------------

تجنب المأزق ومحاربتها
^^^^^^^^^^^^^^^^^^^^^^^^

هناك الكثير من الأشياء التي يمكن أن تسوء عندما تتم توليد عملية جديدة، مع كون السبب الأكثر شيوعًا للمأزق هو الخيوط الخلفية. إذا كان هناك أي خيط يحتفظ بقفل أو يستورد وحدة نمطية، ويتم استدعاء "fork"، فمن المحتمل جدًا أن تكون العملية الفرعية في حالة تالفة وتتوقف أو تفشل بطريقة أخرى. لاحظ أنه حتى إذا لم تقم بذلك، فإن مكتبات بايثون المدمجة تفعل ذلك - لا داعي للنظر إلى أبعد من :mod:`python:multiprocessing`. :class:`python:multiprocessing.Queue` هو في الواقع فئة معقدة للغاية، تقوم بتوليد خيوط متعددة تستخدم لتهيئة التسلسل وإرسال الأشياء واستقبالها، ويمكن أن تسبب هذه الخيوط المشكلات المذكورة أعلاه أيضًا. إذا وجدت نفسك في مثل هذا الموقف، فحاول استخدام :class:`~python:multiprocessing.queues.SimpleQueue`، الذي لا يستخدم أي خيوط إضافية.

نبذل قصارى جهدنا لجعل الأمر سهلاً عليك وضمان عدم حدوث هذه المآزق، ولكن بعض الأمور خارجة عن سيطرتنا. إذا كانت لديك أي مشكلات لا يمكنك التعامل معها لبعض الوقت، فحاول التواصل عبر المنتديات، وسنرى ما إذا كانت هناك مشكلة يمكننا إصلاحها.

إعادة استخدام المصدّرات التي تم تمريرها عبر طابور الانتظار
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

تذكر أنه في كل مرة تضع فيها :class:`~torch.Tensor` في :class:`python:multiprocessing.Queue`، يجب نقله إلى الذاكرة المشتركة. إذا كان مشتركًا بالفعل، فسيتم تجاهل الأمر، وإلا فسيؤدي ذلك إلى نسخ ذاكرة إضافية يمكن أن تبطئ العملية بأكملها. حتى إذا كان لديك مجموعة من العمليات التي ترسل البيانات إلى عملية واحدة، فقم بجعلها ترسل المصدّرات مرة أخرى - فهذا مجاني تقريبًا وسيسمح لك بتجنب النسخ عند إرسال الدفعة التالية.

التدريب متعدد العمليات غير المتزامن (مثل Hogwild)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

باستخدام :mod:`torch.multiprocessing`، من الممكن تدريب نموذج بشكل غير متزامن، مع مشاركة المعلمات إما طوال الوقت، أو يتم مزامنتها بشكل دوري. في الحالة الأولى، نوصي بإرسال كائن النموذج بالكامل، بينما في الحالة الأخيرة، ننصح بإرسال :meth:`~torch.nn.Module.state_dict` فقط.

نوصي باستخدام :class:`python:multiprocessing.Queue` لتمرير جميع أنواع كائنات PyTorch بين العمليات. من الممكن، على سبيل المثال، وراثة المنسقات والتخزين الموجود بالفعل في الذاكرة المشتركة، عند استخدام طريقة "fork" لبدء التشغيل، ومع ذلك، فهي عرضة للأخطاء ويجب استخدامها بحذر، فقط من قبل المستخدمين المتقدمين. تعد الطوابير، على الرغم من أنها في بعض الأحيان حل أقل أناقة، ستعمل بشكل صحيح في جميع الحالات.

.. warning::

   يجب أن تكون حذرًا بشأن وجود عبارات عامة، والتي لا يتم حمايتها باستخدام ``if __name__ == '__main__'``. إذا تم استخدام طريقة بدء تشغيل مختلفة عن "fork"، فسيتم تنفيذها في جميع العمليات الفرعية.

Hogwild
~~~~~~~

يمكن العثور على تنفيذ ملموس لـ Hogwild في مستودع الأمثلة__، ولكن لإظهار الهيكل العام للرمز، هناك أيضًا مثال بسيط أدناه::

   import torch.multiprocessing as mp
   from model import MyModel

   def train(model):
       # إنشاء data_loader وoptimizer، إلخ
       for data, labels in data_loader:
           optimizer.zero_grad()
           loss_fn(model(data), labels).backward()
           optimizer.step()  # سيقوم هذا بتحديث المعلمات المشتركة

   if __name__ == '__main__':
       num_processes = 4
       model = MyModel()
       # ملاحظة: هذا مطلوب لكي تعمل طريقة "fork"
       model.share_memory()
       processes = []
       for rank in range(num_processes):
           p = mp.Process(target=train, args=(model,))
           p.start()
           processes.append(p)
       for p in processes:
           p.join()

.. __: https://github.com/pytorch/examples/tree/master/mnist_hogwild

وحدة المعالجة المركزية في المعالجة المتعددة
------------------------------

يمكن أن يؤدي الاستخدام غير المناسب للمعالجة المتعددة إلى الإفراط في اشتراك وحدة المعالجة المركزية، مما يتسبب في تنافس العمليات المختلفة على موارد وحدة المعالجة المركزية، مما يؤدي إلى انخفاض الكفاءة.

سيوضح هذا البرنامج التعليمي ما هو الإفراط في اشتراك وحدة المعالجة المركزية وكيفية تجنبه.

الإفراط في اشتراك وحدة المعالجة المركزية
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

الإفراط في اشتراك وحدة المعالجة المركزية هو مصطلح تقني يشير إلى حالة يتم فيها تجاوز العدد الإجمالي لوحدات المعالجة المركزية الافتراضية (vCPUs) المخصصة لنظام ما للعدد الإجمالي لوحدات المعالجة المركزية الافتراضية المتوفرة على الأجهزة.

يؤدي هذا إلى حدوث تنافس شديد على موارد وحدة المعالجة المركزية. في مثل هذه الحالات، يكون هناك تبديل متكرر بين العمليات، مما يزيد من النفقات العامة للتبديل بين العمليات ويقلل من كفاءة النظام بشكل عام.

راجع الإفراط في اشتراك وحدة المعالجة المركزية مع أمثلة التعليمات البرمجية في تنفيذ Hogwild في مستودع `الأمثلة <https://github.com/pytorch/examples/tree/main/mnist_hogwild>`__.

عند تشغيل مثال التدريب باستخدام الأمر التالي على وحدة المعالجة المركزية (CPU) باستخدام 4 عمليات:

.. code-block:: bash

   python main.py --num-processes 4

افترض أن هناك N وحدة معالجة مركزية افتراضية (vCPUs) متوفرة على الجهاز، فإن تنفيذ الأمر السابق سيؤدي إلى إنشاء 4 عمليات فرعية. ستقوم كل عملية فرعية بتخصيص N وحدة معالجة مركزية افتراضية لنفسها، مما يؤدي إلى متطلبات تبلغ 4*N وحدة معالجة مركزية افتراضية. ومع ذلك، لا يحتوي الجهاز سوى على N وحدة معالجة مركزية افتراضية. وبالتالي، ستنافس العمليات المختلفة على الموارد، مما يؤدي إلى التبديل المتكرر للعمليات.

تشير الملاحظات التالية إلى وجود إفراط في اشتراك وحدة المعالجة المركزية:

#. ارتفاع استخدام وحدة المعالجة المركزية: من خلال استخدام الأمر ``htop``، يمكنك ملاحظة أن استخدام وحدة المعالجة المركزية مرتفع باستمرار، وغالبًا ما يصل إلى أو يتجاوز سعتها القصوى. يشير هذا إلى أن الطلب على موارد وحدة المعالجة المركزية يتجاوز عدد الأنوية المادية، مما يتسبب في حدوث تنافس بين العمليات على وقت وحدة المعالجة المركزية.

#. التبديل المتكرر للسياق مع انخفاض كفاءة النظام: في حالة الإفراط في اشتراك وحدة المعالجة المركزية، تتنافس العمليات على وقت وحدة المعالجة المركزية، ويجب على نظام التشغيل التبديل بسرعة بين العمليات المختلفة لتخصيص الموارد بشكل عادل. يضيف هذا التبديل المتكرر للسياق النفقات العامة ويقلل من الكفاءة الإجمالية للنظام.

تجنب الإفراط في اشتراك وحدة المعالجة المركزية
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

تتمثل إحدى الطرق الجيدة لتجنب الإفراط في اشتراك وحدة المعالجة المركزية في التخصيص المناسب للموارد. تأكد من أن عدد العمليات أو الخيوط التي تعمل بشكل متزامن لا يتجاوز موارد وحدة المعالجة المركزية المتاحة.

في هذه الحالة، قد يتمثل الحل في تحديد عدد الخيوط المناسب في العمليات الفرعية. يمكن تحقيق ذلك من خلال تعيين عدد الخيوط لكل عملية باستخدام دالة ``torch.set_num_threads(int)`` في العملية الفرعية.

افترض أن هناك N وحدة معالجة مركزية افتراضية (vCPUs) على الجهاز وM عملية سيتم إنشاؤها، فإن القيمة القصوى لـ ``num_threads`` التي تستخدمها كل عملية ستكون ``floor(N/M)``. لتجنب الإفراط في اشتراك وحدة المعالجة المركزية في مثال mnist_hogwild، يلزم إجراء التغييرات التالية لملف ``train.py`` في مستودع `الأمثلة <https://github.com/pytorch/examples/tree/main/mnist_hogwild>`__.

.. code:: python

   def train(rank, args, model, device, dataset, dataloader_kwargs):
       torch.manual_seed(args.seed + rank)

       #### تحديد عدد الخيوط المستخدمة في العمليات الفرعية الحالية
       torch.set_num_threads(floor(N/M))

       train_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)

       optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)
       for epoch in range(1, args.epochs + 1):
           train_epoch(epoch, args, model, device, train_loader, optimizer)

قم بتعيين ``num_thread`` لكل عملية باستخدام ``torch.set_num_threads(floor(N/M))``. حيث تستبدل N بعدد وحدات المعالجة المركزية الافتراضية المتوفرة وM بعدد العمليات المحدد. ستختلف القيمة المناسبة لـ ``num_thread`` حسب المهمة المحددة. ومع ذلك، كقاعدة عامة، يجب أن تكون القيمة القصوى لـ ``num_thread`` هي ``floor(N/M)`` لتجنب الإفراط في اشتراك وحدة المعالجة المركزية. في مثال التدريب `mnist_hogwild <https://github.com/pytorch/examples/tree/main/mnist_hogwild>`__، بعد تجنب الإفراط في اشتراك وحدة المعالجة المركزية، يمكنك تحقيق زيادة في الأداء بمقدار 30 مرة.