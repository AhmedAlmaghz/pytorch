.. _random_ness:

قابلية الإعادة
===============

لا تضمن النتائج القابلة للإعادة تمامًا عبر إصدارات PyTorch أو الالتزامات الفردية أو المنصات المختلفة. علاوة على ذلك، قد لا تكون النتائج قابلة للإعادة بين التنفيذ على وحدة المعالجة المركزية ووحدات معالجة الرسوميات، حتى عند استخدام البذور المحددة.

ومع ذلك، هناك بعض الخطوات التي يمكنك اتخاذها للحد من عدد مصادر السلوك غير الحتمي لمنصة وجهاز وإصدار PyTorch محدد. أولاً، يمكنك التحكم في مصادر العشوائية التي يمكن أن تتسبب في سلوك مختلف لتنفيذات متعددة لتطبيقك. ثانيًا، يمكنك تكوين PyTorch لتجنب استخدام الخوارزميات غير الحتمية لبعض العمليات، بحيث تنتج الاستدعاءات المتعددة لتلك العمليات، مع إعطاء نفس المدخلات، نفس النتيجة.

.. warning::

    العمليات الحتمية تكون غالبًا أبطأ من العمليات غير الحتمية، لذا فقد ينخفض الأداء الفردي لنموذجك. ومع ذلك، قد توفر الحتمية الوقت أثناء التطوير من خلال تسهيل التجريب والتصحيح واختبار الانحدار.

التحكم في مصادر العشوائية
....................

مولد الأرقام العشوائية في PyTorch
--------------------------
يمكنك استخدام :meth:`torch.manual_seed()` لبذر مولد الأرقام العشوائية لجميع الأجهزة (وحدة المعالجة المركزية وCUDA)::

    import torch
    torch.manual_seed(0)

قد تستخدم بعض عمليات PyTorch الأرقام العشوائية داخليًا.
:meth:`torch.svd_lowrank()` يفعل ذلك، على سبيل المثال. وبالتالي، فإن استدعاءه عدة مرات على التوالي بنفس وسائط الإدخال قد يعطي نتائج مختلفة. ومع ذلك، طالما تم تعيين :meth:`torch.manual_seed()` إلى ثابت في بداية التطبيق وتم القضاء على جميع مصادر اللاحتمية الأخرى، فسيتم إنشاء نفس سلسلة الأرقام العشوائية في كل مرة يتم فيها تشغيل التطبيق في نفس البيئة.

ومن الممكن أيضًا الحصول على نتائج متطابقة من عملية تستخدم الأرقام العشوائية عن طريق تعيين :meth:`torch.manual_seed()` إلى نفس القيمة بين الاستدعاءات اللاحقة.

بايثون
------

بالنسبة للمشغلين المخصصين، قد تحتاج إلى تعيين البذور بايثون أيضًا::

    import random
    random.seed(0)

مولدات الأرقام العشوائية في المكتبات الأخرى
--------------------------------
إذا كنت أنت أو أي من المكتبات التي تستخدمها تعتمد على NumPy، فيمكنك بذر مولد الأرقام العشوائية العالمي لـ NumPy باستخدام::

    import numpy as np
    np.random.seed(0)

ومع ذلك، قد تستخدم بعض التطبيقات والمكتبات كائنات مولد الأرقام العشوائية لـ NumPy، وليس مولد الأرقام العشوائية العالمي
(`<https://numpy.org/doc/stable/reference/random/generator.html>`_)، والتي ستحتاج إلى بذرها بشكل متسق أيضًا.

إذا كنت تستخدم أي مكتبات أخرى تستخدم مولدات الأرقام العشوائية، فراجع وثائق تلك المكتبات لمعرفة كيفية تعيين البذور المتسقة لها.

اختبار أداء التجزئة في التحويل
------------------------
يمكن أن تكون مكتبة cuDNN، التي تستخدمها عمليات التجزئة CUDA، مصدرًا لعدم الحتمية
عبر عمليات التنفيذ المتعددة لتطبيق ما. عندما يتم استدعاء تجزئة cuDNN باستخدام
مجموعة جديدة من معلمات الحجم، يمكن لميزة اختيارية تشغيل خوارزميات تجزئة متعددة،
واختبارها لمعرفة أسرعها. ثم يتم استخدام أسرع خوارزمية باستمرار خلال بقية العملية
لمجموعة معلمات الحجم المقابلة. بسبب ضجيج الاختبار والاختلافات في الأجهزة،
قد يختار الاختبار خوارزميات مختلفة في عمليات التشغيل اللاحقة، حتى على نفس الجهاز.

يؤدي تعطيل ميزة الاختبار عن طريق تعيين :code:`torch.backends.cudnn.benchmark = False`
إلى جعل cuDNN يحدد خوارزمية حتمية، ربما على حساب الأداء المنخفض.

ومع ذلك، إذا لم تكن بحاجة إلى قابلية إعادة التطبيق عبر عمليات التنفيذ المتعددة لتطبيقك،
فقد يتحسن الأداء إذا تم تمكين ميزة الاختبار عن طريق تعيين :code:`torch.backends.cudnn.benchmark = True`.

لاحظ أن هذا الإعداد يختلف عن الإعداد :code:`torch.backends.cudnn.deterministic`
الذي تمت مناقشته أدناه.

تجنب الخوارزميات غير الحتمية
......................
:meth:`torch.use_deterministic_algorithms` يتيح لك تكوين PyTorch لاستخدام
خوارزميات حتمية بدلاً من الخوارزميات غير الحتمية حيثما كان ذلك متاحًا،
ولإلقاء خطأ إذا كانت العملية معروفة بأنها غير حتمية (وبدون بديل حتمي).

يرجى مراجعة وثائق :meth:`torch.use_deterministic_algorithms()` للحصول على قائمة كاملة
بالعمليات المتأثرة. إذا لم تعمل عملية ما بشكل صحيح وفقًا للوثائق، أو إذا كنت بحاجة إلى
تنفيذ حتمي لعملية لا تحتوي على واحدة، يرجى تقديم مشكلة:
`<https://github.com/pytorch/pytorch/issues?q=label:%22module:%20determinism%22>`_

على سبيل المثال، يؤدي تشغيل التنفيذ غير الحتمي لـ CUDA لـ :meth:`torch.Tensor.index_add_`
إلى إلقاء خطأ::

    >>> import torch
    >>> torch.use_deterministic_algorithms(True)
    >>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))
    Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    RuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set
    'torch.use_deterministic_algorithms(True)'. ...

عندما يتم استدعاء :meth:`torch.bmm` مع تنسورات CUDA نادرة وكثيفة، فإنه يستخدم عادةً خوارزمية
غير حتمية، ولكن عندما يتم تشغيل العلم الحتمي، سيتم استخدام تنفيذه الحتمي::

    >>> import torch
    >>> torch.use_deterministic_algorithms(True)
    >>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())
    tensor([[[ 1.1900, -2.3409],
             [ 0.4796,  0.8003]],
            [[ 0.1509,  1.8027],
             [ 0.0333, -1.1444]]], device='cuda:0')

علاوة على ذلك، إذا كنت تستخدم تنسورات CUDA، وإصدار CUDA الخاص بك هو 10.2 أو أعلى، فيجب عليك
تعيين متغير البيئة `CUBLAS_WORKSPACE_CONFIG` وفقًا لوثائق CUDA:
`<https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility>`_

حتمية التجزئة CUDA
----------------
في حين أن تعطيل اختبار تجزئة CUDA (كما هو موضح أعلاه) يضمن أن CUDA
تختار نفس الخوارزمية في كل مرة يتم فيها تشغيل التطبيق، فقد تكون الخوارزمية نفسها
غير حتمية، ما لم يتم تعيين :code:`torch.use_deterministic_algorithms(True)` أو
:code:`torch.backends.cudnn.deterministic = True`. يتحكم الإعداد الأخير فقط في هذا السلوك،
على عكس :meth:`torch.use_deterministic_algorithms` الذي سيجعل عمليات PyTorch الأخرى
تتصرف بشكل حتمي أيضًا.

التكرار والشبكات العصبية طويلة المدى في CUDA
---------------------------------
في بعض إصدارات CUDA، قد يكون لدى الشبكات العصبية التكرارية والشبكات العصبية طويلة المدى
سلوك غير حتمي. راجع :meth:`torch.nn.RNN` و :meth:`torch.nn.LSTM` للحصول على التفاصيل والحلول البديلة.

ملء الذاكرة غير المبدئية
------------------
يمكن لعمليات مثل :meth:`torch.empty` و :meth:`torch.Tensor.resize_` أن تعيد
تنسورات بذاكرة غير مبدئية تحتوي على قيم غير محددة. يعد استخدام مثل هذا
التنسور كإدخال لعملية أخرى غير صالح إذا كانت الحتمية مطلوبة، لأن الإخراج
سيكون غير حتمي. ولكن لا يوجد شيء يمنع فعليًا تشغيل مثل هذا الرمز غير الصالح.
لذا، لأسباب تتعلق بالسلامة، يتم تعيين :attr:`torch.utils.deterministic.fill_uninitialized_memory`
إلى ``True`` بشكل افتراضي، والذي سيملأ الذاكرة غير المبدئية بقيمة معروفة إذا
تم تعيين :code:`torch.use_deterministic_algorithms(True)`. سيؤدي هذا إلى منع
إمكانية حدوث هذا النوع من السلوك غير الحتمي.

ومع ذلك، فإن ملء الذاكرة غير المبدئية يضر بالأداء. لذا، إذا كان برنامجك صالحًا ولا
يستخدم الذاكرة غير المبدئية كإدخال لعملية ما، فيمكن إيقاف تشغيل هذا الإعداد لتحسين الأداء.

DataLoader
..........

سيقوم DataLoader بإعادة بذر العمال وفقًا لخوارزمية :ref:`data-loading-randomness`.
استخدم :meth:`worker_init_fn` و `generator` للحفاظ على قابلية إعادة الإنتاج::

    def seed_worker(worker_id):
        worker_seed = torch.initial_seed() % 2**32
        numpy.random.seed(worker_seed)
        random.seed(worker_seed)

    g = torch.Generator()
    g.manual_seed(0)

    DataLoader(
        train_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        worker_init_fn=seed_worker,
        generator=g,
    )